{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large amount of credit goes to:\n",
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# which I've used as a reference for this implementation\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, GlobalAveragePooling2D, Dense, Conv2DTranspose, Flatten, LeakyReLU, Reshape\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from skimage import exposure\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linux Paths for CheXpert Dataset\n",
    "\n",
    "train_dir = os.path.abspath(\"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/train.csv\")\n",
    "traindf=pd.read_csv(train_dir, dtype=str)\n",
    "\n",
    "valid_dir = os.path.abspath(\"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/valid.csv\")\n",
    "validdf=pd.read_csv(valid_dir, dtype=str)\n",
    "\n",
    "for i in range(len(traindf)):\n",
    "    traindf.iloc[i,0] = \"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/\" + traindf.iloc[i,0]\"\"\"\n",
    "    \n",
    "#Windows Paths for CheXpert Dataset\n",
    "train_dir = os.path.abspath(r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/train.csv\")\n",
    "traindf=pd.read_csv(train_dir, dtype=str)\n",
    "\n",
    "#Modify dataframe path\n",
    "for i in range(len(traindf)):\n",
    "    traindf.iloc[i,0] = r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/\" + traindf.iloc[i,0]\n",
    "\n",
    "#valid_dir = os.path.abspath(r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/valid.csv\")\n",
    "#validdf=pd.read_csv(valid_dir, dtype=str)\n",
    "#Only looking at AP (anterior-posterior) view xrays\n",
    "aptrainlist = []\n",
    "for i in range(len(traindf)):\n",
    "    if (traindf.iloc[i,4] == \"AP\"):\n",
    "        aptrainlist.append(traindf.iloc[i,:])\n",
    "\n",
    "aptraindf = pd.DataFrame(aptrainlist)\n",
    "\n",
    "#Only looking at xrays labeled Pneumothorax\n",
    "paths = []\n",
    "for i in range(len(aptraindf[aptraindf[\"Pneumothorax\"] == \"1.0\"][\"Path\"])):\n",
    "    paths.append(aptraindf[aptraindf[\"Pneumothorax\"] == \"1.0\"][\"Path\"].iloc[i])\n",
    "\n",
    "#Normalization called in get_imgs() not used right now\n",
    "def normalize_xray(img):\n",
    "    hist_normal = exposure.equalize_adapthist(img/np.max(img))   \n",
    "    #clache_hist_normal = exposure.equalize_adapthist(hist_normal /np.max(hist_normal))\n",
    "    #return clache_hist_normal\n",
    "    return hist_normal\n",
    "\n",
    "#load 128x128 images\n",
    "IMG_SIZE = 128\n",
    "def get_imgs(paths):\n",
    "    images = []\n",
    "    for i in paths:\n",
    "        #Normalized\n",
    "        #images.append(normalize_xray(cv2.cvtColor(cv2.resize(cv2.imread(i),(IMG_SIZE,IMG_SIZE)), cv2.COLOR_BGR2GRAY)))\n",
    "        #Gray Scale \n",
    "        images.append(cv2.cvtColor(cv2.resize(cv2.imread(i),(IMG_SIZE,IMG_SIZE)), cv2.COLOR_BGR2GRAY))\n",
    "    return images \n",
    "\n",
    "#X_train array of images with values between 0 and 1\n",
    "X_train = np.array(get_imgs(paths)).astype(np.float32)\n",
    "#reshaped X train and shifted pixzel values between -1 and 1 for tanh \n",
    "X_train_dcgan = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1) * 2. - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32,1,1,1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_discriminator = 5\n",
    "        optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "        # Build the generator and discriminator\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the discriminator\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training discriminator\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.discriminator(fake_img)\n",
    "        valid = self.discriminator(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.discriminator(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.discriminator_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "        self.discriminator_model.compile(loss=[self.wasserstein_loss,\n",
    "                                              self.wasserstein_loss,\n",
    "                                              partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the discriminator's layers\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "\n",
    "    \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 32* 32, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((32,32, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        \"\"\"model = Sequential()\n",
    "        model.add(Dense(128 * 32* 32, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((32,32, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\"\"\"\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        \"\"\"model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=[128, 128, 1], padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\"\"\"\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = X_train_dcgan\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        #X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_discriminator):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the discriminator\n",
    "                d_loss = self.discriminator_model.train_on_batch([imgs, noise],\n",
    "                                                                [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 1 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"GAN_images/generated_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nprim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 131072)            13238272  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 64, 64, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 128, 128, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 64)      73792     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 1)       577       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,460,993\n",
      "Trainable params: 13,460,609\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 32)        320       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 33, 33, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 33, 33, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 33, 33, 64)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 33, 33, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 17, 17, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 17, 17, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 17, 17, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 17, 17, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 17, 17, 256)       0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 17, 17, 256)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 73984)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 73985     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 463,617\n",
      "Trainable params: 462,721\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "0 [D loss: 6069.171387] [G loss: -0.285770]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nprim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 15646.968750] [G loss: -0.364617]\n",
      "2 [D loss: 1482.632202] [G loss: -0.435670]\n",
      "3 [D loss: 1308.565186] [G loss: -0.429127]\n",
      "4 [D loss: 2035.594238] [G loss: -0.511388]\n",
      "5 [D loss: 646.273315] [G loss: -0.536591]\n",
      "6 [D loss: 462.767059] [G loss: -0.461389]\n",
      "7 [D loss: 1947.825073] [G loss: -0.554522]\n",
      "8 [D loss: 1117.559692] [G loss: -0.611664]\n",
      "9 [D loss: 1848.716797] [G loss: -0.561276]\n",
      "10 [D loss: 157.336716] [G loss: -0.616786]\n",
      "11 [D loss: 377.265289] [G loss: -0.605333]\n",
      "12 [D loss: 528.857727] [G loss: -0.651687]\n",
      "13 [D loss: 215.178940] [G loss: -0.771999]\n",
      "14 [D loss: 160.800537] [G loss: -0.767033]\n",
      "15 [D loss: 328.614349] [G loss: -0.772966]\n",
      "16 [D loss: 56.079216] [G loss: -0.739302]\n",
      "17 [D loss: 401.473785] [G loss: -0.842094]\n",
      "18 [D loss: 193.755219] [G loss: -0.843394]\n",
      "19 [D loss: 512.067993] [G loss: -0.840605]\n",
      "20 [D loss: 16.533714] [G loss: -0.885977]\n",
      "21 [D loss: 170.668961] [G loss: -0.928551]\n",
      "22 [D loss: 2.759519] [G loss: -0.892162]\n",
      "23 [D loss: 219.742615] [G loss: -0.892829]\n",
      "24 [D loss: 0.360229] [G loss: -0.938604]\n",
      "25 [D loss: 10.331584] [G loss: -0.930865]\n",
      "26 [D loss: 22.304615] [G loss: -0.886099]\n",
      "27 [D loss: 554.463562] [G loss: -0.913070]\n",
      "28 [D loss: 7.709118] [G loss: -0.943174]\n",
      "29 [D loss: 0.652219] [G loss: -0.934770]\n",
      "30 [D loss: 7.364055] [G loss: -0.897333]\n",
      "31 [D loss: 7.236983] [G loss: -0.981751]\n",
      "32 [D loss: 58.057434] [G loss: -0.942576]\n",
      "33 [D loss: 33.837757] [G loss: -0.936354]\n",
      "34 [D loss: 4.748563] [G loss: -0.964727]\n",
      "35 [D loss: 1525.477295] [G loss: -0.961690]\n",
      "36 [D loss: 126.340401] [G loss: -0.953602]\n",
      "37 [D loss: 1.283431] [G loss: -0.961724]\n",
      "38 [D loss: 153.244568] [G loss: -0.979985]\n",
      "39 [D loss: 1.740889] [G loss: -0.980802]\n",
      "40 [D loss: 0.471417] [G loss: -0.986472]\n",
      "41 [D loss: 4.497773] [G loss: -0.987551]\n",
      "42 [D loss: 2.764808] [G loss: -0.979202]\n",
      "43 [D loss: 5.463679] [G loss: -0.990267]\n",
      "44 [D loss: 4.808959] [G loss: -0.990735]\n",
      "45 [D loss: 0.073689] [G loss: -0.988698]\n",
      "46 [D loss: 0.233409] [G loss: -0.980192]\n",
      "47 [D loss: 417.169769] [G loss: -0.983214]\n",
      "48 [D loss: 2.832653] [G loss: -0.972112]\n",
      "49 [D loss: 6.574220] [G loss: -0.979982]\n",
      "50 [D loss: 78.559059] [G loss: -0.978828]\n",
      "51 [D loss: 3.414809] [G loss: -0.928504]\n",
      "52 [D loss: 2.736227] [G loss: -0.987557]\n",
      "53 [D loss: 4.992859] [G loss: -0.972630]\n",
      "54 [D loss: 4.091782] [G loss: -0.984813]\n",
      "55 [D loss: 1783.153076] [G loss: -0.967074]\n",
      "56 [D loss: 0.354062] [G loss: -0.960573]\n",
      "57 [D loss: 480.914307] [G loss: -0.973043]\n",
      "58 [D loss: 228.745438] [G loss: -0.964159]\n",
      "59 [D loss: 10.912499] [G loss: -0.935690]\n",
      "60 [D loss: 0.265959] [G loss: -0.808742]\n",
      "61 [D loss: 213.748581] [G loss: -0.847360]\n",
      "62 [D loss: 6.262297] [G loss: -0.970042]\n",
      "63 [D loss: 278.348358] [G loss: -0.965905]\n",
      "64 [D loss: 6.268198] [G loss: -0.936179]\n",
      "65 [D loss: 79.181709] [G loss: -0.884607]\n",
      "66 [D loss: 341.462494] [G loss: -0.046430]\n",
      "67 [D loss: 8.250171] [G loss: -0.139310]\n",
      "68 [D loss: 49.031055] [G loss: -0.306314]\n",
      "69 [D loss: 3.274587] [G loss: -0.258777]\n",
      "70 [D loss: 50.506481] [G loss: -0.033294]\n",
      "71 [D loss: 55.071545] [G loss: -0.049505]\n",
      "72 [D loss: 186.773300] [G loss: -0.355837]\n",
      "73 [D loss: 2.930740] [G loss: -0.150558]\n",
      "74 [D loss: 246.860870] [G loss: -0.089195]\n",
      "75 [D loss: 0.215577] [G loss: -0.220643]\n",
      "76 [D loss: 442.904694] [G loss: -0.762287]\n",
      "77 [D loss: 2.964716] [G loss: -0.454327]\n",
      "78 [D loss: 147.279938] [G loss: -0.090524]\n",
      "79 [D loss: 3191.402588] [G loss: -0.004072]\n",
      "80 [D loss: 9.588074] [G loss: -0.025552]\n",
      "81 [D loss: 353.803436] [G loss: -0.033533]\n",
      "82 [D loss: 38.591557] [G loss: -0.043552]\n",
      "83 [D loss: 345.478119] [G loss: -0.042838]\n",
      "84 [D loss: 721.793701] [G loss: -0.016176]\n",
      "85 [D loss: 292.165833] [G loss: -0.003109]\n",
      "86 [D loss: 528.044556] [G loss: -0.118687]\n",
      "87 [D loss: 735.663330] [G loss: -0.172087]\n",
      "88 [D loss: 519.655762] [G loss: -0.012485]\n",
      "89 [D loss: 1765.786133] [G loss: -0.618816]\n",
      "90 [D loss: 118.414680] [G loss: -0.704747]\n",
      "91 [D loss: 17.876677] [G loss: -0.768869]\n",
      "92 [D loss: 1.096922] [G loss: -0.600030]\n",
      "93 [D loss: 5.760381] [G loss: -0.825683]\n",
      "94 [D loss: 4.851749] [G loss: -0.853217]\n",
      "95 [D loss: 1.590138] [G loss: -0.728755]\n",
      "96 [D loss: 1.991444] [G loss: -0.651799]\n",
      "97 [D loss: 1233.870605] [G loss: -0.738909]\n",
      "98 [D loss: 128.903259] [G loss: -0.569854]\n",
      "99 [D loss: 40.819370] [G loss: -0.802500]\n",
      "100 [D loss: 1575.484985] [G loss: -0.001959]\n",
      "101 [D loss: 498.129425] [G loss: -0.454402]\n",
      "102 [D loss: 6.753734] [G loss: -0.458193]\n",
      "103 [D loss: 630.616455] [G loss: -0.004121]\n",
      "104 [D loss: 0.791664] [G loss: -0.673550]\n",
      "105 [D loss: 0.328125] [G loss: -0.673997]\n",
      "106 [D loss: 2.810288] [G loss: -0.802328]\n",
      "107 [D loss: 14.603446] [G loss: -0.541348]\n",
      "108 [D loss: 2779.540771] [G loss: -0.289616]\n",
      "109 [D loss: 211.884308] [G loss: -0.218516]\n",
      "110 [D loss: 1132.402344] [G loss: -0.149385]\n",
      "111 [D loss: 676.449036] [G loss: -0.031077]\n",
      "112 [D loss: 51.048981] [G loss: -0.143001]\n",
      "113 [D loss: 250.579300] [G loss: -0.342841]\n",
      "114 [D loss: 53.138767] [G loss: -0.500050]\n",
      "115 [D loss: 64.159546] [G loss: -0.597172]\n",
      "116 [D loss: 286.817963] [G loss: -0.495062]\n",
      "117 [D loss: 994.368347] [G loss: -0.022540]\n",
      "118 [D loss: 1434.230957] [G loss: -0.160301]\n",
      "119 [D loss: 52.489140] [G loss: -0.235211]\n",
      "120 [D loss: 828.533630] [G loss: -0.029171]\n",
      "121 [D loss: 4.405562] [G loss: -0.555809]\n",
      "122 [D loss: 52.430157] [G loss: -0.659426]\n",
      "123 [D loss: 170.213318] [G loss: -0.736550]\n",
      "124 [D loss: 0.885580] [G loss: -0.742319]\n",
      "125 [D loss: 1.848016] [G loss: -0.886628]\n",
      "126 [D loss: 5.159429] [G loss: -0.954459]\n",
      "127 [D loss: 8.718475] [G loss: -0.963890]\n",
      "128 [D loss: 10.418977] [G loss: -0.959876]\n",
      "129 [D loss: 3.134566] [G loss: -0.935130]\n",
      "130 [D loss: 10.808761] [G loss: -0.866066]\n",
      "131 [D loss: 46.177864] [G loss: -0.879021]\n",
      "132 [D loss: 3.276310] [G loss: -0.795476]\n",
      "133 [D loss: 1.057726] [G loss: -0.791025]\n",
      "134 [D loss: 4.095383] [G loss: -0.576987]\n",
      "135 [D loss: 5.749539] [G loss: -0.448552]\n",
      "136 [D loss: 20.497322] [G loss: -0.661259]\n",
      "137 [D loss: 188.466171] [G loss: -0.391303]\n",
      "138 [D loss: 672.811035] [G loss: -0.052179]\n",
      "139 [D loss: 58.819088] [G loss: -0.060828]\n",
      "140 [D loss: 20.590147] [G loss: -0.091766]\n",
      "141 [D loss: 3.910020] [G loss: -0.670433]\n",
      "142 [D loss: 31.308886] [G loss: -0.537630]\n",
      "143 [D loss: 3.877272] [G loss: -0.440413]\n",
      "144 [D loss: 4.293692] [G loss: -0.378468]\n",
      "145 [D loss: 9.629372] [G loss: -0.337305]\n",
      "146 [D loss: 2.804629] [G loss: -0.423513]\n",
      "147 [D loss: 20.313892] [G loss: -0.423497]\n",
      "148 [D loss: 39.441521] [G loss: -0.343829]\n",
      "149 [D loss: 4.561614] [G loss: -0.647015]\n",
      "150 [D loss: 8.929670] [G loss: -0.662654]\n",
      "151 [D loss: 1.053311] [G loss: -0.556838]\n",
      "152 [D loss: 4.482988] [G loss: -0.485345]\n",
      "153 [D loss: 4.335728] [G loss: -0.510930]\n",
      "154 [D loss: 0.191031] [G loss: -0.320292]\n",
      "155 [D loss: 347.069183] [G loss: -0.180713]\n",
      "156 [D loss: 3.082344] [G loss: -0.064647]\n",
      "157 [D loss: 253.267715] [G loss: -0.032937]\n",
      "158 [D loss: 264.995636] [G loss: -0.027443]\n",
      "159 [D loss: 4.584122] [G loss: -0.052930]\n",
      "160 [D loss: 1.486201] [G loss: -0.086697]\n",
      "161 [D loss: 3.698491] [G loss: -0.127228]\n",
      "162 [D loss: 39.887993] [G loss: -0.449515]\n",
      "163 [D loss: 6.215882] [G loss: -0.506493]\n",
      "164 [D loss: 7.209217] [G loss: -0.710820]\n",
      "165 [D loss: 4.443828] [G loss: -0.575956]\n",
      "166 [D loss: 8.211176] [G loss: -0.419507]\n",
      "167 [D loss: 0.612610] [G loss: -0.462126]\n",
      "168 [D loss: 25.169621] [G loss: -0.408871]\n",
      "169 [D loss: 6.980879] [G loss: -0.464508]\n",
      "170 [D loss: 6.522666] [G loss: -0.552580]\n",
      "171 [D loss: 2.757302] [G loss: -0.564361]\n",
      "172 [D loss: 8.500284] [G loss: -0.936479]\n",
      "173 [D loss: 8.922484] [G loss: -0.914953]\n",
      "174 [D loss: 6.438272] [G loss: -0.882019]\n",
      "175 [D loss: 8.757446] [G loss: -0.876599]\n",
      "176 [D loss: 5.014352] [G loss: -0.650356]\n",
      "177 [D loss: 7.303612] [G loss: -0.397930]\n",
      "178 [D loss: 7.664587] [G loss: -0.466258]\n",
      "179 [D loss: 2.302632] [G loss: -0.405758]\n",
      "180 [D loss: 23.726877] [G loss: -0.413012]\n",
      "181 [D loss: 2.522374] [G loss: -0.273931]\n",
      "182 [D loss: 7.512917] [G loss: -0.466349]\n",
      "183 [D loss: 54.576565] [G loss: -0.129511]\n",
      "184 [D loss: 8.987707] [G loss: -0.755932]\n",
      "185 [D loss: 6.063121] [G loss: -0.084197]\n",
      "186 [D loss: 243.363251] [G loss: -0.053774]\n",
      "187 [D loss: 23.118889] [G loss: -0.000290]\n",
      "188 [D loss: 78.948730] [G loss: -0.000042]\n",
      "189 [D loss: 22.754879] [G loss: -0.000058]\n",
      "190 [D loss: 151.723373] [G loss: -0.000152]\n",
      "191 [D loss: 0.516793] [G loss: -0.000004]\n",
      "192 [D loss: 83.744858] [G loss: -0.000004]\n",
      "193 [D loss: 0.675449] [G loss: -0.000010]\n",
      "194 [D loss: 29.888903] [G loss: -0.000000]\n",
      "195 [D loss: 2.674784] [G loss: -0.000003]\n",
      "196 [D loss: 2.998356] [G loss: -0.000000]\n",
      "197 [D loss: 4.108969] [G loss: -0.000002]\n",
      "198 [D loss: 5.274880] [G loss: -0.000010]\n",
      "199 [D loss: 3.883593] [G loss: -0.000001]\n",
      "200 [D loss: 4.703128] [G loss: -0.000000]\n",
      "201 [D loss: 15.868097] [G loss: -0.000000]\n",
      "202 [D loss: 9.152504] [G loss: -0.000000]\n",
      "203 [D loss: 31.179871] [G loss: -0.000000]\n",
      "204 [D loss: 0.487904] [G loss: -0.000000]\n",
      "205 [D loss: 13.752542] [G loss: -0.000000]\n",
      "206 [D loss: 0.931499] [G loss: -0.000000]\n",
      "207 [D loss: 16.704309] [G loss: -0.000000]\n",
      "208 [D loss: 30.690508] [G loss: -0.000000]\n",
      "209 [D loss: 38.222168] [G loss: -0.000000]\n",
      "210 [D loss: 7.788893] [G loss: -0.000001]\n",
      "211 [D loss: 2.890155] [G loss: -0.000005]\n",
      "212 [D loss: 2.524405] [G loss: -0.000000]\n",
      "213 [D loss: 3.699189] [G loss: -0.000000]\n",
      "214 [D loss: 0.988669] [G loss: -0.000000]\n",
      "215 [D loss: 17.871929] [G loss: -0.000000]\n",
      "216 [D loss: 3.285004] [G loss: -0.000000]\n",
      "217 [D loss: 19.138008] [G loss: -0.000000]\n",
      "218 [D loss: 0.974651] [G loss: -0.000000]\n",
      "219 [D loss: 0.523339] [G loss: -0.000000]\n",
      "220 [D loss: 8.824485] [G loss: -0.000000]\n",
      "221 [D loss: 5.128618] [G loss: -0.000000]\n",
      "222 [D loss: 4.916651] [G loss: -0.000000]\n",
      "223 [D loss: 15.406782] [G loss: -0.000002]\n",
      "224 [D loss: 12.288455] [G loss: -0.000000]\n",
      "225 [D loss: 1.435441] [G loss: -0.000000]\n",
      "226 [D loss: 12.725671] [G loss: -0.000000]\n",
      "227 [D loss: 1.770176] [G loss: -0.000000]\n",
      "228 [D loss: 2.896851] [G loss: -0.000000]\n",
      "229 [D loss: 3.476307] [G loss: -0.000000]\n",
      "230 [D loss: 5.815656] [G loss: -0.000000]\n",
      "231 [D loss: 2.784554] [G loss: -0.000000]\n",
      "232 [D loss: 2.441625] [G loss: -0.000000]\n",
      "233 [D loss: 0.816041] [G loss: -0.000000]\n",
      "234 [D loss: 6.183087] [G loss: -0.000000]\n",
      "235 [D loss: 2.782492] [G loss: -0.000000]\n",
      "236 [D loss: 0.623480] [G loss: -0.000000]\n",
      "237 [D loss: 10.470742] [G loss: -0.000000]\n",
      "238 [D loss: 1.254696] [G loss: -0.000000]\n",
      "239 [D loss: 4.619045] [G loss: -0.000014]\n",
      "240 [D loss: 0.827232] [G loss: -0.000000]\n",
      "241 [D loss: 0.802079] [G loss: -0.000000]\n",
      "242 [D loss: 1.993594] [G loss: -0.000000]\n",
      "243 [D loss: 1.139204] [G loss: -0.000000]\n",
      "244 [D loss: 1.538137] [G loss: -0.000000]\n",
      "245 [D loss: 1.511090] [G loss: -0.000000]\n",
      "246 [D loss: 3.422356] [G loss: -0.000000]\n",
      "247 [D loss: 4.395840] [G loss: -0.000000]\n",
      "248 [D loss: 1.932916] [G loss: -0.000000]\n",
      "249 [D loss: 1.581008] [G loss: -0.000000]\n",
      "250 [D loss: 7.918166] [G loss: -0.000000]\n",
      "251 [D loss: 0.626568] [G loss: -0.000000]\n",
      "252 [D loss: 0.808252] [G loss: -0.000000]\n",
      "253 [D loss: 0.582280] [G loss: -0.000000]\n",
      "254 [D loss: 8.690105] [G loss: -0.000000]\n",
      "255 [D loss: 6.372315] [G loss: -0.000000]\n",
      "256 [D loss: 7.105442] [G loss: -0.000000]\n",
      "257 [D loss: 5.938154] [G loss: -0.000000]\n",
      "258 [D loss: 0.511658] [G loss: -0.000000]\n",
      "259 [D loss: 2.068048] [G loss: -0.000006]\n",
      "260 [D loss: 9.881205] [G loss: -0.000000]\n",
      "261 [D loss: 1.011113] [G loss: -0.000043]\n",
      "262 [D loss: 0.332086] [G loss: -0.000040]\n",
      "263 [D loss: 13.198074] [G loss: -0.000000]\n",
      "264 [D loss: 1.102095] [G loss: -0.000000]\n",
      "265 [D loss: 5.298828] [G loss: -0.000000]\n",
      "266 [D loss: 0.785389] [G loss: -0.000000]\n",
      "267 [D loss: 0.432399] [G loss: -0.000000]\n",
      "268 [D loss: 2.424228] [G loss: -0.000000]\n",
      "269 [D loss: 1.998789] [G loss: -0.000000]\n",
      "270 [D loss: 0.643547] [G loss: -0.000000]\n",
      "271 [D loss: 3.402438] [G loss: -0.000000]\n",
      "272 [D loss: 0.639235] [G loss: -0.001641]\n",
      "273 [D loss: 7.222587] [G loss: -0.062051]\n",
      "274 [D loss: 7.403155] [G loss: -0.123731]\n",
      "275 [D loss: 71.437073] [G loss: -0.016208]\n",
      "276 [D loss: 0.614520] [G loss: -0.000559]\n",
      "277 [D loss: 14.131307] [G loss: -0.000000]\n",
      "278 [D loss: 1.623549] [G loss: -0.000000]\n",
      "279 [D loss: 7.688828] [G loss: -0.000000]\n",
      "280 [D loss: 6.788632] [G loss: -0.000000]\n",
      "281 [D loss: 2.770636] [G loss: -0.003826]\n",
      "282 [D loss: 0.530953] [G loss: -0.018428]\n",
      "283 [D loss: 1.219925] [G loss: -0.002496]\n",
      "284 [D loss: 1.654316] [G loss: -0.001040]\n",
      "285 [D loss: 2.005226] [G loss: -0.000004]\n",
      "286 [D loss: 2.597624] [G loss: -0.000001]\n",
      "287 [D loss: 1.575746] [G loss: -0.051680]\n",
      "288 [D loss: 0.599895] [G loss: -0.000019]\n",
      "289 [D loss: 4.832998] [G loss: -0.000005]\n",
      "290 [D loss: 13.455967] [G loss: -0.000035]\n",
      "291 [D loss: 6.281755] [G loss: -0.000000]\n",
      "292 [D loss: 8.832167] [G loss: -0.510101]\n",
      "293 [D loss: 7.494648] [G loss: -0.025601]\n",
      "294 [D loss: 0.891358] [G loss: -0.000268]\n",
      "295 [D loss: 8.984819] [G loss: -0.018754]\n",
      "296 [D loss: 1.987540] [G loss: -0.000001]\n",
      "297 [D loss: 0.495188] [G loss: -0.000028]\n",
      "298 [D loss: 2.629717] [G loss: -0.000005]\n",
      "299 [D loss: 1.578497] [G loss: -0.040144]\n",
      "300 [D loss: 2.764211] [G loss: -0.000029]\n",
      "301 [D loss: 0.912381] [G loss: -0.000000]\n",
      "302 [D loss: 1.915953] [G loss: -0.000000]\n",
      "303 [D loss: 4.064725] [G loss: -0.000000]\n",
      "304 [D loss: 1.222304] [G loss: -0.000000]\n",
      "305 [D loss: 0.337233] [G loss: -0.000000]\n",
      "306 [D loss: 1.052342] [G loss: -0.000000]\n",
      "307 [D loss: 2.450024] [G loss: -0.000000]\n",
      "308 [D loss: 0.428147] [G loss: -0.000000]\n",
      "309 [D loss: 0.703569] [G loss: -0.000000]\n",
      "310 [D loss: 13.573441] [G loss: -0.000000]\n",
      "311 [D loss: 1.440000] [G loss: -0.000000]\n",
      "312 [D loss: 2.234149] [G loss: -0.000000]\n",
      "313 [D loss: 1.429633] [G loss: -0.000000]\n",
      "314 [D loss: 0.414559] [G loss: -0.000000]\n",
      "315 [D loss: 3.760136] [G loss: -0.000000]\n",
      "316 [D loss: 1.000032] [G loss: -0.000000]\n",
      "317 [D loss: 3.390625] [G loss: -0.000000]\n",
      "318 [D loss: 2.090601] [G loss: -0.000000]\n",
      "319 [D loss: 0.485513] [G loss: -0.000000]\n",
      "320 [D loss: 7.930389] [G loss: -0.000000]\n",
      "321 [D loss: 8.247548] [G loss: -0.006139]\n",
      "322 [D loss: 1.164262] [G loss: -0.000005]\n",
      "323 [D loss: 0.688939] [G loss: -0.000001]\n",
      "324 [D loss: 0.336670] [G loss: -0.000071]\n",
      "325 [D loss: 2.019964] [G loss: -0.000010]\n",
      "326 [D loss: 6.318140] [G loss: -0.000183]\n",
      "327 [D loss: 4.730530] [G loss: -0.001173]\n",
      "328 [D loss: 6.387948] [G loss: -0.000000]\n",
      "329 [D loss: 2.127744] [G loss: -0.001603]\n",
      "330 [D loss: 0.991764] [G loss: -0.000027]\n",
      "331 [D loss: 1.663626] [G loss: -0.005455]\n",
      "332 [D loss: 0.389698] [G loss: -0.000302]\n",
      "333 [D loss: 5.730854] [G loss: -0.000053]\n",
      "334 [D loss: 1.094734] [G loss: -0.000046]\n",
      "335 [D loss: 0.441835] [G loss: -0.001843]\n",
      "336 [D loss: 19.542137] [G loss: -0.000059]\n",
      "337 [D loss: 2.698694] [G loss: -0.000160]\n",
      "338 [D loss: 4.935557] [G loss: -0.000421]\n",
      "339 [D loss: 19.284958] [G loss: -0.000031]\n",
      "340 [D loss: 0.646711] [G loss: -0.000000]\n",
      "341 [D loss: 2.680576] [G loss: -0.000000]\n",
      "342 [D loss: 0.202216] [G loss: -0.000015]\n",
      "343 [D loss: 8.791583] [G loss: -0.185861]\n",
      "344 [D loss: 3.476388] [G loss: -0.000449]\n",
      "345 [D loss: 3.767192] [G loss: -0.000027]\n",
      "346 [D loss: 5.445079] [G loss: -0.007372]\n",
      "347 [D loss: 4.468035] [G loss: -0.000001]\n",
      "348 [D loss: 12.798891] [G loss: -0.000058]\n",
      "349 [D loss: 0.249809] [G loss: -0.000244]\n",
      "350 [D loss: 1.238085] [G loss: -0.000068]\n",
      "351 [D loss: 0.855218] [G loss: -0.000013]\n",
      "352 [D loss: 4.880142] [G loss: -0.010027]\n",
      "353 [D loss: 4.481799] [G loss: -0.000414]\n",
      "354 [D loss: 8.249702] [G loss: -0.104602]\n",
      "355 [D loss: 9.741269] [G loss: -0.000054]\n",
      "356 [D loss: 22.083311] [G loss: -0.000007]\n",
      "357 [D loss: 1.852294] [G loss: -0.002338]\n",
      "358 [D loss: 2.342088] [G loss: -0.000004]\n",
      "359 [D loss: 7.373261] [G loss: -0.000150]\n",
      "360 [D loss: 4.039980] [G loss: -0.000053]\n",
      "361 [D loss: 2.629390] [G loss: -0.000341]\n",
      "362 [D loss: 9.855247] [G loss: -0.018886]\n",
      "363 [D loss: 6.982111] [G loss: -0.002803]\n",
      "364 [D loss: 3.007741] [G loss: -0.003438]\n",
      "365 [D loss: 1.710998] [G loss: -0.029577]\n",
      "366 [D loss: 1.686598] [G loss: -0.000039]\n",
      "367 [D loss: -0.023125] [G loss: -0.002644]\n",
      "368 [D loss: 10.714153] [G loss: -0.011309]\n",
      "369 [D loss: 1.931200] [G loss: -0.000393]\n",
      "370 [D loss: 8.366261] [G loss: -0.002007]\n",
      "371 [D loss: 1.839415] [G loss: -0.007571]\n",
      "372 [D loss: 2.545244] [G loss: -0.001045]\n",
      "373 [D loss: 5.257186] [G loss: -0.000602]\n",
      "374 [D loss: 1.532253] [G loss: -0.000399]\n",
      "375 [D loss: 1.404853] [G loss: -0.024928]\n",
      "376 [D loss: 42.431553] [G loss: -0.002106]\n",
      "377 [D loss: 1.257028] [G loss: -0.000465]\n",
      "378 [D loss: 1.865005] [G loss: -0.000330]\n",
      "379 [D loss: 2.409784] [G loss: -0.001825]\n",
      "380 [D loss: 7.101490] [G loss: -0.003021]\n",
      "381 [D loss: 1.877191] [G loss: -0.035023]\n",
      "382 [D loss: 3.291727] [G loss: -0.000397]\n",
      "383 [D loss: 1.620649] [G loss: -0.003875]\n",
      "384 [D loss: 0.449197] [G loss: -0.001469]\n",
      "385 [D loss: 0.438846] [G loss: -0.000413]\n",
      "386 [D loss: 1.447775] [G loss: -0.012550]\n",
      "387 [D loss: 1.358442] [G loss: -0.005687]\n",
      "388 [D loss: 4.576836] [G loss: -0.015363]\n",
      "389 [D loss: 2.789969] [G loss: -0.000839]\n",
      "390 [D loss: 3.523697] [G loss: -0.001369]\n",
      "391 [D loss: 3.772277] [G loss: -0.001367]\n",
      "392 [D loss: 3.512833] [G loss: -0.020952]\n",
      "393 [D loss: 0.101242] [G loss: -0.003317]\n",
      "394 [D loss: 0.678977] [G loss: -0.015499]\n",
      "395 [D loss: 1.326544] [G loss: -0.021123]\n",
      "396 [D loss: 1.500794] [G loss: -0.001160]\n",
      "397 [D loss: 1.238994] [G loss: -0.000923]\n",
      "398 [D loss: 7.472709] [G loss: -0.000971]\n",
      "399 [D loss: 2.701676] [G loss: -0.014791]\n",
      "400 [D loss: 0.423648] [G loss: -0.000155]\n",
      "401 [D loss: 0.252835] [G loss: -0.000543]\n",
      "402 [D loss: 1.791838] [G loss: -0.007513]\n",
      "403 [D loss: 1.637179] [G loss: -0.005550]\n",
      "404 [D loss: 5.897974] [G loss: -0.000082]\n",
      "405 [D loss: 1.348027] [G loss: -0.002420]\n",
      "406 [D loss: 0.340936] [G loss: -0.000834]\n",
      "407 [D loss: 0.936233] [G loss: -0.000135]\n",
      "408 [D loss: 10.003673] [G loss: -0.001529]\n",
      "409 [D loss: 0.974652] [G loss: -0.009360]\n",
      "410 [D loss: 0.684151] [G loss: -0.003006]\n",
      "411 [D loss: 0.137265] [G loss: -0.000228]\n",
      "412 [D loss: 5.431686] [G loss: -0.038575]\n",
      "413 [D loss: 0.886141] [G loss: -0.017750]\n",
      "414 [D loss: 0.765174] [G loss: -0.000203]\n",
      "415 [D loss: 3.754217] [G loss: -0.000272]\n",
      "416 [D loss: -0.198041] [G loss: -0.000221]\n",
      "417 [D loss: 0.318177] [G loss: -0.000596]\n",
      "418 [D loss: 0.276524] [G loss: -0.012005]\n",
      "419 [D loss: 1.897272] [G loss: -0.001850]\n",
      "420 [D loss: 0.305441] [G loss: -0.001165]\n",
      "421 [D loss: 0.743605] [G loss: -0.000605]\n",
      "422 [D loss: 0.247470] [G loss: -0.000794]\n",
      "423 [D loss: 3.144381] [G loss: -0.000005]\n",
      "424 [D loss: 1.587538] [G loss: -0.000149]\n",
      "425 [D loss: 0.378127] [G loss: -0.000106]\n",
      "426 [D loss: 0.555313] [G loss: -0.000159]\n",
      "427 [D loss: 0.861466] [G loss: -0.000125]\n",
      "428 [D loss: 2.815503] [G loss: -0.000046]\n",
      "429 [D loss: 0.039323] [G loss: -0.000060]\n",
      "430 [D loss: 0.631763] [G loss: -0.000703]\n",
      "431 [D loss: 0.992653] [G loss: -0.000004]\n",
      "432 [D loss: 0.074769] [G loss: -0.000400]\n",
      "433 [D loss: 2.965802] [G loss: -0.002753]\n",
      "434 [D loss: 1.131513] [G loss: -0.000246]\n",
      "435 [D loss: -0.103230] [G loss: -0.000478]\n",
      "436 [D loss: 0.282146] [G loss: -0.000828]\n",
      "437 [D loss: 2.707979] [G loss: -0.001290]\n",
      "438 [D loss: 10.869017] [G loss: -0.001321]\n",
      "439 [D loss: 1.647803] [G loss: -0.000255]\n",
      "440 [D loss: -0.071667] [G loss: -0.000096]\n",
      "441 [D loss: 1.874781] [G loss: -0.001394]\n",
      "442 [D loss: 1.385945] [G loss: -0.045521]\n",
      "443 [D loss: -0.136533] [G loss: -0.002112]\n",
      "444 [D loss: 0.411169] [G loss: -0.006915]\n",
      "445 [D loss: 0.606159] [G loss: -0.002054]\n",
      "446 [D loss: 1.376638] [G loss: -0.000174]\n",
      "447 [D loss: 0.181909] [G loss: -0.000063]\n",
      "448 [D loss: 0.400764] [G loss: -0.000407]\n",
      "449 [D loss: 0.028179] [G loss: -0.001151]\n",
      "450 [D loss: 2.081785] [G loss: -0.354524]\n",
      "451 [D loss: 1.883518] [G loss: -0.014308]\n",
      "452 [D loss: 3.995553] [G loss: -0.000766]\n",
      "453 [D loss: 1.419912] [G loss: -0.001379]\n",
      "454 [D loss: 0.019193] [G loss: -0.000241]\n",
      "455 [D loss: 5.950656] [G loss: -0.007438]\n",
      "456 [D loss: 0.431138] [G loss: -0.000139]\n",
      "457 [D loss: 0.569424] [G loss: -0.000431]\n",
      "458 [D loss: 1.346132] [G loss: -0.002458]\n",
      "459 [D loss: 2.511263] [G loss: -0.002634]\n",
      "460 [D loss: 5.470423] [G loss: -0.000040]\n",
      "461 [D loss: 0.036843] [G loss: -0.006133]\n",
      "462 [D loss: -0.006805] [G loss: -0.006013]\n",
      "463 [D loss: 0.947199] [G loss: -0.000936]\n",
      "464 [D loss: 1.315559] [G loss: -0.000045]\n",
      "465 [D loss: 0.276314] [G loss: -0.000389]\n",
      "466 [D loss: 1.136727] [G loss: -0.001770]\n",
      "467 [D loss: -0.003997] [G loss: -0.000616]\n",
      "468 [D loss: 1.088623] [G loss: -0.005619]\n",
      "469 [D loss: -0.003604] [G loss: -0.000619]\n",
      "470 [D loss: 0.989851] [G loss: -0.001363]\n",
      "471 [D loss: -0.235687] [G loss: -0.003668]\n",
      "472 [D loss: 1.298017] [G loss: -0.003613]\n",
      "473 [D loss: 3.040041] [G loss: -0.000140]\n",
      "474 [D loss: 1.751104] [G loss: -0.000562]\n",
      "475 [D loss: 0.995554] [G loss: -0.001671]\n",
      "476 [D loss: 0.769106] [G loss: -0.023515]\n",
      "477 [D loss: -0.052469] [G loss: -0.005158]\n",
      "478 [D loss: 3.335013] [G loss: -0.007139]\n",
      "479 [D loss: 2.369376] [G loss: -0.001933]\n",
      "480 [D loss: 0.647051] [G loss: -0.000522]\n",
      "481 [D loss: 0.917263] [G loss: -0.000855]\n",
      "482 [D loss: 1.206298] [G loss: -0.003068]\n",
      "483 [D loss: 1.349602] [G loss: -0.002901]\n",
      "484 [D loss: 6.451639] [G loss: -0.008955]\n",
      "485 [D loss: -0.169680] [G loss: -0.011471]\n",
      "486 [D loss: -0.389338] [G loss: -0.001402]\n",
      "487 [D loss: 2.737452] [G loss: -0.011779]\n",
      "488 [D loss: 4.614766] [G loss: -0.000559]\n",
      "489 [D loss: 3.832376] [G loss: -0.000622]\n",
      "490 [D loss: 0.820068] [G loss: -0.006901]\n",
      "491 [D loss: 0.009139] [G loss: -0.002805]\n",
      "492 [D loss: 2.205890] [G loss: -0.000343]\n",
      "493 [D loss: 1.723095] [G loss: -0.000323]\n",
      "494 [D loss: 0.417305] [G loss: -0.001955]\n",
      "495 [D loss: 2.301334] [G loss: -0.003381]\n",
      "496 [D loss: 2.982187] [G loss: -0.001798]\n",
      "497 [D loss: 0.405424] [G loss: -0.014462]\n",
      "498 [D loss: 3.837375] [G loss: -0.000479]\n",
      "499 [D loss: 0.499328] [G loss: -0.000646]\n",
      "500 [D loss: 0.022824] [G loss: -0.000119]\n",
      "501 [D loss: 3.844651] [G loss: -0.000881]\n",
      "502 [D loss: 0.329363] [G loss: -0.005389]\n",
      "503 [D loss: 1.354260] [G loss: -0.000320]\n",
      "504 [D loss: 1.874599] [G loss: -0.004480]\n",
      "505 [D loss: 0.692088] [G loss: -0.011440]\n",
      "506 [D loss: 1.129244] [G loss: -0.002671]\n",
      "507 [D loss: 1.167344] [G loss: -0.002300]\n",
      "508 [D loss: 0.614739] [G loss: -0.006478]\n",
      "509 [D loss: 1.644627] [G loss: -0.130736]\n",
      "510 [D loss: 0.103277] [G loss: -0.016933]\n",
      "511 [D loss: 1.678383] [G loss: -0.028556]\n",
      "512 [D loss: 0.844284] [G loss: -0.000939]\n",
      "513 [D loss: 0.653553] [G loss: -0.000917]\n",
      "514 [D loss: 0.159772] [G loss: -0.001773]\n",
      "515 [D loss: 1.193692] [G loss: -0.001553]\n",
      "516 [D loss: 1.492625] [G loss: -0.025729]\n",
      "517 [D loss: 1.840415] [G loss: -0.010274]\n",
      "518 [D loss: 0.960868] [G loss: -0.001238]\n",
      "519 [D loss: 1.823159] [G loss: -0.002756]\n",
      "520 [D loss: 0.064478] [G loss: -0.001270]\n",
      "521 [D loss: 0.257639] [G loss: -0.001175]\n",
      "522 [D loss: 0.911791] [G loss: -0.015741]\n",
      "523 [D loss: 2.079484] [G loss: -0.002197]\n",
      "524 [D loss: 1.184059] [G loss: -0.012697]\n",
      "525 [D loss: 0.112344] [G loss: -0.000169]\n",
      "526 [D loss: 2.039029] [G loss: -0.000137]\n",
      "527 [D loss: 0.676432] [G loss: -0.000584]\n",
      "528 [D loss: -0.261606] [G loss: -0.001712]\n",
      "529 [D loss: 0.827612] [G loss: -0.000085]\n",
      "530 [D loss: 0.160695] [G loss: -0.006893]\n",
      "531 [D loss: 1.039702] [G loss: -0.001433]\n",
      "532 [D loss: 1.424739] [G loss: -0.014306]\n",
      "533 [D loss: 13.437600] [G loss: -0.000681]\n",
      "534 [D loss: 0.052809] [G loss: -0.004665]\n",
      "535 [D loss: -0.379839] [G loss: -0.002571]\n",
      "536 [D loss: 0.322279] [G loss: -0.000759]\n",
      "537 [D loss: -0.035540] [G loss: -0.007065]\n",
      "538 [D loss: 1.658399] [G loss: -0.000375]\n",
      "539 [D loss: -0.234957] [G loss: -0.000562]\n",
      "540 [D loss: 1.109157] [G loss: -0.000616]\n",
      "541 [D loss: 0.114965] [G loss: -0.007717]\n",
      "542 [D loss: 0.502510] [G loss: -0.001287]\n",
      "543 [D loss: 0.177750] [G loss: -0.000644]\n",
      "544 [D loss: 1.124909] [G loss: -0.003700]\n",
      "545 [D loss: 1.120482] [G loss: -0.000467]\n",
      "546 [D loss: -0.329658] [G loss: -0.003293]\n",
      "547 [D loss: 2.787006] [G loss: -0.000010]\n",
      "548 [D loss: 0.228129] [G loss: -0.004433]\n",
      "549 [D loss: 0.022714] [G loss: -0.008084]\n",
      "550 [D loss: 0.949902] [G loss: -0.000946]\n",
      "551 [D loss: 0.225109] [G loss: -0.002623]\n",
      "552 [D loss: 0.497067] [G loss: -0.000636]\n",
      "553 [D loss: 0.262249] [G loss: -0.006447]\n",
      "554 [D loss: -0.063393] [G loss: -0.053308]\n",
      "555 [D loss: 6.370517] [G loss: -0.000904]\n",
      "556 [D loss: -0.256195] [G loss: -0.006266]\n",
      "557 [D loss: 0.294890] [G loss: -0.004335]\n",
      "558 [D loss: -0.096217] [G loss: -0.000285]\n",
      "559 [D loss: 0.728482] [G loss: -0.001105]\n",
      "560 [D loss: 0.051799] [G loss: -0.002081]\n",
      "561 [D loss: 0.425023] [G loss: -0.000822]\n",
      "562 [D loss: 1.784823] [G loss: -0.000836]\n",
      "563 [D loss: 0.171022] [G loss: -0.018490]\n",
      "564 [D loss: -0.236215] [G loss: -0.000530]\n",
      "565 [D loss: 0.021614] [G loss: -0.000441]\n",
      "566 [D loss: 1.228242] [G loss: -0.001320]\n",
      "567 [D loss: 2.362412] [G loss: -0.000050]\n",
      "568 [D loss: 0.971475] [G loss: -0.001319]\n",
      "569 [D loss: 2.840894] [G loss: -0.000347]\n",
      "570 [D loss: 0.758603] [G loss: -0.001897]\n",
      "571 [D loss: 1.520474] [G loss: -0.060739]\n",
      "572 [D loss: -0.150942] [G loss: -0.000666]\n",
      "573 [D loss: 0.870356] [G loss: -0.036820]\n",
      "574 [D loss: -0.221933] [G loss: -0.002668]\n",
      "575 [D loss: 0.653972] [G loss: -0.000905]\n",
      "576 [D loss: 1.038881] [G loss: -0.000148]\n",
      "577 [D loss: 1.080280] [G loss: -0.000264]\n",
      "578 [D loss: 0.128024] [G loss: -0.057876]\n",
      "579 [D loss: 0.412200] [G loss: -0.003702]\n",
      "580 [D loss: 0.116858] [G loss: -0.014601]\n",
      "581 [D loss: -0.254575] [G loss: -0.131207]\n",
      "582 [D loss: 2.077738] [G loss: -0.000185]\n",
      "583 [D loss: 1.059500] [G loss: -0.000893]\n",
      "584 [D loss: 1.903771] [G loss: -0.017352]\n",
      "585 [D loss: 0.013143] [G loss: -0.023362]\n",
      "586 [D loss: 0.439758] [G loss: -0.000194]\n",
      "587 [D loss: 0.652105] [G loss: -0.000347]\n",
      "588 [D loss: 0.077847] [G loss: -0.002708]\n",
      "589 [D loss: 0.061391] [G loss: -0.010881]\n",
      "590 [D loss: 0.156968] [G loss: -0.010556]\n",
      "591 [D loss: 0.361554] [G loss: -0.021832]\n",
      "592 [D loss: 2.451590] [G loss: -0.026854]\n",
      "593 [D loss: -0.411134] [G loss: -0.005156]\n",
      "594 [D loss: -0.194876] [G loss: -0.016861]\n",
      "595 [D loss: 0.232425] [G loss: -0.053659]\n",
      "596 [D loss: -0.172596] [G loss: -0.066606]\n",
      "597 [D loss: 0.189985] [G loss: -0.006747]\n",
      "598 [D loss: 0.602747] [G loss: -0.132199]\n",
      "599 [D loss: -0.443127] [G loss: -0.163541]\n",
      "600 [D loss: 0.126087] [G loss: -0.260316]\n",
      "601 [D loss: 1.808074] [G loss: -0.019603]\n",
      "602 [D loss: -0.333245] [G loss: -0.000427]\n",
      "603 [D loss: 1.500366] [G loss: -0.004045]\n",
      "604 [D loss: 1.107759] [G loss: -0.003521]\n",
      "605 [D loss: 2.212345] [G loss: -0.146191]\n",
      "606 [D loss: -0.161432] [G loss: -0.065951]\n",
      "607 [D loss: 0.304392] [G loss: -0.007060]\n",
      "608 [D loss: -0.193496] [G loss: -0.002731]\n",
      "609 [D loss: -0.288096] [G loss: -0.007953]\n",
      "610 [D loss: 0.690099] [G loss: -0.000725]\n",
      "611 [D loss: 0.389451] [G loss: -0.000863]\n",
      "612 [D loss: 0.091266] [G loss: -0.020468]\n",
      "613 [D loss: 2.519509] [G loss: -0.141697]\n",
      "614 [D loss: -0.199085] [G loss: -0.640249]\n",
      "615 [D loss: 2.614480] [G loss: -0.105733]\n",
      "616 [D loss: 0.525917] [G loss: -0.048766]\n",
      "617 [D loss: -0.261062] [G loss: -0.004364]\n",
      "618 [D loss: 0.471479] [G loss: -0.003636]\n",
      "619 [D loss: 2.506766] [G loss: -0.000369]\n",
      "620 [D loss: 0.156125] [G loss: -0.016141]\n",
      "621 [D loss: -0.274699] [G loss: -0.030813]\n",
      "622 [D loss: -0.267710] [G loss: -0.000811]\n",
      "623 [D loss: -0.284667] [G loss: -0.020171]\n",
      "624 [D loss: -0.272282] [G loss: -0.001011]\n",
      "625 [D loss: -0.266100] [G loss: -0.012868]\n",
      "626 [D loss: 0.506533] [G loss: -0.000907]\n",
      "627 [D loss: 0.413482] [G loss: -0.005761]\n",
      "628 [D loss: 0.567437] [G loss: -0.004382]\n",
      "629 [D loss: 2.196237] [G loss: -0.000328]\n",
      "630 [D loss: 1.168888] [G loss: -0.013749]\n",
      "631 [D loss: 0.460528] [G loss: -0.011034]\n",
      "632 [D loss: -0.222416] [G loss: -0.001576]\n",
      "633 [D loss: 0.205587] [G loss: -0.002558]\n",
      "634 [D loss: 0.002940] [G loss: -0.003713]\n",
      "635 [D loss: 1.230606] [G loss: -0.147204]\n",
      "636 [D loss: 0.306884] [G loss: -0.023761]\n",
      "637 [D loss: -0.020499] [G loss: -0.041853]\n",
      "638 [D loss: 0.266288] [G loss: -0.002308]\n",
      "639 [D loss: 5.991845] [G loss: -0.000098]\n",
      "640 [D loss: -0.244285] [G loss: -0.000729]\n",
      "641 [D loss: 0.053704] [G loss: -0.014051]\n",
      "642 [D loss: -0.501523] [G loss: -0.001016]\n",
      "643 [D loss: -0.305475] [G loss: -0.003714]\n",
      "644 [D loss: 1.049577] [G loss: -0.001009]\n",
      "645 [D loss: 1.352651] [G loss: -0.000805]\n",
      "646 [D loss: 1.080204] [G loss: -0.011396]\n",
      "647 [D loss: -0.584965] [G loss: -0.000513]\n",
      "648 [D loss: 0.561578] [G loss: -0.000240]\n",
      "649 [D loss: -0.228709] [G loss: -0.002977]\n",
      "650 [D loss: -0.367499] [G loss: -0.002042]\n",
      "651 [D loss: -0.471708] [G loss: -0.000294]\n",
      "652 [D loss: -0.586431] [G loss: -0.002426]\n",
      "653 [D loss: 3.558902] [G loss: -0.002208]\n",
      "654 [D loss: -0.491086] [G loss: -0.000296]\n",
      "655 [D loss: 0.595294] [G loss: -0.003832]\n",
      "656 [D loss: -0.094832] [G loss: -0.004136]\n",
      "657 [D loss: 0.881599] [G loss: -0.002802]\n",
      "658 [D loss: 0.563086] [G loss: -0.000307]\n",
      "659 [D loss: -0.457510] [G loss: -0.009476]\n",
      "660 [D loss: 0.916737] [G loss: -0.043109]\n",
      "661 [D loss: 1.133323] [G loss: -0.005484]\n",
      "662 [D loss: 3.233945] [G loss: -0.004313]\n",
      "663 [D loss: 5.882743] [G loss: -0.002335]\n",
      "664 [D loss: 0.153117] [G loss: -0.003290]\n",
      "665 [D loss: -0.459074] [G loss: -0.002712]\n",
      "666 [D loss: -0.419122] [G loss: -0.001871]\n",
      "667 [D loss: -0.345446] [G loss: -0.011381]\n",
      "668 [D loss: 3.040686] [G loss: -0.001313]\n",
      "669 [D loss: 2.984037] [G loss: -0.002574]\n",
      "670 [D loss: -0.472135] [G loss: -0.001062]\n",
      "671 [D loss: 4.180929] [G loss: -0.000123]\n",
      "672 [D loss: -0.446580] [G loss: -0.001662]\n",
      "673 [D loss: 0.200740] [G loss: -0.017595]\n",
      "674 [D loss: -0.391376] [G loss: -0.014043]\n",
      "675 [D loss: -0.390662] [G loss: -0.000296]\n",
      "676 [D loss: -0.364403] [G loss: -0.000870]\n",
      "677 [D loss: 0.243697] [G loss: -0.000965]\n",
      "678 [D loss: 0.708317] [G loss: -0.009007]\n",
      "679 [D loss: -0.359045] [G loss: -0.001146]\n",
      "680 [D loss: 1.713852] [G loss: -0.000124]\n",
      "681 [D loss: -0.305259] [G loss: -0.000722]\n",
      "682 [D loss: 0.337024] [G loss: -0.000294]\n",
      "683 [D loss: 1.557111] [G loss: -0.006310]\n",
      "684 [D loss: 7.260160] [G loss: -0.080008]\n",
      "685 [D loss: -0.704143] [G loss: -0.002295]\n",
      "686 [D loss: 0.637446] [G loss: -0.003941]\n",
      "687 [D loss: -0.366342] [G loss: -0.000500]\n",
      "688 [D loss: -0.546560] [G loss: -0.000429]\n",
      "689 [D loss: 1.115891] [G loss: -0.000256]\n",
      "690 [D loss: 0.496752] [G loss: -0.005419]\n",
      "691 [D loss: 0.419186] [G loss: -0.000511]\n",
      "692 [D loss: 3.678963] [G loss: -0.001165]\n",
      "693 [D loss: -0.497261] [G loss: -0.003501]\n",
      "694 [D loss: 2.006506] [G loss: -0.005334]\n",
      "695 [D loss: -0.079139] [G loss: -0.023352]\n",
      "696 [D loss: -0.459300] [G loss: -0.012254]\n",
      "697 [D loss: 2.472240] [G loss: -0.003014]\n",
      "698 [D loss: 3.366149] [G loss: -0.004926]\n",
      "699 [D loss: 0.042761] [G loss: -0.005859]\n",
      "700 [D loss: 0.177826] [G loss: -0.000052]\n",
      "701 [D loss: -0.234474] [G loss: -0.000798]\n",
      "702 [D loss: 0.421026] [G loss: -0.001326]\n",
      "703 [D loss: -0.326375] [G loss: -0.002244]\n",
      "704 [D loss: 0.184983] [G loss: -0.001748]\n",
      "705 [D loss: 0.145748] [G loss: -0.004815]\n",
      "706 [D loss: 2.434404] [G loss: -0.000199]\n",
      "707 [D loss: 0.364512] [G loss: -0.002249]\n",
      "708 [D loss: -0.393139] [G loss: -0.002241]\n",
      "709 [D loss: 1.761625] [G loss: -0.010098]\n",
      "710 [D loss: 0.933931] [G loss: -0.001547]\n",
      "711 [D loss: 1.365656] [G loss: -0.000130]\n",
      "712 [D loss: 0.563062] [G loss: -0.001169]\n",
      "713 [D loss: 0.646849] [G loss: -0.014543]\n",
      "714 [D loss: -0.549223] [G loss: -0.000593]\n",
      "715 [D loss: -0.341217] [G loss: -0.001157]\n",
      "716 [D loss: 0.863862] [G loss: -0.032989]\n",
      "717 [D loss: -0.129383] [G loss: -0.055938]\n",
      "718 [D loss: -0.182145] [G loss: -0.001458]\n",
      "719 [D loss: -0.164997] [G loss: -0.000402]\n",
      "720 [D loss: -0.449474] [G loss: -0.001525]\n",
      "721 [D loss: 0.886284] [G loss: -0.000553]\n",
      "722 [D loss: 0.570541] [G loss: -0.000524]\n",
      "723 [D loss: 0.505308] [G loss: -0.007116]\n",
      "724 [D loss: 1.299200] [G loss: -0.000224]\n",
      "725 [D loss: -0.586370] [G loss: -0.007453]\n",
      "726 [D loss: 1.965593] [G loss: -0.018430]\n",
      "727 [D loss: -0.281069] [G loss: -0.002720]\n",
      "728 [D loss: -0.363606] [G loss: -0.006749]\n",
      "729 [D loss: 1.897530] [G loss: -0.000696]\n",
      "730 [D loss: -0.736938] [G loss: -0.018390]\n",
      "731 [D loss: -0.575792] [G loss: -0.003489]\n",
      "732 [D loss: -0.088526] [G loss: -0.003165]\n",
      "733 [D loss: -0.393178] [G loss: -0.001367]\n",
      "734 [D loss: 0.064953] [G loss: -0.001046]\n",
      "735 [D loss: 0.325372] [G loss: -0.036602]\n",
      "736 [D loss: -0.233116] [G loss: -0.008779]\n",
      "737 [D loss: -0.144780] [G loss: -0.003557]\n",
      "738 [D loss: -0.072285] [G loss: -0.000632]\n",
      "739 [D loss: 7.141177] [G loss: -0.867539]\n",
      "740 [D loss: -0.353865] [G loss: -0.114124]\n",
      "741 [D loss: -0.234650] [G loss: -0.066173]\n",
      "742 [D loss: 0.096274] [G loss: -0.000218]\n",
      "743 [D loss: 0.066941] [G loss: -0.005705]\n",
      "744 [D loss: 0.383215] [G loss: -0.000108]\n",
      "745 [D loss: 2.303607] [G loss: -0.002590]\n",
      "746 [D loss: -0.510894] [G loss: -0.000116]\n",
      "747 [D loss: 0.079682] [G loss: -0.000140]\n",
      "748 [D loss: 0.934312] [G loss: -0.000403]\n",
      "749 [D loss: -0.608222] [G loss: -0.000219]\n",
      "750 [D loss: -0.629765] [G loss: -0.000707]\n",
      "751 [D loss: -0.592570] [G loss: -0.001915]\n",
      "752 [D loss: -0.355699] [G loss: -0.000527]\n",
      "753 [D loss: 0.283677] [G loss: -0.000247]\n",
      "754 [D loss: -0.402394] [G loss: -0.000757]\n",
      "755 [D loss: -0.588370] [G loss: -0.000991]\n",
      "756 [D loss: -0.254773] [G loss: -0.000165]\n",
      "757 [D loss: 1.740538] [G loss: -0.001628]\n",
      "758 [D loss: -0.011455] [G loss: -0.011605]\n",
      "759 [D loss: 2.926607] [G loss: -0.000039]\n",
      "760 [D loss: -0.661839] [G loss: -0.000087]\n",
      "761 [D loss: -0.641816] [G loss: -0.000331]\n",
      "762 [D loss: 3.148724] [G loss: -0.000145]\n",
      "763 [D loss: -0.487242] [G loss: -0.000417]\n",
      "764 [D loss: 0.522830] [G loss: -0.001317]\n",
      "765 [D loss: 0.486245] [G loss: -0.003204]\n",
      "766 [D loss: 1.445196] [G loss: -0.002196]\n",
      "767 [D loss: -0.799601] [G loss: -0.001586]\n",
      "768 [D loss: -0.640535] [G loss: -0.006218]\n",
      "769 [D loss: 1.423335] [G loss: -0.000397]\n",
      "770 [D loss: -0.011022] [G loss: -0.000652]\n",
      "771 [D loss: 0.629069] [G loss: -0.005507]\n",
      "772 [D loss: 0.413669] [G loss: -0.010994]\n",
      "773 [D loss: -0.276079] [G loss: -0.002087]\n",
      "774 [D loss: 0.043122] [G loss: -0.380004]\n",
      "775 [D loss: 0.406808] [G loss: -0.000376]\n",
      "776 [D loss: 0.573681] [G loss: -0.000663]\n",
      "777 [D loss: -0.478367] [G loss: -0.000012]\n",
      "778 [D loss: -0.344495] [G loss: -0.002318]\n",
      "779 [D loss: 2.583063] [G loss: -0.000142]\n",
      "780 [D loss: -0.140368] [G loss: -0.000428]\n",
      "781 [D loss: 0.821484] [G loss: -0.001429]\n",
      "782 [D loss: -0.769461] [G loss: -0.000989]\n",
      "783 [D loss: 1.566816] [G loss: -0.001261]\n",
      "784 [D loss: 4.293178] [G loss: -0.096600]\n",
      "785 [D loss: -0.654199] [G loss: -0.000419]\n",
      "786 [D loss: -0.426730] [G loss: -0.002548]\n",
      "787 [D loss: 1.079868] [G loss: -0.153628]\n",
      "788 [D loss: 1.074200] [G loss: -0.001269]\n",
      "789 [D loss: -0.728345] [G loss: -0.003720]\n",
      "790 [D loss: -0.647853] [G loss: -0.000221]\n",
      "791 [D loss: 0.596350] [G loss: -0.018630]\n",
      "792 [D loss: -0.567734] [G loss: -0.001358]\n",
      "793 [D loss: 6.339552] [G loss: -0.048609]\n",
      "794 [D loss: 2.360480] [G loss: -0.000030]\n",
      "795 [D loss: 1.990387] [G loss: -0.001090]\n",
      "796 [D loss: -0.379760] [G loss: -0.000202]\n",
      "797 [D loss: 1.185925] [G loss: -0.001872]\n",
      "798 [D loss: -0.358755] [G loss: -0.000820]\n",
      "799 [D loss: 1.936421] [G loss: -0.007432]\n",
      "800 [D loss: -0.642994] [G loss: -0.000494]\n",
      "801 [D loss: 2.393685] [G loss: -0.082676]\n",
      "802 [D loss: -0.054301] [G loss: -0.004550]\n",
      "803 [D loss: 0.360438] [G loss: -0.000706]\n",
      "804 [D loss: -0.538705] [G loss: -0.000042]\n",
      "805 [D loss: -0.409209] [G loss: -0.000090]\n",
      "806 [D loss: -0.774805] [G loss: -0.000095]\n",
      "807 [D loss: -0.212105] [G loss: -0.000117]\n",
      "808 [D loss: 1.771455] [G loss: -0.001914]\n",
      "809 [D loss: -0.449304] [G loss: -0.001139]\n",
      "810 [D loss: -0.222202] [G loss: -0.000102]\n",
      "811 [D loss: 0.493175] [G loss: -0.015869]\n",
      "812 [D loss: -0.605959] [G loss: -0.006518]\n",
      "813 [D loss: -0.577805] [G loss: -0.000003]\n",
      "814 [D loss: -0.089855] [G loss: -0.000005]\n",
      "815 [D loss: -0.688388] [G loss: -0.000010]\n",
      "816 [D loss: -0.648787] [G loss: -0.000730]\n",
      "817 [D loss: -0.482919] [G loss: -0.000466]\n",
      "818 [D loss: -0.205642] [G loss: -0.000003]\n",
      "819 [D loss: -0.488241] [G loss: -0.000080]\n",
      "820 [D loss: 0.225694] [G loss: -0.000009]\n",
      "821 [D loss: -0.374763] [G loss: -0.000054]\n",
      "822 [D loss: 2.675891] [G loss: -0.147535]\n",
      "823 [D loss: -0.684562] [G loss: -0.000562]\n",
      "824 [D loss: -0.071878] [G loss: -0.000051]\n",
      "825 [D loss: -0.573398] [G loss: -0.000834]\n",
      "826 [D loss: -0.435299] [G loss: -0.000130]\n",
      "827 [D loss: -0.024510] [G loss: -0.000680]\n",
      "828 [D loss: -0.306615] [G loss: -0.037511]\n",
      "829 [D loss: -0.360414] [G loss: -0.000367]\n",
      "830 [D loss: 5.355168] [G loss: -0.000015]\n",
      "831 [D loss: -0.540821] [G loss: -0.005642]\n",
      "832 [D loss: 0.008613] [G loss: -0.005446]\n",
      "833 [D loss: -0.468977] [G loss: -0.002708]\n",
      "834 [D loss: -0.414568] [G loss: -0.009901]\n",
      "835 [D loss: -0.679090] [G loss: -0.000441]\n",
      "836 [D loss: -0.526828] [G loss: -0.000090]\n",
      "837 [D loss: 3.571488] [G loss: -0.002694]\n",
      "838 [D loss: -0.426918] [G loss: -0.000317]\n",
      "839 [D loss: -0.667690] [G loss: -0.000566]\n",
      "840 [D loss: -0.517865] [G loss: -0.000586]\n",
      "841 [D loss: -0.679276] [G loss: -0.001138]\n",
      "842 [D loss: 0.708900] [G loss: -0.000173]\n",
      "843 [D loss: -0.678565] [G loss: -0.000885]\n",
      "844 [D loss: 3.266598] [G loss: -0.000005]\n",
      "845 [D loss: -0.184811] [G loss: -0.002547]\n",
      "846 [D loss: -0.726997] [G loss: -0.000866]\n",
      "847 [D loss: -0.488140] [G loss: -0.000132]\n",
      "848 [D loss: -0.619860] [G loss: -0.000142]\n",
      "849 [D loss: 0.383347] [G loss: -0.000365]\n",
      "850 [D loss: 7.106370] [G loss: -0.002053]\n",
      "851 [D loss: 0.081873] [G loss: -0.000409]\n",
      "852 [D loss: -0.275580] [G loss: -0.002455]\n",
      "853 [D loss: 3.716949] [G loss: -0.004940]\n",
      "854 [D loss: 0.212820] [G loss: -0.000097]\n",
      "855 [D loss: -0.337484] [G loss: -0.000972]\n",
      "856 [D loss: 0.524028] [G loss: -0.006837]\n",
      "857 [D loss: -0.341516] [G loss: -0.000128]\n",
      "858 [D loss: -0.107412] [G loss: -0.000243]\n",
      "859 [D loss: -0.099956] [G loss: -0.002401]\n",
      "860 [D loss: -0.130569] [G loss: -0.000082]\n",
      "861 [D loss: -0.017197] [G loss: -0.006959]\n",
      "862 [D loss: -0.424538] [G loss: -0.014238]\n",
      "863 [D loss: 1.299921] [G loss: -0.060227]\n",
      "864 [D loss: -0.418915] [G loss: -0.000590]\n",
      "865 [D loss: 2.289256] [G loss: -0.001037]\n",
      "866 [D loss: -0.539612] [G loss: -0.000041]\n",
      "867 [D loss: 0.602767] [G loss: -0.000255]\n",
      "868 [D loss: 0.977266] [G loss: -0.000561]\n",
      "869 [D loss: 0.386184] [G loss: -0.000655]\n",
      "870 [D loss: -0.160545] [G loss: -0.000082]\n",
      "871 [D loss: -0.690333] [G loss: -0.000348]\n",
      "872 [D loss: -0.541321] [G loss: -0.000117]\n",
      "873 [D loss: -0.559444] [G loss: -0.000142]\n",
      "874 [D loss: 5.501173] [G loss: -0.000001]\n",
      "875 [D loss: 0.327197] [G loss: -0.000046]\n",
      "876 [D loss: 0.330429] [G loss: -0.000140]\n",
      "877 [D loss: 0.081400] [G loss: -0.000117]\n",
      "878 [D loss: -0.447667] [G loss: -0.000028]\n",
      "879 [D loss: -0.194523] [G loss: -0.005247]\n",
      "880 [D loss: 2.866472] [G loss: -0.000007]\n",
      "881 [D loss: 0.712262] [G loss: -0.000097]\n",
      "882 [D loss: 0.364278] [G loss: -0.000204]\n",
      "883 [D loss: -0.635063] [G loss: -0.000732]\n",
      "884 [D loss: -0.712335] [G loss: -0.006864]\n",
      "885 [D loss: 0.989316] [G loss: -0.000002]\n",
      "886 [D loss: 2.248144] [G loss: -0.000091]\n",
      "887 [D loss: -0.016470] [G loss: -0.000159]\n",
      "888 [D loss: -0.413751] [G loss: -0.000896]\n",
      "889 [D loss: 0.692765] [G loss: -0.000007]\n",
      "890 [D loss: 1.560411] [G loss: -0.000224]\n",
      "891 [D loss: 0.471067] [G loss: -0.000097]\n",
      "892 [D loss: -0.351534] [G loss: -0.000102]\n",
      "893 [D loss: 0.445762] [G loss: -0.003817]\n",
      "894 [D loss: -0.449065] [G loss: -0.002061]\n",
      "895 [D loss: 0.259215] [G loss: -0.003276]\n",
      "896 [D loss: -0.725458] [G loss: -0.000191]\n",
      "897 [D loss: 0.966494] [G loss: -0.000011]\n",
      "898 [D loss: -0.365272] [G loss: -0.001358]\n",
      "899 [D loss: 0.200504] [G loss: -0.000176]\n",
      "900 [D loss: 6.303933] [G loss: -0.000041]\n",
      "901 [D loss: -0.498500] [G loss: -0.000009]\n",
      "902 [D loss: -0.643713] [G loss: -0.000016]\n",
      "903 [D loss: 0.091063] [G loss: -0.000271]\n",
      "904 [D loss: -0.716787] [G loss: -0.001829]\n",
      "905 [D loss: -0.502762] [G loss: -0.000252]\n",
      "906 [D loss: 0.296354] [G loss: -0.001618]\n",
      "907 [D loss: 0.339745] [G loss: -0.000393]\n",
      "908 [D loss: -0.502840] [G loss: -0.000317]\n",
      "909 [D loss: 0.216039] [G loss: -0.000321]\n",
      "910 [D loss: -0.243753] [G loss: -0.000063]\n",
      "911 [D loss: -0.722878] [G loss: -0.000476]\n",
      "912 [D loss: 4.409880] [G loss: -0.086128]\n",
      "913 [D loss: 1.422744] [G loss: -0.065619]\n",
      "914 [D loss: -0.446695] [G loss: -0.003314]\n",
      "915 [D loss: -0.613357] [G loss: -0.006758]\n",
      "916 [D loss: 0.031597] [G loss: -0.002186]\n",
      "917 [D loss: -0.453123] [G loss: -0.000229]\n",
      "918 [D loss: -0.780648] [G loss: -0.002122]\n",
      "919 [D loss: -0.057715] [G loss: -0.006023]\n",
      "920 [D loss: -0.493111] [G loss: -0.003432]\n",
      "921 [D loss: -0.668836] [G loss: -0.001479]\n",
      "922 [D loss: -0.528287] [G loss: -0.000217]\n",
      "923 [D loss: 0.141500] [G loss: -0.003082]\n",
      "924 [D loss: -0.583768] [G loss: -0.005311]\n",
      "925 [D loss: -0.309231] [G loss: -0.000013]\n",
      "926 [D loss: -0.077009] [G loss: -0.000879]\n",
      "927 [D loss: -0.762799] [G loss: -0.000082]\n",
      "928 [D loss: 0.310997] [G loss: -0.000001]\n",
      "929 [D loss: 0.057191] [G loss: -0.000039]\n",
      "930 [D loss: -0.582621] [G loss: -0.000775]\n",
      "931 [D loss: -0.582663] [G loss: -0.000035]\n",
      "932 [D loss: -0.655025] [G loss: -0.000257]\n",
      "933 [D loss: 0.380981] [G loss: -0.000535]\n",
      "934 [D loss: -0.645816] [G loss: -0.000130]\n",
      "935 [D loss: -0.660616] [G loss: -0.000082]\n",
      "936 [D loss: -0.251178] [G loss: -0.000364]\n",
      "937 [D loss: -0.768492] [G loss: -0.000103]\n",
      "938 [D loss: 1.580820] [G loss: -0.000009]\n",
      "939 [D loss: -0.861659] [G loss: -0.000095]\n",
      "940 [D loss: 5.488337] [G loss: -0.009216]\n",
      "941 [D loss: 1.934966] [G loss: -0.000935]\n",
      "942 [D loss: 1.671876] [G loss: -0.000048]\n",
      "943 [D loss: -0.137630] [G loss: -0.000832]\n",
      "944 [D loss: -0.732077] [G loss: -0.000890]\n",
      "945 [D loss: -0.712195] [G loss: -0.000238]\n",
      "946 [D loss: -0.477199] [G loss: -0.001878]\n",
      "947 [D loss: -0.258367] [G loss: -0.000196]\n",
      "948 [D loss: -0.243788] [G loss: -0.000008]\n",
      "949 [D loss: -0.480139] [G loss: -0.008921]\n",
      "950 [D loss: 1.258183] [G loss: -0.000047]\n",
      "951 [D loss: -0.280080] [G loss: -0.000167]\n",
      "952 [D loss: -0.400354] [G loss: -0.002387]\n",
      "953 [D loss: 1.850506] [G loss: -0.000040]\n",
      "954 [D loss: 0.836516] [G loss: -0.000117]\n",
      "955 [D loss: 0.721438] [G loss: -0.001772]\n",
      "956 [D loss: -0.555396] [G loss: -0.000033]\n",
      "957 [D loss: 1.408534] [G loss: -0.000014]\n",
      "958 [D loss: 0.050523] [G loss: -0.000141]\n",
      "959 [D loss: -0.715160] [G loss: -0.000218]\n",
      "960 [D loss: -0.323041] [G loss: -0.001522]\n",
      "961 [D loss: -0.515864] [G loss: -0.000391]\n",
      "962 [D loss: 0.384494] [G loss: -0.005193]\n",
      "963 [D loss: -0.844973] [G loss: -0.001531]\n",
      "964 [D loss: -0.403775] [G loss: -0.000307]\n",
      "965 [D loss: -0.110394] [G loss: -0.001298]\n",
      "966 [D loss: -0.010282] [G loss: -0.000099]\n",
      "967 [D loss: -0.307545] [G loss: -0.000242]\n",
      "968 [D loss: 0.659734] [G loss: -0.008997]\n",
      "969 [D loss: 0.540585] [G loss: -0.068581]\n",
      "970 [D loss: -0.343363] [G loss: -0.001328]\n",
      "971 [D loss: -0.373751] [G loss: -0.001803]\n",
      "972 [D loss: -0.701576] [G loss: -0.007413]\n",
      "973 [D loss: 2.753182] [G loss: -0.008514]\n",
      "974 [D loss: -0.602480] [G loss: -0.000106]\n",
      "975 [D loss: -0.310997] [G loss: -0.000394]\n",
      "976 [D loss: -0.574549] [G loss: -0.001749]\n",
      "977 [D loss: 0.556225] [G loss: -0.000396]\n",
      "978 [D loss: -0.398744] [G loss: -0.008249]\n",
      "979 [D loss: -0.413194] [G loss: -0.000250]\n",
      "980 [D loss: 4.449808] [G loss: -0.000005]\n",
      "981 [D loss: 0.203209] [G loss: -0.003234]\n",
      "982 [D loss: 0.116073] [G loss: -0.000194]\n",
      "983 [D loss: -0.735055] [G loss: -0.001622]\n",
      "984 [D loss: -0.527167] [G loss: -0.113201]\n",
      "985 [D loss: -0.254506] [G loss: -0.001498]\n",
      "986 [D loss: 0.532037] [G loss: -0.014297]\n",
      "987 [D loss: 0.167121] [G loss: -0.004163]\n",
      "988 [D loss: -0.520026] [G loss: -0.179497]\n",
      "989 [D loss: -0.382310] [G loss: -0.792322]\n",
      "990 [D loss: 1.773761] [G loss: -0.017784]\n",
      "991 [D loss: -0.702799] [G loss: -0.006064]\n",
      "992 [D loss: 1.600957] [G loss: -0.000315]\n",
      "993 [D loss: 2.186756] [G loss: -0.001468]\n",
      "994 [D loss: -0.677275] [G loss: -0.036077]\n",
      "995 [D loss: 3.052508] [G loss: -0.074866]\n",
      "996 [D loss: 1.919070] [G loss: -0.085182]\n",
      "997 [D loss: -0.103524] [G loss: -0.228033]\n",
      "998 [D loss: -0.060518] [G loss: -0.044475]\n",
      "999 [D loss: 0.321166] [G loss: -0.031963]\n",
      "1000 [D loss: -0.689143] [G loss: -0.001479]\n",
      "1001 [D loss: 1.145235] [G loss: -0.647313]\n",
      "1002 [D loss: 10.664651] [G loss: -0.964979]\n",
      "1003 [D loss: -0.170439] [G loss: -0.709730]\n",
      "1004 [D loss: -0.530623] [G loss: -0.024907]\n",
      "1005 [D loss: -0.426802] [G loss: -0.066369]\n",
      "1006 [D loss: 2.050044] [G loss: -0.028270]\n",
      "1007 [D loss: 1.129600] [G loss: -0.461623]\n",
      "1008 [D loss: -0.145757] [G loss: -0.319551]\n",
      "1009 [D loss: -0.652395] [G loss: -0.095843]\n",
      "1010 [D loss: -0.267164] [G loss: -0.051430]\n",
      "1011 [D loss: 0.449294] [G loss: -0.092436]\n",
      "1012 [D loss: 0.087220] [G loss: -0.030030]\n",
      "1013 [D loss: -0.865499] [G loss: -0.012534]\n",
      "1014 [D loss: 0.144669] [G loss: -0.000077]\n",
      "1015 [D loss: 0.175677] [G loss: -0.013399]\n",
      "1016 [D loss: -0.116708] [G loss: -0.013850]\n",
      "1017 [D loss: 2.799599] [G loss: -0.014564]\n",
      "1018 [D loss: -0.509365] [G loss: -0.060230]\n",
      "1019 [D loss: -0.381650] [G loss: -0.011609]\n",
      "1020 [D loss: 0.136063] [G loss: -0.028442]\n",
      "1021 [D loss: 0.342994] [G loss: -0.027691]\n",
      "1022 [D loss: -0.836948] [G loss: -0.002807]\n",
      "1023 [D loss: -0.175323] [G loss: -0.002339]\n",
      "1024 [D loss: 0.763613] [G loss: -0.000721]\n",
      "1025 [D loss: -0.751793] [G loss: -0.001096]\n",
      "1026 [D loss: -0.621171] [G loss: -0.005410]\n",
      "1027 [D loss: -0.652146] [G loss: -0.030326]\n",
      "1028 [D loss: 0.034962] [G loss: -0.000337]\n",
      "1029 [D loss: -0.574593] [G loss: -0.013156]\n",
      "1030 [D loss: 0.448985] [G loss: -0.000847]\n",
      "1031 [D loss: -0.611879] [G loss: -0.022951]\n",
      "1032 [D loss: -0.419358] [G loss: -0.038097]\n",
      "1033 [D loss: 1.371615] [G loss: -0.032882]\n",
      "1034 [D loss: 0.190330] [G loss: -0.002455]\n",
      "1035 [D loss: -0.167395] [G loss: -0.040682]\n",
      "1036 [D loss: 0.532745] [G loss: -0.041018]\n",
      "1037 [D loss: 2.256361] [G loss: -0.001914]\n",
      "1038 [D loss: 0.285373] [G loss: -0.000922]\n",
      "1039 [D loss: -0.611812] [G loss: -0.000187]\n",
      "1040 [D loss: -0.281301] [G loss: -0.010171]\n",
      "1041 [D loss: 0.586331] [G loss: -0.000178]\n",
      "1042 [D loss: -0.143759] [G loss: -0.001392]\n",
      "1043 [D loss: -0.566517] [G loss: -0.002090]\n",
      "1044 [D loss: -0.570107] [G loss: -0.000175]\n",
      "1045 [D loss: -0.282315] [G loss: -0.001140]\n",
      "1046 [D loss: 1.933222] [G loss: -0.001529]\n",
      "1047 [D loss: -0.003474] [G loss: -0.001729]\n",
      "1048 [D loss: -0.242843] [G loss: -0.003449]\n",
      "1049 [D loss: 0.866257] [G loss: -0.008069]\n",
      "1050 [D loss: -0.403953] [G loss: -0.002094]\n",
      "1051 [D loss: -0.619487] [G loss: -0.004172]\n",
      "1052 [D loss: -0.692215] [G loss: -0.005682]\n",
      "1053 [D loss: 0.570765] [G loss: -0.000609]\n",
      "1054 [D loss: 0.052733] [G loss: -0.000905]\n",
      "1055 [D loss: -0.102122] [G loss: -0.002010]\n",
      "1056 [D loss: -0.325114] [G loss: -0.012735]\n",
      "1057 [D loss: 9.449474] [G loss: -0.000001]\n",
      "1058 [D loss: 2.475015] [G loss: -0.000794]\n",
      "1059 [D loss: -0.055421] [G loss: -0.001430]\n",
      "1060 [D loss: -0.790579] [G loss: -0.001705]\n",
      "1061 [D loss: -0.025355] [G loss: -0.000361]\n",
      "1062 [D loss: -0.350219] [G loss: -0.039451]\n",
      "1063 [D loss: -0.447527] [G loss: -0.002483]\n",
      "1064 [D loss: -0.631849] [G loss: -0.000092]\n",
      "1065 [D loss: -0.192128] [G loss: -0.001890]\n",
      "1066 [D loss: 0.145386] [G loss: -0.003684]\n",
      "1067 [D loss: -0.342743] [G loss: -0.010523]\n",
      "1068 [D loss: 0.685807] [G loss: -0.003156]\n",
      "1069 [D loss: 0.556206] [G loss: -0.000393]\n",
      "1070 [D loss: -0.075109] [G loss: -0.024440]\n",
      "1071 [D loss: -0.753269] [G loss: -0.001977]\n",
      "1072 [D loss: 0.072431] [G loss: -0.007399]\n",
      "1073 [D loss: -0.048219] [G loss: -0.008079]\n",
      "1074 [D loss: 0.505036] [G loss: -0.000420]\n",
      "1075 [D loss: -0.496656] [G loss: -0.042061]\n",
      "1076 [D loss: 0.267155] [G loss: -0.000244]\n",
      "1077 [D loss: 0.639212] [G loss: -0.001024]\n",
      "1078 [D loss: -0.645339] [G loss: -0.000664]\n",
      "1079 [D loss: 4.176024] [G loss: -0.003969]\n",
      "1080 [D loss: -0.380823] [G loss: -0.008612]\n",
      "1081 [D loss: 5.795908] [G loss: -0.069156]\n",
      "1082 [D loss: -0.679570] [G loss: -0.012730]\n",
      "1083 [D loss: -0.236837] [G loss: -0.141027]\n",
      "1084 [D loss: -0.813838] [G loss: -0.014536]\n",
      "1085 [D loss: 2.987548] [G loss: -0.069200]\n",
      "1086 [D loss: -0.786279] [G loss: -0.006807]\n",
      "1087 [D loss: 1.478245] [G loss: -0.000133]\n",
      "1088 [D loss: -0.180521] [G loss: -0.035004]\n",
      "1089 [D loss: 0.158150] [G loss: -0.009157]\n",
      "1090 [D loss: -0.186475] [G loss: -0.081354]\n",
      "1091 [D loss: -0.561627] [G loss: -0.001908]\n",
      "1092 [D loss: -0.240151] [G loss: -0.072636]\n",
      "1093 [D loss: -0.479192] [G loss: -0.013417]\n",
      "1094 [D loss: -0.623735] [G loss: -0.006560]\n",
      "1095 [D loss: 1.871963] [G loss: -0.000101]\n",
      "1096 [D loss: 0.724454] [G loss: -0.001182]\n",
      "1097 [D loss: -0.655740] [G loss: -0.009884]\n",
      "1098 [D loss: -0.722323] [G loss: -0.000858]\n",
      "1099 [D loss: -0.230834] [G loss: -0.091404]\n",
      "1100 [D loss: -0.037677] [G loss: -0.040275]\n",
      "1101 [D loss: 0.282249] [G loss: -0.057053]\n",
      "1102 [D loss: 1.510956] [G loss: -0.001065]\n",
      "1103 [D loss: -0.247309] [G loss: -0.001952]\n",
      "1104 [D loss: 3.400932] [G loss: -0.000075]\n",
      "1105 [D loss: -0.430282] [G loss: -0.006792]\n",
      "1106 [D loss: 0.807296] [G loss: -0.000363]\n",
      "1107 [D loss: -0.156877] [G loss: -0.009138]\n",
      "1108 [D loss: -0.121660] [G loss: -0.008141]\n",
      "1109 [D loss: -0.637864] [G loss: -0.002287]\n",
      "1110 [D loss: -0.584155] [G loss: -0.005621]\n",
      "1111 [D loss: -0.075022] [G loss: -0.011803]\n",
      "1112 [D loss: -0.544871] [G loss: -0.059557]\n",
      "1113 [D loss: -0.508066] [G loss: -0.006403]\n",
      "1114 [D loss: -0.318780] [G loss: -0.080916]\n",
      "1115 [D loss: -0.339377] [G loss: -0.004787]\n",
      "1116 [D loss: -0.789843] [G loss: -0.003220]\n",
      "1117 [D loss: -0.690340] [G loss: -0.001737]\n",
      "1118 [D loss: 0.165202] [G loss: -0.013294]\n",
      "1119 [D loss: -0.712540] [G loss: -0.028428]\n",
      "1120 [D loss: -0.751986] [G loss: -0.020701]\n",
      "1121 [D loss: 0.159804] [G loss: -0.001833]\n",
      "1122 [D loss: -0.612697] [G loss: -0.317395]\n",
      "1123 [D loss: 1.952112] [G loss: -0.030628]\n",
      "1124 [D loss: -0.481659] [G loss: -0.001676]\n",
      "1125 [D loss: 0.769206] [G loss: -0.001257]\n",
      "1126 [D loss: 3.498819] [G loss: -0.000134]\n",
      "1127 [D loss: -0.745925] [G loss: -0.003750]\n",
      "1128 [D loss: 0.429408] [G loss: -0.000821]\n",
      "1129 [D loss: -0.655517] [G loss: -0.003628]\n",
      "1130 [D loss: 1.835006] [G loss: -0.008005]\n",
      "1131 [D loss: -0.317328] [G loss: -0.001787]\n",
      "1132 [D loss: 0.837825] [G loss: -0.087868]\n",
      "1133 [D loss: -0.270397] [G loss: -0.001241]\n",
      "1134 [D loss: 0.509246] [G loss: -0.035916]\n",
      "1135 [D loss: -0.444650] [G loss: -0.000209]\n",
      "1136 [D loss: -0.495161] [G loss: -0.001170]\n",
      "1137 [D loss: -0.207728] [G loss: -0.000906]\n",
      "1138 [D loss: -0.582832] [G loss: -0.000835]\n",
      "1139 [D loss: 1.000100] [G loss: -0.000090]\n",
      "1140 [D loss: 1.290191] [G loss: -0.032711]\n",
      "1141 [D loss: -0.388343] [G loss: -0.056117]\n",
      "1142 [D loss: -0.812698] [G loss: -0.001388]\n",
      "1143 [D loss: 0.303919] [G loss: -0.003480]\n",
      "1144 [D loss: 0.063730] [G loss: -0.084852]\n",
      "1145 [D loss: 0.503983] [G loss: -0.276715]\n",
      "1146 [D loss: 1.489101] [G loss: -0.020830]\n",
      "1147 [D loss: 0.857568] [G loss: -0.005372]\n",
      "1148 [D loss: -0.845410] [G loss: -0.015310]\n",
      "1149 [D loss: 0.167321] [G loss: -0.000456]\n",
      "1150 [D loss: -0.531600] [G loss: -0.003593]\n",
      "1151 [D loss: -0.326281] [G loss: -0.000067]\n",
      "1152 [D loss: -0.166829] [G loss: -0.032265]\n",
      "1153 [D loss: 0.973836] [G loss: -0.049271]\n",
      "1154 [D loss: -0.375853] [G loss: -0.038842]\n",
      "1155 [D loss: 0.476813] [G loss: -0.010050]\n",
      "1156 [D loss: 1.615761] [G loss: -0.000174]\n",
      "1157 [D loss: 1.111201] [G loss: -0.054655]\n",
      "1158 [D loss: -0.036349] [G loss: -0.003058]\n",
      "1159 [D loss: -0.416250] [G loss: -0.007484]\n",
      "1160 [D loss: 0.091077] [G loss: -0.352323]\n",
      "1161 [D loss: -0.344846] [G loss: -0.020623]\n",
      "1162 [D loss: -0.348073] [G loss: -0.011818]\n",
      "1163 [D loss: 0.807577] [G loss: -0.115900]\n",
      "1164 [D loss: 4.936866] [G loss: -0.058448]\n",
      "1165 [D loss: -0.678487] [G loss: -0.014570]\n",
      "1166 [D loss: -0.631011] [G loss: -0.010133]\n",
      "1167 [D loss: -0.286020] [G loss: -0.020137]\n",
      "1168 [D loss: 0.501007] [G loss: -0.008746]\n",
      "1169 [D loss: -0.594587] [G loss: -0.004129]\n",
      "1170 [D loss: 1.711048] [G loss: -0.000673]\n",
      "1171 [D loss: 0.668103] [G loss: -0.011261]\n",
      "1172 [D loss: -0.669262] [G loss: -0.058533]\n",
      "1173 [D loss: 2.262142] [G loss: -0.506942]\n",
      "1174 [D loss: -0.174602] [G loss: -0.005046]\n",
      "1175 [D loss: 0.303656] [G loss: -0.095945]\n",
      "1176 [D loss: -0.068522] [G loss: -0.293624]\n",
      "1177 [D loss: -0.716994] [G loss: -0.171894]\n",
      "1178 [D loss: 0.489919] [G loss: -0.000218]\n",
      "1179 [D loss: 0.931179] [G loss: -0.149305]\n",
      "1180 [D loss: 6.201915] [G loss: -0.003220]\n",
      "1181 [D loss: -0.424480] [G loss: -0.001880]\n",
      "1182 [D loss: 0.562145] [G loss: -0.265941]\n",
      "1183 [D loss: -0.757390] [G loss: -0.020700]\n",
      "1184 [D loss: -0.192808] [G loss: -0.005920]\n",
      "1185 [D loss: -0.528623] [G loss: -0.052078]\n",
      "1186 [D loss: -0.549365] [G loss: -0.000483]\n",
      "1187 [D loss: 0.718283] [G loss: -0.053939]\n",
      "1188 [D loss: 0.085580] [G loss: -0.285103]\n",
      "1189 [D loss: -0.492141] [G loss: -0.012385]\n",
      "1190 [D loss: -0.424568] [G loss: -0.002860]\n",
      "1191 [D loss: 0.404820] [G loss: -0.006716]\n",
      "1192 [D loss: -0.077720] [G loss: -0.041078]\n",
      "1193 [D loss: -0.781980] [G loss: -0.001577]\n",
      "1194 [D loss: -0.637472] [G loss: -0.002885]\n",
      "1195 [D loss: 0.094552] [G loss: -0.104106]\n",
      "1196 [D loss: -0.920504] [G loss: -0.000029]\n",
      "1197 [D loss: -0.562124] [G loss: -0.017981]\n",
      "1198 [D loss: -0.775226] [G loss: -0.008955]\n",
      "1199 [D loss: 0.538585] [G loss: -0.168408]\n",
      "1200 [D loss: 1.069118] [G loss: -0.004151]\n",
      "1201 [D loss: -0.126607] [G loss: -0.058794]\n",
      "1202 [D loss: -0.719126] [G loss: -0.024434]\n",
      "1203 [D loss: -0.175594] [G loss: -0.005741]\n",
      "1204 [D loss: -0.158023] [G loss: -0.004021]\n",
      "1205 [D loss: 6.244311] [G loss: -0.087681]\n",
      "1206 [D loss: -0.144814] [G loss: -0.093586]\n",
      "1207 [D loss: -0.686851] [G loss: -0.085353]\n",
      "1208 [D loss: -0.506669] [G loss: -0.004327]\n",
      "1209 [D loss: 1.233237] [G loss: -0.002577]\n",
      "1210 [D loss: -0.651078] [G loss: -0.331388]\n",
      "1211 [D loss: -0.774918] [G loss: -0.128774]\n",
      "1212 [D loss: -0.576686] [G loss: -0.024784]\n",
      "1213 [D loss: -0.522098] [G loss: -0.114093]\n",
      "1214 [D loss: 2.027308] [G loss: -0.709329]\n",
      "1215 [D loss: 1.234619] [G loss: -0.051655]\n",
      "1216 [D loss: -0.155751] [G loss: -0.023906]\n",
      "1217 [D loss: -0.577690] [G loss: -0.078037]\n",
      "1218 [D loss: -0.256775] [G loss: -0.008848]\n",
      "1219 [D loss: 1.423292] [G loss: -0.000115]\n",
      "1220 [D loss: -0.355199] [G loss: -0.072306]\n",
      "1221 [D loss: 2.851518] [G loss: -0.436512]\n",
      "1222 [D loss: -0.603261] [G loss: -0.000760]\n",
      "1223 [D loss: -0.567592] [G loss: -0.009493]\n",
      "1224 [D loss: 2.536530] [G loss: -0.051997]\n",
      "1225 [D loss: 0.213286] [G loss: -0.240557]\n",
      "1226 [D loss: -0.722519] [G loss: -0.060983]\n",
      "1227 [D loss: 0.816391] [G loss: -0.143332]\n",
      "1228 [D loss: -0.716510] [G loss: -0.136436]\n",
      "1229 [D loss: -0.825919] [G loss: -0.299730]\n",
      "1230 [D loss: 0.099738] [G loss: -0.065174]\n",
      "1231 [D loss: -0.807490] [G loss: -0.607414]\n",
      "1232 [D loss: -0.675643] [G loss: -0.094575]\n",
      "1233 [D loss: -0.603956] [G loss: -0.002589]\n",
      "1234 [D loss: -0.875567] [G loss: -0.052384]\n",
      "1235 [D loss: -0.375255] [G loss: -0.003844]\n",
      "1236 [D loss: 0.922323] [G loss: -0.037436]\n",
      "1237 [D loss: -0.521036] [G loss: -0.648388]\n",
      "1238 [D loss: 1.688010] [G loss: -0.304464]\n",
      "1239 [D loss: -0.547949] [G loss: -0.224911]\n",
      "1240 [D loss: -0.826317] [G loss: -0.172673]\n",
      "1241 [D loss: -0.717964] [G loss: -0.048335]\n",
      "1242 [D loss: -0.692755] [G loss: -0.037552]\n",
      "1243 [D loss: -0.803412] [G loss: -0.145683]\n",
      "1244 [D loss: -0.667539] [G loss: -0.014997]\n",
      "1245 [D loss: -0.118814] [G loss: -0.054581]\n",
      "1246 [D loss: 4.008735] [G loss: -0.179437]\n",
      "1247 [D loss: 1.586151] [G loss: -0.060418]\n",
      "1248 [D loss: -0.585888] [G loss: -0.538903]\n",
      "1249 [D loss: 0.904888] [G loss: -0.460948]\n",
      "1250 [D loss: -0.387026] [G loss: -0.337078]\n",
      "1251 [D loss: -0.742540] [G loss: -0.202926]\n",
      "1252 [D loss: 6.674932] [G loss: -0.197498]\n",
      "1253 [D loss: -0.611298] [G loss: -0.562242]\n",
      "1254 [D loss: 1.585577] [G loss: -0.720621]\n",
      "1255 [D loss: -0.640210] [G loss: -0.083348]\n",
      "1256 [D loss: -0.701429] [G loss: -0.248791]\n",
      "1257 [D loss: 0.164965] [G loss: -0.148935]\n",
      "1258 [D loss: -0.216910] [G loss: -0.334477]\n",
      "1259 [D loss: -0.523953] [G loss: -0.147728]\n",
      "1260 [D loss: 0.579600] [G loss: -0.780033]\n",
      "1261 [D loss: 0.226103] [G loss: -0.243962]\n",
      "1262 [D loss: 1.133722] [G loss: -0.385659]\n",
      "1263 [D loss: 0.321982] [G loss: -0.299240]\n",
      "1264 [D loss: -0.480058] [G loss: -0.761413]\n",
      "1265 [D loss: 2.908376] [G loss: -0.016194]\n",
      "1266 [D loss: 0.058931] [G loss: -0.084770]\n",
      "1267 [D loss: -0.862300] [G loss: -0.106120]\n",
      "1268 [D loss: 1.524960] [G loss: -0.025741]\n",
      "1269 [D loss: -0.031969] [G loss: -0.006908]\n",
      "1270 [D loss: -0.153718] [G loss: -0.027773]\n",
      "1271 [D loss: 0.792300] [G loss: -0.003601]\n",
      "1272 [D loss: 2.421023] [G loss: -0.258240]\n",
      "1273 [D loss: 0.714210] [G loss: -0.234928]\n",
      "1274 [D loss: -0.677775] [G loss: -0.383881]\n",
      "1275 [D loss: 2.576527] [G loss: -0.013514]\n",
      "1276 [D loss: 0.379963] [G loss: -0.019775]\n",
      "1277 [D loss: -0.695317] [G loss: -0.040665]\n",
      "1278 [D loss: 0.097602] [G loss: -0.001166]\n",
      "1279 [D loss: -0.643960] [G loss: -0.092824]\n",
      "1280 [D loss: -0.478739] [G loss: -0.112133]\n",
      "1281 [D loss: 1.014640] [G loss: -0.596579]\n",
      "1282 [D loss: -0.186456] [G loss: -0.348968]\n",
      "1283 [D loss: -0.429676] [G loss: -0.504389]\n",
      "1284 [D loss: 0.609212] [G loss: -0.047433]\n",
      "1285 [D loss: 0.364385] [G loss: -0.028090]\n",
      "1286 [D loss: 0.177962] [G loss: -0.010666]\n",
      "1287 [D loss: -0.278040] [G loss: -0.036785]\n",
      "1288 [D loss: -0.019448] [G loss: -0.052182]\n",
      "1289 [D loss: -0.677482] [G loss: -0.056064]\n",
      "1290 [D loss: 0.401774] [G loss: -0.109135]\n",
      "1291 [D loss: -0.698966] [G loss: -0.043645]\n",
      "1292 [D loss: 0.246397] [G loss: -0.049107]\n",
      "1293 [D loss: -0.688842] [G loss: -0.060488]\n",
      "1294 [D loss: -0.779202] [G loss: -0.020113]\n",
      "1295 [D loss: 0.946436] [G loss: -0.000151]\n",
      "1296 [D loss: -0.462178] [G loss: -0.159468]\n",
      "1297 [D loss: -0.584242] [G loss: -0.014154]\n",
      "1298 [D loss: 1.941427] [G loss: -0.206978]\n",
      "1299 [D loss: -0.423256] [G loss: -0.104556]\n",
      "1300 [D loss: 0.759467] [G loss: -0.619632]\n",
      "1301 [D loss: 1.168755] [G loss: -0.104810]\n",
      "1302 [D loss: 0.950444] [G loss: -0.740346]\n",
      "1303 [D loss: -0.383725] [G loss: -0.021741]\n",
      "1304 [D loss: -0.036472] [G loss: -0.006081]\n",
      "1305 [D loss: -0.800239] [G loss: -0.034390]\n",
      "1306 [D loss: -0.821732] [G loss: -0.051347]\n",
      "1307 [D loss: 0.092712] [G loss: -0.973298]\n",
      "1308 [D loss: -0.408852] [G loss: -0.728229]\n",
      "1309 [D loss: -0.309708] [G loss: -0.828033]\n",
      "1310 [D loss: 0.789464] [G loss: -0.921318]\n",
      "1311 [D loss: -0.463688] [G loss: -0.812059]\n",
      "1312 [D loss: -0.718299] [G loss: -0.788284]\n",
      "1313 [D loss: -0.702964] [G loss: -0.587064]\n",
      "1314 [D loss: -0.718992] [G loss: -0.746366]\n",
      "1315 [D loss: -0.226141] [G loss: -0.372991]\n",
      "1316 [D loss: 1.670805] [G loss: -0.431159]\n",
      "1317 [D loss: -0.034373] [G loss: -0.239254]\n",
      "1318 [D loss: -0.892310] [G loss: -0.201104]\n",
      "1319 [D loss: 2.386987] [G loss: -0.998281]\n",
      "1320 [D loss: 2.449928] [G loss: -0.999773]\n",
      "1321 [D loss: -0.469835] [G loss: -0.962192]\n",
      "1322 [D loss: 0.549005] [G loss: -0.774214]\n",
      "1323 [D loss: -0.751812] [G loss: -0.974384]\n",
      "1324 [D loss: -0.015117] [G loss: -0.904062]\n",
      "1325 [D loss: -0.100769] [G loss: -0.231634]\n",
      "1326 [D loss: -0.909873] [G loss: -0.184866]\n",
      "1327 [D loss: 1.392972] [G loss: -0.987106]\n",
      "1328 [D loss: 5.343068] [G loss: -0.954511]\n",
      "1329 [D loss: -0.680891] [G loss: -0.985879]\n",
      "1330 [D loss: -0.392526] [G loss: -0.958415]\n",
      "1331 [D loss: 0.232699] [G loss: -0.884870]\n",
      "1332 [D loss: -0.322560] [G loss: -0.774147]\n",
      "1333 [D loss: 0.196007] [G loss: -0.451215]\n",
      "1334 [D loss: -0.466762] [G loss: -0.207956]\n",
      "1335 [D loss: -0.719903] [G loss: -0.899812]\n",
      "1336 [D loss: -0.580358] [G loss: -0.953964]\n",
      "1337 [D loss: -0.679066] [G loss: -0.871383]\n",
      "1338 [D loss: -0.160923] [G loss: -0.405867]\n",
      "1339 [D loss: -0.737473] [G loss: -0.493619]\n",
      "1340 [D loss: -0.431449] [G loss: -0.959040]\n",
      "1341 [D loss: -0.745467] [G loss: -0.818056]\n",
      "1342 [D loss: -0.741014] [G loss: -0.857860]\n",
      "1343 [D loss: -0.742922] [G loss: -0.881047]\n",
      "1344 [D loss: 0.223093] [G loss: -0.962937]\n",
      "1345 [D loss: -0.844132] [G loss: -0.919117]\n",
      "1346 [D loss: 1.303958] [G loss: -0.967375]\n",
      "1347 [D loss: -0.205947] [G loss: -0.617239]\n",
      "1348 [D loss: -0.002623] [G loss: -0.051793]\n",
      "1349 [D loss: 0.670065] [G loss: -0.006890]\n",
      "1350 [D loss: -0.272337] [G loss: -0.799103]\n",
      "1351 [D loss: -0.719873] [G loss: -0.816768]\n",
      "1352 [D loss: 0.348884] [G loss: -0.958420]\n",
      "1353 [D loss: -0.302710] [G loss: -0.962906]\n",
      "1354 [D loss: -0.591301] [G loss: -0.758278]\n",
      "1355 [D loss: 2.729773] [G loss: -0.509785]\n",
      "1356 [D loss: 0.082319] [G loss: -0.157386]\n",
      "1357 [D loss: -0.693309] [G loss: -0.105169]\n",
      "1358 [D loss: -0.153931] [G loss: -0.761751]\n",
      "1359 [D loss: -0.096537] [G loss: -0.431550]\n",
      "1360 [D loss: 0.928388] [G loss: -0.028312]\n",
      "1361 [D loss: -0.898211] [G loss: -0.070314]\n",
      "1362 [D loss: -0.797343] [G loss: -0.462461]\n",
      "1363 [D loss: -0.499985] [G loss: -0.849045]\n",
      "1364 [D loss: -0.101835] [G loss: -0.872049]\n",
      "1365 [D loss: -0.071253] [G loss: -0.850114]\n",
      "1366 [D loss: -0.561070] [G loss: -0.566955]\n",
      "1367 [D loss: -0.137005] [G loss: -0.349531]\n",
      "1368 [D loss: 0.334695] [G loss: -0.785434]\n",
      "1369 [D loss: -0.550440] [G loss: -0.901545]\n",
      "1370 [D loss: -0.263435] [G loss: -0.982241]\n",
      "1371 [D loss: -0.701792] [G loss: -0.993804]\n",
      "1372 [D loss: -0.592536] [G loss: -0.981132]\n",
      "1373 [D loss: -0.366442] [G loss: -0.762424]\n",
      "1374 [D loss: -0.497235] [G loss: -0.887397]\n",
      "1375 [D loss: -0.253274] [G loss: -0.949912]\n",
      "1376 [D loss: -0.768946] [G loss: -0.999355]\n",
      "1377 [D loss: 0.244864] [G loss: -0.999303]\n",
      "1378 [D loss: -0.061390] [G loss: -0.969513]\n",
      "1379 [D loss: 1.228151] [G loss: -0.411451]\n",
      "1380 [D loss: -0.343977] [G loss: -0.888212]\n",
      "1381 [D loss: -0.732213] [G loss: -0.949899]\n",
      "1382 [D loss: -0.780101] [G loss: -0.710279]\n",
      "1383 [D loss: -0.408698] [G loss: -0.211176]\n",
      "1384 [D loss: 0.783475] [G loss: -0.730818]\n",
      "1385 [D loss: -0.202788] [G loss: -0.950806]\n",
      "1386 [D loss: -0.556695] [G loss: -0.960234]\n",
      "1387 [D loss: -0.712330] [G loss: -0.632729]\n",
      "1388 [D loss: -0.888444] [G loss: -0.643297]\n",
      "1389 [D loss: 0.256416] [G loss: -0.474157]\n",
      "1390 [D loss: -0.530795] [G loss: -0.933843]\n",
      "1391 [D loss: -0.145295] [G loss: -0.997121]\n",
      "1392 [D loss: 3.211790] [G loss: -0.940466]\n",
      "1393 [D loss: -0.715043] [G loss: -0.722650]\n",
      "1394 [D loss: -0.101961] [G loss: -0.983319]\n",
      "1395 [D loss: 0.750912] [G loss: -0.871123]\n",
      "1396 [D loss: 0.679456] [G loss: -0.999535]\n",
      "1397 [D loss: -0.752663] [G loss: -0.818518]\n",
      "1398 [D loss: 0.897144] [G loss: -0.990051]\n",
      "1399 [D loss: -0.550967] [G loss: -0.840169]\n",
      "1400 [D loss: 2.489176] [G loss: -0.672642]\n",
      "1401 [D loss: 0.194855] [G loss: -0.940974]\n",
      "1402 [D loss: 0.458082] [G loss: -0.714077]\n",
      "1403 [D loss: 0.506187] [G loss: -0.701054]\n",
      "1404 [D loss: -0.172489] [G loss: -0.446305]\n",
      "1405 [D loss: -0.684848] [G loss: -0.935411]\n",
      "1406 [D loss: -0.048342] [G loss: -0.969276]\n",
      "1407 [D loss: -0.397758] [G loss: -0.746122]\n",
      "1408 [D loss: 0.245865] [G loss: -0.794898]\n",
      "1409 [D loss: 2.819696] [G loss: -0.623092]\n",
      "1410 [D loss: -0.701606] [G loss: -0.882003]\n",
      "1411 [D loss: -0.175155] [G loss: -0.861905]\n",
      "1412 [D loss: 1.230665] [G loss: -0.383121]\n",
      "1413 [D loss: -0.417575] [G loss: -0.475773]\n",
      "1414 [D loss: -0.868145] [G loss: -0.161943]\n",
      "1415 [D loss: 1.192665] [G loss: -0.062663]\n",
      "1416 [D loss: -0.322781] [G loss: -0.703831]\n",
      "1417 [D loss: 0.059200] [G loss: -0.591022]\n",
      "1418 [D loss: 3.058529] [G loss: -0.979062]\n",
      "1419 [D loss: -0.145565] [G loss: -0.994027]\n",
      "1420 [D loss: -0.441804] [G loss: -0.758533]\n",
      "1421 [D loss: -0.108683] [G loss: -0.724387]\n",
      "1422 [D loss: 0.037555] [G loss: -0.617832]\n",
      "1423 [D loss: 1.017252] [G loss: -0.628531]\n",
      "1424 [D loss: 0.189361] [G loss: -0.946099]\n",
      "1425 [D loss: -0.350555] [G loss: -0.993993]\n",
      "1426 [D loss: -0.464287] [G loss: -0.959943]\n",
      "1427 [D loss: -0.322821] [G loss: -0.460999]\n",
      "1428 [D loss: 0.449781] [G loss: -0.991494]\n",
      "1429 [D loss: 1.385675] [G loss: -0.932401]\n",
      "1430 [D loss: 1.895340] [G loss: -0.963274]\n",
      "1431 [D loss: -0.152339] [G loss: -0.996956]\n",
      "1432 [D loss: -0.321156] [G loss: -0.998727]\n",
      "1433 [D loss: 0.129182] [G loss: -0.999893]\n",
      "1434 [D loss: 0.539030] [G loss: -0.987581]\n",
      "1435 [D loss: -0.176940] [G loss: -0.999371]\n",
      "1436 [D loss: 0.788309] [G loss: -0.996093]\n",
      "1437 [D loss: -0.382812] [G loss: -0.996146]\n",
      "1438 [D loss: 7.225417] [G loss: -0.999728]\n",
      "1439 [D loss: 0.675318] [G loss: -0.975450]\n",
      "1440 [D loss: 0.074031] [G loss: -0.977174]\n",
      "1441 [D loss: -0.522260] [G loss: -0.998316]\n",
      "1442 [D loss: -0.416929] [G loss: -0.997125]\n",
      "1443 [D loss: 0.681733] [G loss: -0.947488]\n",
      "1444 [D loss: -0.470209] [G loss: -0.996300]\n",
      "1445 [D loss: -0.514725] [G loss: -0.943043]\n",
      "1446 [D loss: -0.441122] [G loss: -0.634364]\n",
      "1447 [D loss: -0.789251] [G loss: -0.998947]\n",
      "1448 [D loss: -0.239302] [G loss: -0.999807]\n",
      "1449 [D loss: -0.480583] [G loss: -0.994608]\n",
      "1450 [D loss: -0.724818] [G loss: -0.997473]\n",
      "1451 [D loss: 0.627011] [G loss: -0.992835]\n",
      "1452 [D loss: -0.421021] [G loss: -0.622117]\n",
      "1453 [D loss: -0.133363] [G loss: -0.963158]\n",
      "1454 [D loss: 0.274031] [G loss: -0.950061]\n",
      "1455 [D loss: -0.390998] [G loss: -0.978168]\n",
      "1456 [D loss: 0.363007] [G loss: -0.988178]\n",
      "1457 [D loss: -0.519450] [G loss: -0.978373]\n",
      "1458 [D loss: -0.520052] [G loss: -0.982131]\n",
      "1459 [D loss: 2.421983] [G loss: -0.974862]\n",
      "1460 [D loss: -0.184785] [G loss: -0.999344]\n",
      "1461 [D loss: 0.106453] [G loss: -0.986019]\n",
      "1462 [D loss: 0.291567] [G loss: -0.982623]\n",
      "1463 [D loss: 0.713722] [G loss: -0.989164]\n",
      "1464 [D loss: 0.000877] [G loss: -0.844092]\n",
      "1465 [D loss: 0.211608] [G loss: -0.667557]\n",
      "1466 [D loss: 0.928359] [G loss: -0.741819]\n",
      "1467 [D loss: 0.596696] [G loss: -0.128658]\n",
      "1468 [D loss: -0.778453] [G loss: -0.901426]\n",
      "1469 [D loss: -0.606116] [G loss: -0.996577]\n",
      "1470 [D loss: 4.432792] [G loss: -0.879577]\n",
      "1471 [D loss: -0.525464] [G loss: -0.821995]\n",
      "1472 [D loss: 0.062139] [G loss: -0.833250]\n",
      "1473 [D loss: -0.664338] [G loss: -0.928570]\n",
      "1474 [D loss: 0.358102] [G loss: -0.984940]\n",
      "1475 [D loss: -0.775038] [G loss: -0.928563]\n",
      "1476 [D loss: -0.792471] [G loss: -0.927342]\n",
      "1477 [D loss: -0.797379] [G loss: -0.972182]\n",
      "1478 [D loss: -0.348643] [G loss: -0.992975]\n",
      "1479 [D loss: 2.718902] [G loss: -0.981471]\n",
      "1480 [D loss: 1.740210] [G loss: -0.996198]\n",
      "1481 [D loss: 0.003753] [G loss: -0.971022]\n",
      "1482 [D loss: -0.805381] [G loss: -0.994641]\n",
      "1483 [D loss: -0.052432] [G loss: -0.987267]\n",
      "1484 [D loss: 0.578781] [G loss: -0.998815]\n",
      "1485 [D loss: -0.449981] [G loss: -0.999949]\n",
      "1486 [D loss: -0.095355] [G loss: -0.975816]\n",
      "1487 [D loss: 0.462576] [G loss: -0.999626]\n",
      "1488 [D loss: 0.065591] [G loss: -0.999495]\n",
      "1489 [D loss: 0.266430] [G loss: -0.999997]\n",
      "1490 [D loss: 0.704156] [G loss: -0.999822]\n",
      "1491 [D loss: -0.435079] [G loss: -0.999988]\n",
      "1492 [D loss: -0.403395] [G loss: -0.999607]\n",
      "1493 [D loss: 3.314075] [G loss: -0.998724]\n",
      "1494 [D loss: -0.814583] [G loss: -0.999720]\n",
      "1495 [D loss: -0.418459] [G loss: -0.999709]\n",
      "1496 [D loss: 1.058003] [G loss: -0.573287]\n",
      "1497 [D loss: 3.850529] [G loss: -0.998683]\n",
      "1498 [D loss: -0.561910] [G loss: -0.999118]\n",
      "1499 [D loss: -0.097012] [G loss: -0.895909]\n",
      "1500 [D loss: -0.133579] [G loss: -0.975420]\n",
      "1501 [D loss: 2.765918] [G loss: -0.997416]\n",
      "1502 [D loss: 4.042441] [G loss: -0.999982]\n",
      "1503 [D loss: -0.436090] [G loss: -0.959834]\n",
      "1504 [D loss: -0.378440] [G loss: -0.907783]\n",
      "1505 [D loss: 0.246809] [G loss: -0.613268]\n",
      "1506 [D loss: 0.467144] [G loss: -0.953678]\n",
      "1507 [D loss: 0.100857] [G loss: -0.999904]\n",
      "1508 [D loss: 0.669220] [G loss: -0.998967]\n",
      "1509 [D loss: 1.202205] [G loss: -0.594062]\n",
      "1510 [D loss: 0.695392] [G loss: -0.997102]\n",
      "1511 [D loss: 0.417898] [G loss: -0.888031]\n",
      "1512 [D loss: -0.511357] [G loss: -0.999919]\n",
      "1513 [D loss: 0.220394] [G loss: -0.995938]\n",
      "1514 [D loss: -0.557278] [G loss: -0.997133]\n",
      "1515 [D loss: -0.727073] [G loss: -0.999989]\n",
      "1516 [D loss: -0.772620] [G loss: -0.999991]\n",
      "1517 [D loss: -0.649301] [G loss: -0.999908]\n",
      "1518 [D loss: 0.389544] [G loss: -0.942985]\n",
      "1519 [D loss: -0.257875] [G loss: -0.977851]\n",
      "1520 [D loss: -0.645101] [G loss: -0.988778]\n",
      "1521 [D loss: -0.033008] [G loss: -0.979966]\n",
      "1522 [D loss: -0.416659] [G loss: -0.769096]\n",
      "1523 [D loss: 0.004586] [G loss: -0.996932]\n",
      "1524 [D loss: -0.257509] [G loss: -0.334442]\n",
      "1525 [D loss: -0.503742] [G loss: -0.960283]\n",
      "1526 [D loss: -0.473794] [G loss: -0.591272]\n",
      "1527 [D loss: -0.273907] [G loss: -0.969207]\n",
      "1528 [D loss: -0.163976] [G loss: -0.953168]\n",
      "1529 [D loss: -0.480114] [G loss: -0.969532]\n",
      "1530 [D loss: -0.836006] [G loss: -0.905989]\n",
      "1531 [D loss: 0.165231] [G loss: -0.997850]\n",
      "1532 [D loss: -0.403771] [G loss: -0.938881]\n",
      "1533 [D loss: 0.393577] [G loss: -0.907152]\n",
      "1534 [D loss: 1.554717] [G loss: -0.856737]\n",
      "1535 [D loss: -0.784426] [G loss: -0.924430]\n",
      "1536 [D loss: -0.693539] [G loss: -0.884107]\n",
      "1537 [D loss: -0.655369] [G loss: -0.978027]\n",
      "1538 [D loss: -0.475491] [G loss: -0.849679]\n",
      "1539 [D loss: -0.681862] [G loss: -0.994045]\n",
      "1540 [D loss: -0.515333] [G loss: -0.999845]\n",
      "1541 [D loss: 1.506480] [G loss: -0.965653]\n",
      "1542 [D loss: -0.556405] [G loss: -0.999871]\n",
      "1543 [D loss: -0.382537] [G loss: -0.998176]\n",
      "1544 [D loss: 1.450296] [G loss: -0.852405]\n",
      "1545 [D loss: -0.385570] [G loss: -0.992282]\n",
      "1546 [D loss: -0.789546] [G loss: -0.998225]\n",
      "1547 [D loss: 20.719055] [G loss: -0.998839]\n",
      "1548 [D loss: 4.158305] [G loss: -0.992957]\n",
      "1549 [D loss: -0.496283] [G loss: -0.974258]\n",
      "1550 [D loss: -0.785651] [G loss: -0.980995]\n",
      "1551 [D loss: 2.656800] [G loss: -0.859422]\n",
      "1552 [D loss: -0.677637] [G loss: -0.995099]\n",
      "1553 [D loss: -0.749442] [G loss: -0.984752]\n",
      "1554 [D loss: 0.657684] [G loss: -0.799967]\n",
      "1555 [D loss: 0.362822] [G loss: -0.523810]\n",
      "1556 [D loss: 0.039979] [G loss: -0.778122]\n",
      "1557 [D loss: -0.625188] [G loss: -0.851674]\n",
      "1558 [D loss: 0.673956] [G loss: -0.969392]\n",
      "1559 [D loss: -0.558539] [G loss: -0.992888]\n",
      "1560 [D loss: 0.284407] [G loss: -0.963769]\n",
      "1561 [D loss: 0.087502] [G loss: -0.489836]\n",
      "1562 [D loss: 1.597137] [G loss: -0.970570]\n",
      "1563 [D loss: 2.462232] [G loss: -0.918539]\n",
      "1564 [D loss: -0.146718] [G loss: -0.998701]\n",
      "1565 [D loss: -0.681744] [G loss: -0.997448]\n",
      "1566 [D loss: 0.302075] [G loss: -0.998014]\n",
      "1567 [D loss: -0.042875] [G loss: -0.988205]\n",
      "1568 [D loss: -0.234563] [G loss: -0.831787]\n",
      "1569 [D loss: -0.822147] [G loss: -0.903282]\n",
      "1570 [D loss: 1.059193] [G loss: -0.906342]\n",
      "1571 [D loss: -0.247775] [G loss: -0.966677]\n",
      "1572 [D loss: 0.187604] [G loss: -0.928763]\n",
      "1573 [D loss: 2.180977] [G loss: -0.841762]\n",
      "1574 [D loss: 0.390875] [G loss: -0.994930]\n",
      "1575 [D loss: 3.435802] [G loss: -0.983977]\n",
      "1576 [D loss: -0.792250] [G loss: -0.999851]\n",
      "1577 [D loss: -0.358493] [G loss: -0.992118]\n",
      "1578 [D loss: -0.784186] [G loss: -0.835791]\n",
      "1579 [D loss: -0.740840] [G loss: -0.859235]\n",
      "1580 [D loss: 0.106967] [G loss: -0.590559]\n",
      "1581 [D loss: 10.915606] [G loss: -0.952426]\n",
      "1582 [D loss: 1.740404] [G loss: -0.924565]\n",
      "1583 [D loss: 2.036575] [G loss: -0.984615]\n",
      "1584 [D loss: -0.805212] [G loss: -0.842087]\n",
      "1585 [D loss: 0.879640] [G loss: -0.363901]\n",
      "1586 [D loss: -0.812995] [G loss: -0.952304]\n",
      "1587 [D loss: -0.387220] [G loss: -0.675158]\n",
      "1588 [D loss: -0.888458] [G loss: -0.719509]\n",
      "1589 [D loss: 0.189647] [G loss: -0.900613]\n",
      "1590 [D loss: -0.683327] [G loss: -0.974399]\n",
      "1591 [D loss: -0.178590] [G loss: -0.881411]\n",
      "1592 [D loss: -0.307884] [G loss: -0.908504]\n",
      "1593 [D loss: -0.845456] [G loss: -0.882529]\n",
      "1594 [D loss: -0.325644] [G loss: -0.678121]\n",
      "1595 [D loss: -0.426985] [G loss: -0.940275]\n",
      "1596 [D loss: 4.518308] [G loss: -0.342315]\n",
      "1597 [D loss: 0.953770] [G loss: -0.000435]\n",
      "1598 [D loss: -0.638139] [G loss: -0.500107]\n",
      "1599 [D loss: 0.154289] [G loss: -0.944345]\n",
      "1600 [D loss: -0.645968] [G loss: -0.960704]\n",
      "1601 [D loss: 1.318854] [G loss: -0.768019]\n",
      "1602 [D loss: 2.930039] [G loss: -0.517691]\n",
      "1603 [D loss: -0.772565] [G loss: -0.497427]\n",
      "1604 [D loss: 1.368806] [G loss: -0.898873]\n",
      "1605 [D loss: -0.528753] [G loss: -0.877547]\n",
      "1606 [D loss: -0.628048] [G loss: -0.883552]\n",
      "1607 [D loss: -0.187819] [G loss: -0.233220]\n",
      "1608 [D loss: -0.594231] [G loss: -0.837611]\n",
      "1609 [D loss: 0.148640] [G loss: -0.885417]\n",
      "1610 [D loss: -0.149288] [G loss: -0.217805]\n",
      "1611 [D loss: -0.878821] [G loss: -0.911213]\n",
      "1612 [D loss: -0.051316] [G loss: -0.814811]\n",
      "1613 [D loss: -0.525279] [G loss: -0.996737]\n",
      "1614 [D loss: 0.219927] [G loss: -0.624235]\n",
      "1615 [D loss: 1.041018] [G loss: -0.990794]\n",
      "1616 [D loss: -0.945467] [G loss: -0.870835]\n",
      "1617 [D loss: -0.446055] [G loss: -0.963096]\n",
      "1618 [D loss: 0.351338] [G loss: -0.994271]\n",
      "1619 [D loss: -0.488426] [G loss: -0.999298]\n",
      "1620 [D loss: -0.612347] [G loss: -0.999741]\n",
      "1621 [D loss: -0.816827] [G loss: -0.989712]\n",
      "1622 [D loss: -0.310644] [G loss: -0.999576]\n",
      "1623 [D loss: -0.716049] [G loss: -0.999988]\n",
      "1624 [D loss: 0.527773] [G loss: -0.999887]\n",
      "1625 [D loss: -0.704234] [G loss: -0.999709]\n",
      "1626 [D loss: -0.758597] [G loss: -0.998153]\n",
      "1627 [D loss: -0.755464] [G loss: -0.993596]\n",
      "1628 [D loss: -0.195638] [G loss: -0.892820]\n",
      "1629 [D loss: -0.696075] [G loss: -0.999269]\n",
      "1630 [D loss: -0.422285] [G loss: -0.996539]\n",
      "1631 [D loss: 2.281615] [G loss: -0.999972]\n",
      "1632 [D loss: -0.561076] [G loss: -0.996787]\n",
      "1633 [D loss: 0.451197] [G loss: -0.999712]\n",
      "1634 [D loss: -0.125024] [G loss: -0.854166]\n",
      "1635 [D loss: -0.126174] [G loss: -0.981143]\n",
      "1636 [D loss: -0.789405] [G loss: -0.998676]\n",
      "1637 [D loss: -0.228886] [G loss: -0.990561]\n",
      "1638 [D loss: -0.148301] [G loss: -0.973181]\n",
      "1639 [D loss: 0.927988] [G loss: -0.685604]\n",
      "1640 [D loss: 4.125255] [G loss: -0.990205]\n",
      "1641 [D loss: -0.550956] [G loss: -0.996450]\n",
      "1642 [D loss: 0.970037] [G loss: -0.963545]\n",
      "1643 [D loss: -0.118777] [G loss: -0.995175]\n",
      "1644 [D loss: 1.287975] [G loss: -0.999699]\n",
      "1645 [D loss: 0.181919] [G loss: -0.985497]\n",
      "1646 [D loss: 0.537841] [G loss: -0.999163]\n",
      "1647 [D loss: -0.645866] [G loss: -0.999775]\n",
      "1648 [D loss: -0.888557] [G loss: -0.996467]\n",
      "1649 [D loss: -0.553905] [G loss: -0.998325]\n",
      "1650 [D loss: 0.573820] [G loss: -0.999744]\n",
      "1651 [D loss: -0.568343] [G loss: -0.999963]\n",
      "1652 [D loss: -0.479227] [G loss: -0.999989]\n",
      "1653 [D loss: -0.277585] [G loss: -0.984809]\n",
      "1654 [D loss: 0.004533] [G loss: -0.980182]\n",
      "1655 [D loss: 0.882582] [G loss: -0.999969]\n",
      "1656 [D loss: -0.417512] [G loss: -0.999161]\n",
      "1657 [D loss: 1.300339] [G loss: -0.998029]\n",
      "1658 [D loss: -0.468780] [G loss: -0.999797]\n",
      "1659 [D loss: -0.726071] [G loss: -0.999996]\n",
      "1660 [D loss: -0.705969] [G loss: -0.999988]\n",
      "1661 [D loss: -0.220842] [G loss: -0.999702]\n",
      "1662 [D loss: -0.342552] [G loss: -0.999886]\n",
      "1663 [D loss: -0.671364] [G loss: -0.953719]\n",
      "1664 [D loss: -0.445063] [G loss: -0.993349]\n",
      "1665 [D loss: -0.110349] [G loss: -0.888070]\n",
      "1666 [D loss: -0.427962] [G loss: -0.998777]\n",
      "1667 [D loss: -0.093592] [G loss: -0.830778]\n",
      "1668 [D loss: 0.816301] [G loss: -0.955409]\n",
      "1669 [D loss: -0.778443] [G loss: -0.975128]\n",
      "1670 [D loss: 1.358412] [G loss: -0.999415]\n",
      "1671 [D loss: 0.254521] [G loss: -0.999419]\n",
      "1672 [D loss: 0.115307] [G loss: -0.999527]\n",
      "1673 [D loss: -0.631626] [G loss: -0.999324]\n",
      "1674 [D loss: 0.146611] [G loss: -0.999773]\n",
      "1675 [D loss: 0.350283] [G loss: -0.999537]\n",
      "1676 [D loss: -0.820308] [G loss: -0.996289]\n",
      "1677 [D loss: 0.383967] [G loss: -0.996974]\n",
      "1678 [D loss: 0.394645] [G loss: -0.997931]\n",
      "1679 [D loss: -0.863674] [G loss: -0.999086]\n",
      "1680 [D loss: 0.151982] [G loss: -0.998386]\n",
      "1681 [D loss: -0.399759] [G loss: -0.998713]\n",
      "1682 [D loss: 1.380308] [G loss: -0.983576]\n",
      "1683 [D loss: -0.626815] [G loss: -0.991308]\n",
      "1684 [D loss: 0.315004] [G loss: -0.998229]\n",
      "1685 [D loss: -0.814992] [G loss: -0.957123]\n",
      "1686 [D loss: -0.340286] [G loss: -0.977652]\n",
      "1687 [D loss: -0.675168] [G loss: -0.876655]\n",
      "1688 [D loss: 0.266951] [G loss: -0.946474]\n",
      "1689 [D loss: 0.033162] [G loss: -0.998597]\n",
      "1690 [D loss: 0.149550] [G loss: -0.999991]\n",
      "1691 [D loss: 1.364914] [G loss: -0.791074]\n",
      "1692 [D loss: -0.133708] [G loss: -0.994451]\n",
      "1693 [D loss: -0.338962] [G loss: -0.936201]\n",
      "1694 [D loss: -0.343994] [G loss: -0.998907]\n",
      "1695 [D loss: -0.683204] [G loss: -0.822634]\n",
      "1696 [D loss: 1.722290] [G loss: -0.995894]\n",
      "1697 [D loss: -0.154818] [G loss: -0.140851]\n",
      "1698 [D loss: -0.473923] [G loss: -0.871341]\n",
      "1699 [D loss: -0.536105] [G loss: -0.998571]\n",
      "1700 [D loss: 0.735768] [G loss: -0.999996]\n",
      "1701 [D loss: -0.098326] [G loss: -0.989841]\n",
      "1702 [D loss: 0.983131] [G loss: -0.999649]\n",
      "1703 [D loss: 1.701394] [G loss: -0.514576]\n",
      "1704 [D loss: 3.945675] [G loss: -0.250841]\n",
      "1705 [D loss: 0.223927] [G loss: -0.758080]\n",
      "1706 [D loss: -0.323258] [G loss: -0.980330]\n",
      "1707 [D loss: 0.203738] [G loss: -0.926522]\n",
      "1708 [D loss: 1.647147] [G loss: -0.953736]\n",
      "1709 [D loss: -0.787493] [G loss: -0.999722]\n",
      "1710 [D loss: -0.407564] [G loss: -0.994052]\n",
      "1711 [D loss: 0.179644] [G loss: -0.997321]\n",
      "1712 [D loss: 1.868009] [G loss: -0.977726]\n",
      "1713 [D loss: -0.573668] [G loss: -0.965413]\n",
      "1714 [D loss: 0.151458] [G loss: -0.981427]\n",
      "1715 [D loss: -0.950526] [G loss: -0.960417]\n",
      "1716 [D loss: 2.040058] [G loss: -0.802673]\n",
      "1717 [D loss: 3.141482] [G loss: -0.929559]\n",
      "1718 [D loss: -0.829766] [G loss: -0.929003]\n",
      "1719 [D loss: -0.415337] [G loss: -0.842589]\n",
      "1720 [D loss: -0.081459] [G loss: -0.045596]\n",
      "1721 [D loss: -0.346894] [G loss: -0.003825]\n",
      "1722 [D loss: 2.714200] [G loss: -0.003061]\n",
      "1723 [D loss: -0.604810] [G loss: -0.906205]\n",
      "1724 [D loss: -0.770775] [G loss: -0.921796]\n",
      "1725 [D loss: -0.656152] [G loss: -0.984681]\n",
      "1726 [D loss: -0.631297] [G loss: -0.960973]\n",
      "1727 [D loss: 2.538741] [G loss: -0.989504]\n",
      "1728 [D loss: 1.773032] [G loss: -0.999677]\n",
      "1729 [D loss: -0.007593] [G loss: -0.999823]\n",
      "1730 [D loss: 0.202376] [G loss: -0.936150]\n",
      "1731 [D loss: 0.884629] [G loss: -0.298514]\n",
      "1732 [D loss: 2.408656] [G loss: -0.888994]\n",
      "1733 [D loss: 0.160677] [G loss: -0.969830]\n",
      "1734 [D loss: -0.441934] [G loss: -0.981665]\n",
      "1735 [D loss: 0.882278] [G loss: -0.984534]\n",
      "1736 [D loss: -0.755622] [G loss: -0.960621]\n",
      "1737 [D loss: 0.508125] [G loss: -0.753346]\n",
      "1738 [D loss: -0.315537] [G loss: -0.983442]\n",
      "1739 [D loss: 0.455340] [G loss: -0.998485]\n",
      "1740 [D loss: 1.074264] [G loss: -0.855145]\n",
      "1741 [D loss: -0.721087] [G loss: -0.940605]\n",
      "1742 [D loss: 0.450740] [G loss: -0.996122]\n",
      "1743 [D loss: -0.524868] [G loss: -0.999155]\n",
      "1744 [D loss: -0.337144] [G loss: -0.990383]\n",
      "1745 [D loss: -0.597382] [G loss: -0.996330]\n",
      "1746 [D loss: 0.163701] [G loss: -0.999648]\n",
      "1747 [D loss: 0.507046] [G loss: -0.962311]\n",
      "1748 [D loss: -0.883843] [G loss: -0.999866]\n",
      "1749 [D loss: -0.480417] [G loss: -0.999852]\n",
      "1750 [D loss: -0.737694] [G loss: -0.992943]\n",
      "1751 [D loss: 2.562982] [G loss: -0.994521]\n",
      "1752 [D loss: -0.612485] [G loss: -0.999330]\n",
      "1753 [D loss: -0.712105] [G loss: -0.986892]\n",
      "1754 [D loss: -0.591932] [G loss: -0.999840]\n",
      "1755 [D loss: -0.784248] [G loss: -0.988260]\n",
      "1756 [D loss: 0.826821] [G loss: -0.997633]\n",
      "1757 [D loss: -0.156866] [G loss: -0.999672]\n",
      "1758 [D loss: 0.366673] [G loss: -0.979334]\n",
      "1759 [D loss: 0.209484] [G loss: -0.999958]\n",
      "1760 [D loss: 0.216925] [G loss: -0.999942]\n",
      "1761 [D loss: -0.886679] [G loss: -0.998359]\n",
      "1762 [D loss: -0.697199] [G loss: -0.989764]\n",
      "1763 [D loss: 4.727744] [G loss: -0.999308]\n",
      "1764 [D loss: -0.711795] [G loss: -0.998374]\n",
      "1765 [D loss: -0.835738] [G loss: -0.999646]\n",
      "1766 [D loss: 1.777687] [G loss: -0.996839]\n",
      "1767 [D loss: -0.243169] [G loss: -0.989834]\n",
      "1768 [D loss: 0.529930] [G loss: -0.999313]\n",
      "1769 [D loss: 5.455685] [G loss: -0.999705]\n",
      "1770 [D loss: -0.542984] [G loss: -0.970957]\n",
      "1771 [D loss: -0.134454] [G loss: -0.997566]\n",
      "1772 [D loss: 1.635568] [G loss: -0.999527]\n",
      "1773 [D loss: 0.657811] [G loss: -0.994846]\n",
      "1774 [D loss: 0.598697] [G loss: -0.835914]\n",
      "1775 [D loss: 0.601418] [G loss: -0.931347]\n",
      "1776 [D loss: -0.238617] [G loss: -0.985064]\n",
      "1777 [D loss: -0.755265] [G loss: -0.997706]\n",
      "1778 [D loss: 3.143816] [G loss: -0.964688]\n",
      "1779 [D loss: 0.028029] [G loss: -0.746154]\n",
      "1780 [D loss: -0.442313] [G loss: -0.970685]\n",
      "1781 [D loss: 0.323439] [G loss: -0.543123]\n",
      "1782 [D loss: 3.076105] [G loss: -0.987602]\n",
      "1783 [D loss: -0.333200] [G loss: -0.974204]\n",
      "1784 [D loss: 1.434959] [G loss: -0.998325]\n",
      "1785 [D loss: 0.568421] [G loss: -0.986741]\n",
      "1786 [D loss: -0.560933] [G loss: -0.991545]\n",
      "1787 [D loss: -0.718805] [G loss: -0.994041]\n",
      "1788 [D loss: -0.427178] [G loss: -0.999787]\n",
      "1789 [D loss: 2.361895] [G loss: -0.999987]\n",
      "1790 [D loss: -0.525579] [G loss: -0.999615]\n",
      "1791 [D loss: -0.492959] [G loss: -0.998562]\n",
      "1792 [D loss: 0.189846] [G loss: -0.996649]\n",
      "1793 [D loss: 0.849082] [G loss: -0.947905]\n",
      "1794 [D loss: 0.071400] [G loss: -0.991030]\n",
      "1795 [D loss: 1.078114] [G loss: -0.819200]\n",
      "1796 [D loss: 0.393230] [G loss: -0.982694]\n",
      "1797 [D loss: 2.235282] [G loss: -0.967870]\n",
      "1798 [D loss: -0.519381] [G loss: -0.955058]\n",
      "1799 [D loss: -0.649920] [G loss: -0.986232]\n",
      "1800 [D loss: -0.703959] [G loss: -0.986034]\n",
      "1801 [D loss: 0.264236] [G loss: -0.996700]\n",
      "1802 [D loss: -0.592746] [G loss: -0.971484]\n",
      "1803 [D loss: -0.160090] [G loss: -0.985355]\n",
      "1804 [D loss: -0.833342] [G loss: -0.972988]\n",
      "1805 [D loss: -0.046741] [G loss: -0.998773]\n",
      "1806 [D loss: -0.302162] [G loss: -0.998539]\n",
      "1807 [D loss: 0.195397] [G loss: -0.986456]\n",
      "1808 [D loss: -0.513772] [G loss: -0.987390]\n",
      "1809 [D loss: -0.860344] [G loss: -0.999662]\n",
      "1810 [D loss: 1.845633] [G loss: -0.998931]\n",
      "1811 [D loss: 0.905040] [G loss: -0.996360]\n",
      "1812 [D loss: 1.843207] [G loss: -0.998938]\n",
      "1813 [D loss: 0.510217] [G loss: -0.994604]\n",
      "1814 [D loss: 2.720272] [G loss: -0.924720]\n",
      "1815 [D loss: 5.097240] [G loss: -0.999393]\n",
      "1816 [D loss: -0.489978] [G loss: -0.999904]\n",
      "1817 [D loss: -0.781536] [G loss: -0.979249]\n",
      "1818 [D loss: 0.023678] [G loss: -0.967108]\n",
      "1819 [D loss: -0.817311] [G loss: -0.997175]\n",
      "1820 [D loss: 0.689949] [G loss: -0.979594]\n",
      "1821 [D loss: -0.346358] [G loss: -0.949735]\n",
      "1822 [D loss: -0.283979] [G loss: -0.942848]\n",
      "1823 [D loss: 1.308453] [G loss: -0.996872]\n",
      "1824 [D loss: -0.440583] [G loss: -0.989117]\n",
      "1825 [D loss: 1.920641] [G loss: -0.627876]\n",
      "1826 [D loss: -0.551510] [G loss: -0.998333]\n",
      "1827 [D loss: -0.634101] [G loss: -0.999991]\n",
      "1828 [D loss: -0.562378] [G loss: -0.999857]\n",
      "1829 [D loss: 1.131705] [G loss: -0.999485]\n",
      "1830 [D loss: 0.777843] [G loss: -0.996090]\n",
      "1831 [D loss: -0.469308] [G loss: -0.999667]\n",
      "1832 [D loss: 1.215584] [G loss: -1.000000]\n",
      "1833 [D loss: 0.278975] [G loss: -1.000000]\n",
      "1834 [D loss: 0.593943] [G loss: -0.998761]\n",
      "1835 [D loss: -0.425018] [G loss: -0.999995]\n",
      "1836 [D loss: 0.713864] [G loss: -0.999986]\n",
      "1837 [D loss: 0.643088] [G loss: -0.999941]\n",
      "1838 [D loss: -0.217692] [G loss: -0.999972]\n",
      "1839 [D loss: 1.886095] [G loss: -0.999940]\n",
      "1840 [D loss: 0.768630] [G loss: -1.000000]\n",
      "1841 [D loss: 1.146086] [G loss: -1.000000]\n",
      "1842 [D loss: -0.153797] [G loss: -0.999980]\n",
      "1843 [D loss: 0.820031] [G loss: -0.999992]\n",
      "1844 [D loss: 0.318060] [G loss: -0.996902]\n",
      "1845 [D loss: -0.936946] [G loss: -0.998925]\n",
      "1846 [D loss: 0.427281] [G loss: -0.999519]\n",
      "1847 [D loss: -0.530547] [G loss: -0.998781]\n",
      "1848 [D loss: 0.086554] [G loss: -0.981446]\n",
      "1849 [D loss: -0.886876] [G loss: -0.887018]\n",
      "1850 [D loss: -0.190930] [G loss: -0.990384]\n",
      "1851 [D loss: 0.889814] [G loss: -0.998524]\n",
      "1852 [D loss: -0.548029] [G loss: -0.998699]\n",
      "1853 [D loss: -0.846313] [G loss: -0.987614]\n",
      "1854 [D loss: -0.503592] [G loss: -0.999959]\n",
      "1855 [D loss: 1.406125] [G loss: -0.981231]\n",
      "1856 [D loss: 0.675585] [G loss: -0.999892]\n",
      "1857 [D loss: 4.253238] [G loss: -0.998653]\n",
      "1858 [D loss: 4.757860] [G loss: -0.994437]\n",
      "1859 [D loss: 2.951935] [G loss: -0.999992]\n",
      "1860 [D loss: -0.350403] [G loss: -0.999949]\n",
      "1861 [D loss: -0.757315] [G loss: -0.999963]\n",
      "1862 [D loss: 0.097262] [G loss: -0.999926]\n",
      "1863 [D loss: 1.627008] [G loss: -0.998642]\n",
      "1864 [D loss: 0.587034] [G loss: -0.995401]\n",
      "1865 [D loss: -0.167906] [G loss: -0.995254]\n",
      "1866 [D loss: -0.811904] [G loss: -0.999581]\n",
      "1867 [D loss: -0.280621] [G loss: -0.997214]\n",
      "1868 [D loss: -0.635507] [G loss: -0.999171]\n",
      "1869 [D loss: 0.019725] [G loss: -0.988741]\n",
      "1870 [D loss: 0.967408] [G loss: -0.999645]\n",
      "1871 [D loss: -0.764783] [G loss: -0.962621]\n",
      "1872 [D loss: -0.693515] [G loss: -0.975395]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGANGP()\n",
    "wgan.train(epochs=10000, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1696fff12340fbeacd8891884860ba5d4999e3a236c837e4d2afed27776e33eb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
