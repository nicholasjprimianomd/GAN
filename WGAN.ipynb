{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large amount of credit goes to:\n",
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# which I've used as a reference for this implementation\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, GlobalAveragePooling2D, Dense, Conv2DTranspose, Flatten, LeakyReLU, Reshape\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from skimage import exposure\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linux Paths for CheXpert Dataset\n",
    "\n",
    "train_dir = os.path.abspath(\"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/train.csv\")\n",
    "traindf=pd.read_csv(train_dir, dtype=str)\n",
    "\n",
    "valid_dir = os.path.abspath(\"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/valid.csv\")\n",
    "validdf=pd.read_csv(valid_dir, dtype=str)\n",
    "\n",
    "for i in range(len(traindf)):\n",
    "    traindf.iloc[i,0] = \"/media/nicholasjprimiano/8A5C72285C720F67/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/\" + traindf.iloc[i,0]\"\"\"\n",
    "    \n",
    "#Windows Paths for CheXpert Dataset\n",
    "train_dir = os.path.abspath(r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/train.csv\")\n",
    "traindf=pd.read_csv(train_dir, dtype=str)\n",
    "\n",
    "#Modify dataframe path\n",
    "for i in range(len(traindf)):\n",
    "    traindf.iloc[i,0] = r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/\" + traindf.iloc[i,0]\n",
    "\n",
    "#valid_dir = os.path.abspath(r\"C:/ML_C/CheXpert/CheXpert-Keras-master/data/default_split/CheXpert-v1.0-small/CheXpert-v1.0-small/valid.csv\")\n",
    "#validdf=pd.read_csv(valid_dir, dtype=str)\n",
    "#Only looking at AP (anterior-posterior) view xrays\n",
    "aptrainlist = []\n",
    "for i in range(len(traindf)):\n",
    "    if (traindf.iloc[i,4] == \"AP\"):\n",
    "        aptrainlist.append(traindf.iloc[i,:])\n",
    "\n",
    "aptraindf = pd.DataFrame(aptrainlist)\n",
    "\n",
    "#Only looking at xrays labeled Pneumothorax\n",
    "paths = []\n",
    "for i in range(len(aptraindf[aptraindf[\"Pneumothorax\"] == \"1.0\"][\"Path\"])):\n",
    "    paths.append(aptraindf[aptraindf[\"Pneumothorax\"] == \"1.0\"][\"Path\"].iloc[i])\n",
    "\n",
    "#Normalization called in get_imgs() not used right now\n",
    "def normalize_xray(img):\n",
    "    hist_normal = exposure.equalize_adapthist(img/np.max(img))   \n",
    "    #clache_hist_normal = exposure.equalize_adapthist(hist_normal /np.max(hist_normal))\n",
    "    #return clache_hist_normal\n",
    "    return hist_normal\n",
    "\n",
    "#load 128x128 images\n",
    "IMG_SIZE = 128\n",
    "def get_imgs(paths):\n",
    "    images = []\n",
    "    for i in paths:\n",
    "        #Normalized\n",
    "        #images.append(normalize_xray(cv2.cvtColor(cv2.resize(cv2.imread(i),(IMG_SIZE,IMG_SIZE)), cv2.COLOR_BGR2GRAY)))\n",
    "        #Gray Scale \n",
    "        images.append(cv2.cvtColor(cv2.resize(cv2.imread(i),(IMG_SIZE,IMG_SIZE)), cv2.COLOR_BGR2GRAY))\n",
    "    return images \n",
    "\n",
    "#X_train array of images with values between 0 and 1\n",
    "X_train = np.array(get_imgs(paths)).astype(np.float32)\n",
    "#reshaped X train and shifted pixzel values between -1 and 1 for tanh \n",
    "X_train_dcgan = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1) * 2. - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32,1,1,1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                              self.wasserstein_loss,\n",
    "                                              partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        \"\"\"model = Sequential()\n",
    "        model.add(Dense(128 * 32* 32, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((32,32, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 32* 32, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((32,32, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "    \n",
    "        #keras.layers.GaussianNoise(stddev=.1),\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "    \n",
    "        #keras.layers.GaussianNoise(stddev=.1),\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        \"\"\"model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=[128, 128, 1], padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, epochs, batch_size, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = X_train_dcgan\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        #X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        #X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.train_on_batch([imgs, noise],\n",
    "                                                                [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 1 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/generated_%d.png\" % epoch)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 131072)            13238272  \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 64, 64, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 64, 64, 128)       262272    \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 64, 64, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 128, 128, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 128, 128, 64)      131136    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 128, 128, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 128, 128, 1)       1025      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,633,473\n",
      "Trainable params: 13,633,089\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_31 (Conv2D)          (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " zero_padding2d_4 (ZeroPaddi  (None, 33, 33, 32)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 33, 33, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 33, 33, 32)        0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 33, 33, 32)        0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 17, 17, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 17, 17, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 17, 17, 64)        0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 17, 17, 64)        0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 17, 17, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 17, 17, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 36992)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 36993     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,041\n",
      "Trainable params: 134,593\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "0 [D loss: 8.774928] [G loss: -0.196219]\n",
      "1 [D loss: 8.161586] [G loss: -0.909119]\n",
      "2 [D loss: 8.071084] [G loss: -2.396467]\n",
      "3 [D loss: 8.333414] [G loss: -1.614836]\n",
      "4 [D loss: 7.704467] [G loss: -3.208329]\n",
      "5 [D loss: 8.096053] [G loss: -4.218995]\n",
      "6 [D loss: 7.380231] [G loss: -4.730541]\n",
      "7 [D loss: 7.648969] [G loss: -6.128632]\n",
      "8 [D loss: 7.379838] [G loss: -6.806681]\n",
      "9 [D loss: 6.905040] [G loss: -7.705241]\n",
      "10 [D loss: 8.169394] [G loss: -6.845915]\n",
      "11 [D loss: 7.280988] [G loss: -8.260743]\n",
      "12 [D loss: 7.639892] [G loss: -7.738358]\n",
      "13 [D loss: 7.688433] [G loss: -7.811811]\n",
      "14 [D loss: 7.991879] [G loss: -9.184707]\n",
      "15 [D loss: 7.815954] [G loss: -9.764388]\n",
      "16 [D loss: 9.188570] [G loss: -10.598742]\n",
      "17 [D loss: 7.121658] [G loss: -9.350203]\n",
      "18 [D loss: 8.454301] [G loss: -10.049881]\n",
      "19 [D loss: 7.643580] [G loss: -10.688572]\n",
      "20 [D loss: 8.729094] [G loss: -10.943956]\n",
      "21 [D loss: 7.627492] [G loss: -11.295536]\n",
      "22 [D loss: 9.192208] [G loss: -11.898598]\n",
      "23 [D loss: 8.158474] [G loss: -13.068441]\n",
      "24 [D loss: 9.085710] [G loss: -13.247993]\n",
      "25 [D loss: 8.713089] [G loss: -13.210888]\n",
      "26 [D loss: 7.542562] [G loss: -12.662291]\n",
      "27 [D loss: 7.331339] [G loss: -14.114600]\n",
      "28 [D loss: 8.206361] [G loss: -13.206171]\n",
      "29 [D loss: 8.218671] [G loss: -14.324121]\n",
      "30 [D loss: 6.729086] [G loss: -15.158976]\n",
      "31 [D loss: 6.748726] [G loss: -14.567371]\n",
      "32 [D loss: 6.503094] [G loss: -14.762619]\n",
      "33 [D loss: 7.451143] [G loss: -15.392772]\n",
      "34 [D loss: 8.207751] [G loss: -15.933613]\n",
      "35 [D loss: 6.188290] [G loss: -16.434467]\n",
      "36 [D loss: 6.728909] [G loss: -15.475389]\n",
      "37 [D loss: 5.523273] [G loss: -14.899303]\n",
      "38 [D loss: 4.810134] [G loss: -15.559355]\n",
      "39 [D loss: 4.086760] [G loss: -15.372370]\n",
      "40 [D loss: 6.143665] [G loss: -17.133440]\n",
      "41 [D loss: 5.479300] [G loss: -16.581024]\n",
      "42 [D loss: 4.726484] [G loss: -16.187775]\n",
      "43 [D loss: 4.325424] [G loss: -15.977333]\n",
      "44 [D loss: 4.403599] [G loss: -16.311577]\n",
      "45 [D loss: 3.730007] [G loss: -16.826260]\n",
      "46 [D loss: 2.582632] [G loss: -16.593018]\n",
      "47 [D loss: 2.308507] [G loss: -16.891695]\n",
      "48 [D loss: 0.652389] [G loss: -17.195808]\n",
      "49 [D loss: 2.124107] [G loss: -16.617706]\n",
      "50 [D loss: 0.520607] [G loss: -15.405558]\n",
      "51 [D loss: 1.442990] [G loss: -15.498273]\n",
      "52 [D loss: -1.158992] [G loss: -15.629953]\n",
      "53 [D loss: -0.924750] [G loss: -15.637158]\n",
      "54 [D loss: -0.863100] [G loss: -15.982435]\n",
      "55 [D loss: 0.219108] [G loss: -16.700130]\n",
      "56 [D loss: -1.632960] [G loss: -16.269672]\n",
      "57 [D loss: -1.149655] [G loss: -16.271191]\n",
      "58 [D loss: -4.335794] [G loss: -15.793949]\n",
      "59 [D loss: -5.405341] [G loss: -14.851070]\n",
      "60 [D loss: -4.238123] [G loss: -17.835489]\n",
      "61 [D loss: -5.821033] [G loss: -16.852154]\n",
      "62 [D loss: -5.062115] [G loss: -18.655956]\n",
      "63 [D loss: -6.638024] [G loss: -18.054932]\n",
      "64 [D loss: -11.287383] [G loss: -19.719795]\n",
      "65 [D loss: -10.446478] [G loss: -21.499542]\n",
      "66 [D loss: -6.634962] [G loss: -20.711433]\n",
      "67 [D loss: -7.977324] [G loss: -20.368790]\n",
      "68 [D loss: -11.334178] [G loss: -21.575302]\n",
      "69 [D loss: -13.605381] [G loss: -20.063162]\n",
      "70 [D loss: -8.660336] [G loss: -20.466463]\n",
      "71 [D loss: -10.642115] [G loss: -20.659416]\n",
      "72 [D loss: -10.514002] [G loss: -18.901075]\n",
      "73 [D loss: -16.260780] [G loss: -20.291267]\n",
      "74 [D loss: -15.599176] [G loss: -18.358435]\n",
      "75 [D loss: -16.000492] [G loss: -19.732336]\n",
      "76 [D loss: -18.319765] [G loss: -16.848572]\n",
      "77 [D loss: -21.372898] [G loss: -17.713104]\n",
      "78 [D loss: -23.741796] [G loss: -20.619713]\n",
      "79 [D loss: -24.014071] [G loss: -22.171371]\n",
      "80 [D loss: -30.745579] [G loss: -17.995903]\n",
      "81 [D loss: -31.887127] [G loss: -15.533073]\n",
      "82 [D loss: -30.412939] [G loss: -19.288551]\n",
      "83 [D loss: -34.896751] [G loss: -17.001348]\n",
      "84 [D loss: -36.173244] [G loss: -14.140324]\n",
      "85 [D loss: -32.252686] [G loss: -13.758691]\n",
      "86 [D loss: -43.559929] [G loss: -14.931516]\n",
      "87 [D loss: -43.355843] [G loss: -12.188397]\n",
      "88 [D loss: -51.963951] [G loss: -13.107941]\n",
      "89 [D loss: -54.769897] [G loss: -10.789253]\n",
      "90 [D loss: -62.639565] [G loss: -9.684949]\n",
      "91 [D loss: -64.521103] [G loss: -6.849862]\n",
      "92 [D loss: -64.565094] [G loss: -9.217722]\n",
      "93 [D loss: -70.684784] [G loss: -6.488573]\n",
      "94 [D loss: -79.007431] [G loss: -8.926426]\n",
      "95 [D loss: -81.687836] [G loss: -7.930010]\n",
      "96 [D loss: -83.013512] [G loss: -2.896543]\n",
      "97 [D loss: -98.118652] [G loss: -1.887702]\n",
      "98 [D loss: -105.171913] [G loss: -5.177629]\n",
      "99 [D loss: -106.082588] [G loss: 3.312724]\n",
      "100 [D loss: -109.707626] [G loss: 7.817567]\n",
      "101 [D loss: -114.763466] [G loss: 5.168045]\n",
      "102 [D loss: -123.077621] [G loss: 5.413758]\n",
      "103 [D loss: -125.129700] [G loss: 3.496508]\n",
      "104 [D loss: -130.099716] [G loss: 0.426475]\n",
      "105 [D loss: -144.146194] [G loss: 4.381199]\n",
      "106 [D loss: -145.346725] [G loss: 5.224946]\n",
      "107 [D loss: -159.782806] [G loss: 4.652310]\n",
      "108 [D loss: -165.077347] [G loss: 6.119082]\n",
      "109 [D loss: -169.949188] [G loss: 9.337305]\n",
      "110 [D loss: -183.403870] [G loss: 4.239276]\n",
      "111 [D loss: -194.753265] [G loss: -6.130705]\n",
      "112 [D loss: -204.035919] [G loss: -10.046360]\n",
      "113 [D loss: -207.669586] [G loss: -14.791613]\n",
      "114 [D loss: -217.105881] [G loss: -20.031517]\n",
      "115 [D loss: -223.230362] [G loss: -21.175194]\n",
      "116 [D loss: -220.957031] [G loss: -16.130619]\n",
      "117 [D loss: -231.068359] [G loss: -10.621816]\n",
      "118 [D loss: -229.474930] [G loss: -12.003534]\n",
      "119 [D loss: -240.810425] [G loss: -10.144458]\n",
      "120 [D loss: -249.872177] [G loss: -8.977709]\n",
      "121 [D loss: -273.487335] [G loss: 0.834246]\n",
      "122 [D loss: -273.113617] [G loss: 2.317600]\n",
      "123 [D loss: -274.655457] [G loss: -0.503190]\n",
      "124 [D loss: -281.412476] [G loss: 11.334761]\n",
      "125 [D loss: -292.665100] [G loss: 8.219533]\n",
      "126 [D loss: -310.350037] [G loss: 11.500820]\n",
      "127 [D loss: -329.238068] [G loss: 11.388528]\n",
      "128 [D loss: -326.079712] [G loss: 11.510878]\n",
      "129 [D loss: -328.019287] [G loss: 8.146711]\n",
      "130 [D loss: -340.967590] [G loss: 12.058887]\n",
      "131 [D loss: -351.169403] [G loss: 9.181655]\n",
      "132 [D loss: -353.740479] [G loss: 14.715059]\n",
      "133 [D loss: -365.980011] [G loss: 13.702657]\n",
      "134 [D loss: -363.349426] [G loss: 16.412651]\n",
      "135 [D loss: -388.105804] [G loss: 1.990472]\n",
      "136 [D loss: -394.567993] [G loss: -3.876071]\n",
      "137 [D loss: -386.934174] [G loss: 7.542303]\n",
      "138 [D loss: -395.363037] [G loss: 2.024252]\n",
      "139 [D loss: -406.326569] [G loss: 8.478395]\n",
      "140 [D loss: -411.876770] [G loss: 2.913993]\n",
      "141 [D loss: -429.606476] [G loss: 1.839014]\n",
      "142 [D loss: -419.739594] [G loss: -3.545528]\n",
      "143 [D loss: -448.698975] [G loss: -2.804762]\n",
      "144 [D loss: -444.403778] [G loss: 2.843391]\n",
      "145 [D loss: -443.609802] [G loss: -1.896263]\n",
      "146 [D loss: -469.792267] [G loss: -6.705533]\n",
      "147 [D loss: -453.821320] [G loss: -10.339706]\n",
      "148 [D loss: -469.831390] [G loss: -7.641604]\n",
      "149 [D loss: -488.048706] [G loss: -25.813869]\n",
      "150 [D loss: -491.011108] [G loss: -33.688873]\n",
      "151 [D loss: -528.044128] [G loss: -44.360981]\n",
      "152 [D loss: -530.642151] [G loss: -58.398003]\n",
      "153 [D loss: -533.022583] [G loss: -74.696693]\n",
      "154 [D loss: -510.930542] [G loss: -129.681030]\n",
      "155 [D loss: -421.340454] [G loss: -135.060974]\n",
      "156 [D loss: -280.906403] [G loss: -176.987976]\n",
      "157 [D loss: -220.247803] [G loss: -167.605835]\n",
      "158 [D loss: -90.001770] [G loss: -176.697937]\n",
      "159 [D loss: -46.832687] [G loss: -185.499557]\n",
      "160 [D loss: 29.232399] [G loss: -184.770355]\n",
      "161 [D loss: 130.533188] [G loss: -196.155319]\n",
      "162 [D loss: 229.548096] [G loss: -223.848907]\n",
      "163 [D loss: 269.448059] [G loss: -208.598221]\n",
      "164 [D loss: 288.699524] [G loss: -210.319305]\n",
      "165 [D loss: 478.593994] [G loss: -223.162109]\n",
      "166 [D loss: 447.119446] [G loss: -230.592834]\n",
      "167 [D loss: 751.589600] [G loss: -232.081696]\n",
      "168 [D loss: 711.275513] [G loss: -237.959579]\n",
      "169 [D loss: 738.094727] [G loss: -239.857269]\n",
      "170 [D loss: 860.818481] [G loss: -239.587006]\n",
      "171 [D loss: 918.903381] [G loss: -237.557556]\n",
      "172 [D loss: 1025.807007] [G loss: -236.059036]\n",
      "173 [D loss: 1069.939209] [G loss: -245.606705]\n",
      "174 [D loss: 1066.311279] [G loss: -236.975937]\n",
      "175 [D loss: 1251.059082] [G loss: -228.950577]\n",
      "176 [D loss: 1170.908081] [G loss: -217.150177]\n",
      "177 [D loss: 1109.344116] [G loss: -199.535278]\n",
      "178 [D loss: 1310.687988] [G loss: -178.544708]\n",
      "179 [D loss: 1323.745117] [G loss: -158.002579]\n",
      "180 [D loss: 1190.480225] [G loss: -117.183212]\n",
      "181 [D loss: 1018.073059] [G loss: -122.495003]\n",
      "182 [D loss: 1046.531982] [G loss: -114.632187]\n",
      "183 [D loss: 921.425903] [G loss: -110.125832]\n",
      "184 [D loss: 871.904358] [G loss: -109.641220]\n",
      "185 [D loss: 747.760132] [G loss: -124.965714]\n",
      "186 [D loss: 568.434814] [G loss: -112.910324]\n",
      "187 [D loss: 494.591095] [G loss: -129.191956]\n",
      "188 [D loss: 550.269165] [G loss: -120.726410]\n",
      "189 [D loss: 367.034393] [G loss: -105.405396]\n",
      "190 [D loss: 293.193420] [G loss: -100.389603]\n",
      "191 [D loss: 219.747330] [G loss: -99.566376]\n",
      "192 [D loss: 199.721359] [G loss: -106.543320]\n",
      "193 [D loss: 141.873779] [G loss: -87.326332]\n",
      "194 [D loss: 102.572113] [G loss: -82.950974]\n",
      "195 [D loss: 66.255814] [G loss: -69.755341]\n",
      "196 [D loss: 57.371452] [G loss: -58.292271]\n",
      "197 [D loss: 13.399273] [G loss: -57.687489]\n",
      "198 [D loss: -22.591248] [G loss: -40.171692]\n",
      "199 [D loss: -51.519936] [G loss: -33.545185]\n",
      "200 [D loss: -28.975964] [G loss: -39.444733]\n",
      "201 [D loss: -59.110909] [G loss: -30.102448]\n",
      "202 [D loss: -69.176506] [G loss: -33.781334]\n",
      "203 [D loss: -80.570724] [G loss: -23.311970]\n",
      "204 [D loss: -80.740250] [G loss: -20.649694]\n",
      "205 [D loss: -110.991165] [G loss: -11.172464]\n",
      "206 [D loss: -107.360687] [G loss: -2.242301]\n",
      "207 [D loss: -138.599167] [G loss: -2.230158]\n",
      "208 [D loss: -155.276337] [G loss: 8.154070]\n",
      "209 [D loss: -182.097244] [G loss: 2.957488]\n",
      "210 [D loss: -195.306030] [G loss: 3.060679]\n",
      "211 [D loss: -190.409454] [G loss: 9.104232]\n",
      "212 [D loss: -210.592072] [G loss: 10.045671]\n",
      "213 [D loss: -230.859833] [G loss: 7.992117]\n",
      "214 [D loss: -239.549652] [G loss: 15.491969]\n",
      "215 [D loss: -250.386520] [G loss: 4.171979]\n",
      "216 [D loss: -252.683197] [G loss: 0.643248]\n",
      "217 [D loss: -257.407806] [G loss: -6.107903]\n",
      "218 [D loss: -244.356232] [G loss: -13.845934]\n",
      "219 [D loss: -248.274445] [G loss: -16.778282]\n",
      "220 [D loss: -227.121796] [G loss: -15.218017]\n",
      "221 [D loss: -246.388321] [G loss: -27.318445]\n",
      "222 [D loss: -242.990463] [G loss: -26.276192]\n",
      "223 [D loss: -246.016357] [G loss: -19.377171]\n",
      "224 [D loss: -245.365326] [G loss: -20.116505]\n",
      "225 [D loss: -231.186707] [G loss: -31.616013]\n",
      "226 [D loss: -235.650650] [G loss: -21.354267]\n",
      "227 [D loss: -202.531586] [G loss: -50.162628]\n",
      "228 [D loss: -156.762039] [G loss: -69.162064]\n",
      "229 [D loss: -203.215057] [G loss: -33.285164]\n",
      "230 [D loss: -219.168839] [G loss: -34.693443]\n",
      "231 [D loss: -231.357376] [G loss: -34.006458]\n",
      "232 [D loss: -256.703674] [G loss: -22.575514]\n",
      "233 [D loss: -280.980957] [G loss: -12.377443]\n",
      "234 [D loss: -287.647461] [G loss: -35.286678]\n",
      "235 [D loss: -265.254517] [G loss: -28.290852]\n",
      "236 [D loss: -272.444214] [G loss: -39.818245]\n",
      "237 [D loss: -291.867706] [G loss: -35.506355]\n",
      "238 [D loss: -291.442871] [G loss: -52.127281]\n",
      "239 [D loss: -277.772552] [G loss: -56.992088]\n",
      "240 [D loss: -295.200073] [G loss: -49.797558]\n",
      "241 [D loss: -301.044952] [G loss: -53.879616]\n",
      "242 [D loss: -317.620605] [G loss: -45.252388]\n",
      "243 [D loss: -333.925873] [G loss: -30.954418]\n",
      "244 [D loss: -325.609680] [G loss: -43.393593]\n",
      "245 [D loss: -325.429413] [G loss: -39.269741]\n",
      "246 [D loss: -330.465881] [G loss: -61.891483]\n",
      "247 [D loss: -310.890076] [G loss: -61.189377]\n",
      "248 [D loss: -314.956360] [G loss: -54.377865]\n",
      "249 [D loss: -313.825256] [G loss: -64.713051]\n",
      "250 [D loss: -330.534485] [G loss: -59.815331]\n",
      "251 [D loss: -345.784973] [G loss: -54.871532]\n",
      "252 [D loss: -334.927460] [G loss: -66.214691]\n",
      "253 [D loss: -338.393463] [G loss: -67.067543]\n",
      "254 [D loss: -338.956085] [G loss: -78.969131]\n",
      "255 [D loss: -343.068542] [G loss: -74.357681]\n",
      "256 [D loss: -360.420654] [G loss: -61.668034]\n",
      "257 [D loss: -362.869965] [G loss: -80.004272]\n",
      "258 [D loss: -362.523224] [G loss: -66.513664]\n",
      "259 [D loss: -369.477692] [G loss: -80.381569]\n",
      "260 [D loss: -400.202454] [G loss: -67.273720]\n",
      "261 [D loss: -386.272980] [G loss: -64.098549]\n",
      "262 [D loss: -402.275330] [G loss: -77.065033]\n",
      "263 [D loss: -375.505188] [G loss: -78.031837]\n",
      "264 [D loss: -407.184845] [G loss: -84.151039]\n",
      "265 [D loss: -400.780640] [G loss: -77.029495]\n",
      "266 [D loss: -403.925079] [G loss: -85.228027]\n",
      "267 [D loss: -367.536621] [G loss: -89.566071]\n",
      "268 [D loss: -432.828186] [G loss: -97.080963]\n",
      "269 [D loss: -454.233032] [G loss: -116.933182]\n",
      "270 [D loss: -449.811737] [G loss: -121.654404]\n",
      "271 [D loss: -433.561798] [G loss: -129.988022]\n",
      "272 [D loss: -417.514740] [G loss: -137.891571]\n",
      "273 [D loss: -391.046570] [G loss: -132.120758]\n",
      "274 [D loss: -392.425232] [G loss: -128.363251]\n",
      "275 [D loss: -348.773865] [G loss: -140.557465]\n",
      "276 [D loss: -344.285675] [G loss: -137.467178]\n",
      "277 [D loss: -337.834442] [G loss: -129.078537]\n",
      "278 [D loss: -332.168182] [G loss: -123.343613]\n",
      "279 [D loss: -321.907806] [G loss: -121.826195]\n",
      "280 [D loss: -306.076141] [G loss: -130.967773]\n",
      "281 [D loss: -267.552521] [G loss: -130.996933]\n",
      "282 [D loss: -268.508514] [G loss: -131.898926]\n",
      "283 [D loss: -227.046692] [G loss: -112.798920]\n",
      "284 [D loss: -247.821564] [G loss: -99.514153]\n",
      "285 [D loss: -225.409363] [G loss: -104.902489]\n",
      "286 [D loss: -203.991104] [G loss: -98.239899]\n",
      "287 [D loss: -228.476913] [G loss: -93.644516]\n",
      "288 [D loss: -189.399826] [G loss: -87.600449]\n",
      "289 [D loss: -209.926910] [G loss: -76.275009]\n",
      "290 [D loss: -197.535385] [G loss: -74.924438]\n",
      "291 [D loss: -208.361191] [G loss: -59.570450]\n",
      "292 [D loss: -147.151947] [G loss: -76.415680]\n",
      "293 [D loss: -170.118958] [G loss: -62.012581]\n",
      "294 [D loss: -162.746689] [G loss: -70.487793]\n",
      "295 [D loss: -143.737701] [G loss: -56.673370]\n",
      "296 [D loss: -150.990250] [G loss: -63.326221]\n",
      "297 [D loss: -202.757355] [G loss: -44.741711]\n",
      "298 [D loss: -182.372986] [G loss: -34.828331]\n",
      "299 [D loss: -180.399780] [G loss: -45.911182]\n",
      "300 [D loss: -180.564056] [G loss: -54.891285]\n",
      "301 [D loss: -193.785553] [G loss: -79.282997]\n",
      "302 [D loss: -209.383011] [G loss: -90.774208]\n",
      "303 [D loss: -196.477966] [G loss: -109.035149]\n",
      "304 [D loss: -148.414612] [G loss: -112.204605]\n",
      "305 [D loss: -103.381210] [G loss: -118.417366]\n",
      "306 [D loss: -41.335159] [G loss: -129.122116]\n",
      "307 [D loss: -19.982681] [G loss: -129.916550]\n",
      "308 [D loss: 0.472237] [G loss: -125.767395]\n",
      "309 [D loss: 30.803291] [G loss: -120.050446]\n",
      "310 [D loss: 20.880051] [G loss: -122.813927]\n",
      "311 [D loss: 33.661728] [G loss: -138.149139]\n",
      "312 [D loss: 27.141254] [G loss: -130.053406]\n",
      "313 [D loss: 16.793190] [G loss: -113.522697]\n",
      "314 [D loss: 35.663582] [G loss: -115.264526]\n",
      "315 [D loss: 24.388294] [G loss: -112.229233]\n",
      "316 [D loss: 22.326603] [G loss: -99.420776]\n",
      "317 [D loss: 18.187889] [G loss: -96.747902]\n",
      "318 [D loss: 24.355328] [G loss: -91.357971]\n",
      "319 [D loss: 16.054485] [G loss: -77.277405]\n",
      "320 [D loss: 11.422630] [G loss: -77.777588]\n",
      "321 [D loss: 0.693424] [G loss: -66.461784]\n",
      "322 [D loss: 3.664616] [G loss: -70.078552]\n",
      "323 [D loss: 0.966492] [G loss: -55.072304]\n",
      "324 [D loss: -13.109840] [G loss: -57.704880]\n",
      "325 [D loss: -15.986919] [G loss: -43.221516]\n",
      "326 [D loss: -15.254932] [G loss: -43.065117]\n",
      "327 [D loss: 9.632690] [G loss: -37.112808]\n",
      "328 [D loss: -15.903831] [G loss: -28.957802]\n",
      "329 [D loss: -23.149300] [G loss: -32.278023]\n",
      "330 [D loss: -39.995964] [G loss: -24.568640]\n",
      "331 [D loss: -42.615341] [G loss: -16.761522]\n",
      "332 [D loss: -35.076298] [G loss: -14.531043]\n",
      "333 [D loss: -42.819901] [G loss: -17.650223]\n",
      "334 [D loss: -49.196068] [G loss: -2.608904]\n",
      "335 [D loss: -39.725639] [G loss: -0.303555]\n",
      "336 [D loss: -50.893570] [G loss: 2.502481]\n",
      "337 [D loss: -47.533623] [G loss: 4.462853]\n",
      "338 [D loss: -58.499760] [G loss: 15.519729]\n",
      "339 [D loss: -46.681057] [G loss: 8.478020]\n",
      "340 [D loss: -47.939590] [G loss: 12.044779]\n",
      "341 [D loss: -39.595795] [G loss: 19.000122]\n",
      "342 [D loss: -43.381378] [G loss: 21.961006]\n",
      "343 [D loss: -45.061394] [G loss: 27.970596]\n",
      "344 [D loss: -46.102371] [G loss: 23.857517]\n",
      "345 [D loss: -28.940800] [G loss: 18.448435]\n",
      "346 [D loss: -36.361443] [G loss: 25.763536]\n",
      "347 [D loss: -18.062637] [G loss: 20.129826]\n",
      "348 [D loss: -22.216278] [G loss: 20.466078]\n",
      "349 [D loss: -19.930761] [G loss: 14.571366]\n",
      "350 [D loss: -12.824215] [G loss: 18.416340]\n",
      "351 [D loss: -18.024868] [G loss: 8.700857]\n",
      "352 [D loss: -17.682137] [G loss: 12.358126]\n",
      "353 [D loss: -12.711815] [G loss: 7.418766]\n",
      "354 [D loss: 14.428894] [G loss: 2.460784]\n",
      "355 [D loss: -4.496361] [G loss: -3.128314]\n",
      "356 [D loss: -2.021782] [G loss: -6.671494]\n",
      "357 [D loss: -7.674225] [G loss: -9.792755]\n",
      "358 [D loss: -12.034946] [G loss: -14.091138]\n",
      "359 [D loss: -12.350010] [G loss: -11.871313]\n",
      "360 [D loss: -23.464020] [G loss: -17.285313]\n",
      "361 [D loss: -27.009897] [G loss: -19.180822]\n",
      "362 [D loss: -24.888439] [G loss: -22.942236]\n",
      "363 [D loss: -36.834545] [G loss: -28.408554]\n",
      "364 [D loss: -20.384186] [G loss: -35.863594]\n",
      "365 [D loss: -6.130575] [G loss: -49.821163]\n",
      "366 [D loss: 1.186390] [G loss: -64.228271]\n",
      "367 [D loss: -8.187519] [G loss: -67.302803]\n",
      "368 [D loss: -9.482664] [G loss: -72.414780]\n",
      "369 [D loss: -12.261934] [G loss: -78.671646]\n",
      "370 [D loss: -20.899481] [G loss: -72.626053]\n",
      "371 [D loss: -18.454151] [G loss: -75.311737]\n",
      "372 [D loss: -34.537697] [G loss: -74.385536]\n",
      "373 [D loss: -29.721039] [G loss: -76.454346]\n",
      "374 [D loss: -38.984737] [G loss: -69.907784]\n",
      "375 [D loss: -44.059963] [G loss: -71.050797]\n",
      "376 [D loss: -42.695717] [G loss: -74.651962]\n",
      "377 [D loss: -40.227173] [G loss: -84.531509]\n",
      "378 [D loss: -28.867546] [G loss: -96.862823]\n",
      "379 [D loss: -34.997955] [G loss: -96.675385]\n",
      "380 [D loss: -34.127586] [G loss: -96.108765]\n",
      "381 [D loss: -48.354126] [G loss: -92.074829]\n",
      "382 [D loss: -65.896759] [G loss: -89.194473]\n",
      "383 [D loss: -72.204262] [G loss: -77.435974]\n",
      "384 [D loss: -85.962128] [G loss: -79.014389]\n",
      "385 [D loss: -87.243614] [G loss: -80.802490]\n",
      "386 [D loss: -113.294487] [G loss: -81.264084]\n",
      "387 [D loss: -111.227150] [G loss: -76.177643]\n",
      "388 [D loss: -135.596069] [G loss: -72.716797]\n",
      "389 [D loss: -156.699509] [G loss: -73.444519]\n",
      "390 [D loss: -178.052612] [G loss: -75.468658]\n",
      "391 [D loss: -184.923615] [G loss: -81.840546]\n",
      "392 [D loss: -189.940079] [G loss: -98.370644]\n",
      "393 [D loss: -225.045868] [G loss: -107.906914]\n",
      "394 [D loss: -229.843018] [G loss: -121.281456]\n",
      "395 [D loss: -239.891663] [G loss: -135.403641]\n",
      "396 [D loss: -234.971375] [G loss: -152.810760]\n",
      "397 [D loss: -219.098785] [G loss: -166.767502]\n",
      "398 [D loss: -209.309296] [G loss: -175.789032]\n",
      "399 [D loss: -174.293549] [G loss: -168.763702]\n",
      "400 [D loss: -132.126343] [G loss: -176.306549]\n",
      "401 [D loss: -97.854378] [G loss: -185.085510]\n",
      "402 [D loss: -101.670647] [G loss: -182.311279]\n",
      "403 [D loss: -66.407028] [G loss: -198.228973]\n",
      "404 [D loss: -51.951561] [G loss: -211.110199]\n",
      "405 [D loss: -35.491280] [G loss: -206.590851]\n",
      "406 [D loss: -23.587292] [G loss: -213.547180]\n",
      "407 [D loss: -28.607821] [G loss: -212.270889]\n",
      "408 [D loss: -31.008612] [G loss: -209.502502]\n",
      "409 [D loss: -20.465652] [G loss: -210.055450]\n",
      "410 [D loss: -32.943802] [G loss: -201.720123]\n",
      "411 [D loss: -29.170557] [G loss: -201.122299]\n",
      "412 [D loss: -23.344072] [G loss: -199.376877]\n",
      "413 [D loss: -27.500385] [G loss: -197.415421]\n",
      "414 [D loss: -37.332024] [G loss: -186.290955]\n",
      "415 [D loss: -40.760223] [G loss: -182.004639]\n",
      "416 [D loss: -27.416811] [G loss: -180.705597]\n",
      "417 [D loss: -35.208618] [G loss: -182.498718]\n",
      "418 [D loss: -33.232529] [G loss: -176.562744]\n",
      "419 [D loss: -42.597031] [G loss: -164.880737]\n",
      "420 [D loss: -45.452293] [G loss: -156.612808]\n",
      "421 [D loss: -57.930771] [G loss: -155.694687]\n",
      "422 [D loss: -64.593826] [G loss: -152.331268]\n",
      "423 [D loss: -78.788086] [G loss: -148.015381]\n",
      "424 [D loss: -54.597458] [G loss: -150.961212]\n",
      "425 [D loss: -45.562012] [G loss: -158.108597]\n",
      "426 [D loss: -32.877033] [G loss: -160.000031]\n",
      "427 [D loss: -17.026289] [G loss: -175.431534]\n",
      "428 [D loss: 1.020900] [G loss: -184.941895]\n",
      "429 [D loss: -2.219519] [G loss: -183.481796]\n",
      "430 [D loss: -6.714293] [G loss: -182.448517]\n",
      "431 [D loss: -12.481553] [G loss: -182.757858]\n",
      "432 [D loss: -7.120910] [G loss: -188.044922]\n",
      "433 [D loss: -8.089203] [G loss: -182.337311]\n",
      "434 [D loss: -13.984800] [G loss: -179.921204]\n",
      "435 [D loss: -19.193855] [G loss: -174.901215]\n",
      "436 [D loss: -15.423321] [G loss: -172.328217]\n",
      "437 [D loss: -15.089224] [G loss: -171.990784]\n",
      "438 [D loss: -24.733101] [G loss: -164.865875]\n",
      "439 [D loss: -22.890129] [G loss: -166.398315]\n",
      "440 [D loss: -18.767498] [G loss: -166.406311]\n",
      "441 [D loss: -21.939690] [G loss: -161.727615]\n",
      "442 [D loss: -17.828457] [G loss: -169.692368]\n",
      "443 [D loss: -18.647675] [G loss: -161.178329]\n",
      "444 [D loss: -18.031925] [G loss: -168.094833]\n",
      "445 [D loss: -17.223036] [G loss: -159.119659]\n",
      "446 [D loss: -20.557489] [G loss: -161.285797]\n",
      "447 [D loss: -20.442051] [G loss: -156.292023]\n",
      "448 [D loss: -20.118538] [G loss: -151.806625]\n",
      "449 [D loss: -25.307251] [G loss: -153.112732]\n",
      "450 [D loss: -20.058430] [G loss: -151.425568]\n",
      "451 [D loss: -27.675108] [G loss: -146.129333]\n",
      "452 [D loss: -38.403950] [G loss: -143.167984]\n",
      "453 [D loss: -34.684269] [G loss: -147.248749]\n",
      "454 [D loss: -41.828194] [G loss: -136.882965]\n",
      "455 [D loss: -56.166306] [G loss: -129.176147]\n",
      "456 [D loss: -59.280910] [G loss: -133.461655]\n",
      "457 [D loss: -83.967064] [G loss: -129.076218]\n",
      "458 [D loss: -85.035042] [G loss: -129.179749]\n",
      "459 [D loss: -102.515251] [G loss: -126.025475]\n",
      "460 [D loss: -93.094566] [G loss: -132.256485]\n",
      "461 [D loss: -96.074928] [G loss: -129.780533]\n",
      "462 [D loss: -109.974426] [G loss: -131.988647]\n",
      "463 [D loss: -111.145065] [G loss: -139.588806]\n",
      "464 [D loss: -106.362854] [G loss: -145.657745]\n",
      "465 [D loss: -109.996155] [G loss: -147.625000]\n",
      "466 [D loss: -96.077888] [G loss: -156.020599]\n",
      "467 [D loss: -82.345695] [G loss: -163.512161]\n",
      "468 [D loss: -115.525002] [G loss: -161.961639]\n",
      "469 [D loss: -90.912018] [G loss: -164.278458]\n",
      "470 [D loss: -75.646042] [G loss: -177.469452]\n",
      "471 [D loss: -61.124084] [G loss: -177.063660]\n",
      "472 [D loss: -70.050247] [G loss: -176.874542]\n",
      "473 [D loss: -94.526077] [G loss: -177.018448]\n",
      "474 [D loss: -68.626740] [G loss: -180.398224]\n",
      "475 [D loss: -63.202530] [G loss: -184.880005]\n",
      "476 [D loss: -62.741997] [G loss: -188.363098]\n",
      "477 [D loss: -58.466366] [G loss: -189.050339]\n",
      "478 [D loss: -43.261147] [G loss: -197.884033]\n",
      "479 [D loss: -37.909790] [G loss: -204.537872]\n",
      "480 [D loss: -34.464619] [G loss: -208.806122]\n",
      "481 [D loss: -40.784134] [G loss: -199.443726]\n",
      "482 [D loss: -33.281384] [G loss: -204.711273]\n",
      "483 [D loss: -42.128963] [G loss: -196.472137]\n",
      "484 [D loss: -43.256493] [G loss: -198.943268]\n",
      "485 [D loss: -43.514019] [G loss: -199.691772]\n",
      "486 [D loss: -47.306450] [G loss: -195.501007]\n",
      "487 [D loss: -61.430199] [G loss: -182.185333]\n",
      "488 [D loss: -50.595119] [G loss: -186.896454]\n",
      "489 [D loss: -52.071877] [G loss: -194.306778]\n",
      "490 [D loss: -51.940941] [G loss: -193.541382]\n",
      "491 [D loss: -57.367699] [G loss: -188.092758]\n",
      "492 [D loss: -62.514256] [G loss: -189.584839]\n",
      "493 [D loss: -69.672592] [G loss: -183.670334]\n",
      "494 [D loss: -75.191208] [G loss: -180.312988]\n",
      "495 [D loss: -73.770149] [G loss: -178.205261]\n",
      "496 [D loss: -73.373154] [G loss: -177.577332]\n",
      "497 [D loss: -70.280304] [G loss: -182.041550]\n",
      "498 [D loss: -87.540527] [G loss: -169.774445]\n",
      "499 [D loss: -87.161911] [G loss: -172.996277]\n",
      "500 [D loss: -86.538048] [G loss: -170.275452]\n",
      "501 [D loss: -85.473099] [G loss: -164.380203]\n",
      "502 [D loss: -90.692688] [G loss: -159.571259]\n",
      "503 [D loss: -91.228180] [G loss: -165.447296]\n",
      "504 [D loss: -106.526833] [G loss: -162.774506]\n",
      "505 [D loss: -99.436852] [G loss: -158.724457]\n",
      "506 [D loss: -98.767807] [G loss: -167.803696]\n",
      "507 [D loss: -101.520721] [G loss: -165.587036]\n",
      "508 [D loss: -103.282280] [G loss: -155.639648]\n",
      "509 [D loss: -107.356552] [G loss: -163.886749]\n",
      "510 [D loss: -101.910507] [G loss: -159.447937]\n",
      "511 [D loss: -106.441254] [G loss: -158.072571]\n",
      "512 [D loss: -103.208366] [G loss: -159.776367]\n",
      "513 [D loss: -119.040588] [G loss: -159.282883]\n",
      "514 [D loss: -104.652260] [G loss: -165.492966]\n",
      "515 [D loss: -114.029892] [G loss: -161.590332]\n",
      "516 [D loss: -113.915779] [G loss: -165.537781]\n",
      "517 [D loss: -105.197952] [G loss: -157.572128]\n",
      "518 [D loss: -106.410751] [G loss: -156.521149]\n",
      "519 [D loss: -104.167885] [G loss: -162.568390]\n",
      "520 [D loss: -104.555664] [G loss: -160.838409]\n",
      "521 [D loss: -106.002769] [G loss: -161.066132]\n",
      "522 [D loss: -107.727509] [G loss: -161.046051]\n",
      "523 [D loss: -112.435188] [G loss: -157.061737]\n",
      "524 [D loss: -117.279312] [G loss: -159.285553]\n",
      "525 [D loss: -108.573608] [G loss: -161.511688]\n",
      "526 [D loss: -129.295197] [G loss: -155.929199]\n",
      "527 [D loss: -124.740128] [G loss: -157.327881]\n",
      "528 [D loss: -138.103729] [G loss: -148.342178]\n",
      "529 [D loss: -145.589767] [G loss: -151.240341]\n",
      "530 [D loss: -147.330978] [G loss: -153.174301]\n",
      "531 [D loss: -136.978882] [G loss: -158.129944]\n",
      "532 [D loss: -157.489075] [G loss: -148.826294]\n",
      "533 [D loss: -165.231323] [G loss: -146.196533]\n",
      "534 [D loss: -174.648453] [G loss: -147.591827]\n",
      "535 [D loss: -181.359848] [G loss: -152.666901]\n",
      "536 [D loss: -193.532104] [G loss: -151.020187]\n",
      "537 [D loss: -211.503754] [G loss: -152.249420]\n",
      "538 [D loss: -208.250824] [G loss: -158.021561]\n",
      "539 [D loss: -205.791718] [G loss: -160.591370]\n",
      "540 [D loss: -226.548965] [G loss: -151.640915]\n",
      "541 [D loss: -204.175644] [G loss: -166.530640]\n",
      "542 [D loss: -199.128555] [G loss: -167.036163]\n",
      "543 [D loss: -196.387665] [G loss: -160.597656]\n",
      "544 [D loss: -208.332886] [G loss: -156.489716]\n",
      "545 [D loss: -215.233383] [G loss: -149.295837]\n",
      "546 [D loss: -216.232956] [G loss: -144.953293]\n",
      "547 [D loss: -231.121948] [G loss: -137.817276]\n",
      "548 [D loss: -221.105438] [G loss: -139.978027]\n",
      "549 [D loss: -236.080917] [G loss: -129.022888]\n",
      "550 [D loss: -244.623703] [G loss: -127.261887]\n",
      "551 [D loss: -247.210159] [G loss: -119.014832]\n",
      "552 [D loss: -262.706940] [G loss: -109.640633]\n",
      "553 [D loss: -261.851715] [G loss: -107.339111]\n",
      "554 [D loss: -273.622498] [G loss: -108.293282]\n",
      "555 [D loss: -275.680298] [G loss: -108.141159]\n",
      "556 [D loss: -274.949371] [G loss: -103.918213]\n",
      "557 [D loss: -302.179962] [G loss: -94.935669]\n",
      "558 [D loss: -300.876038] [G loss: -84.210716]\n",
      "559 [D loss: -304.186340] [G loss: -85.027664]\n",
      "560 [D loss: -312.767578] [G loss: -79.136398]\n",
      "561 [D loss: -321.713135] [G loss: -74.316803]\n",
      "562 [D loss: -320.936981] [G loss: -73.039299]\n",
      "563 [D loss: -326.287537] [G loss: -66.033997]\n",
      "564 [D loss: -329.506866] [G loss: -57.770058]\n",
      "565 [D loss: -329.708923] [G loss: -58.612087]\n",
      "566 [D loss: -344.679932] [G loss: -55.079422]\n",
      "567 [D loss: -351.954620] [G loss: -55.842209]\n",
      "568 [D loss: -354.212860] [G loss: -51.025871]\n",
      "569 [D loss: -355.690613] [G loss: -57.473034]\n",
      "570 [D loss: -341.567078] [G loss: -58.711044]\n",
      "571 [D loss: -374.389679] [G loss: -44.869312]\n",
      "572 [D loss: -368.210144] [G loss: -52.170685]\n",
      "573 [D loss: -374.297729] [G loss: -55.026730]\n",
      "574 [D loss: -361.193756] [G loss: -68.196648]\n",
      "575 [D loss: -347.902527] [G loss: -71.881607]\n",
      "576 [D loss: -359.130524] [G loss: -62.139034]\n",
      "577 [D loss: -373.194397] [G loss: -48.181812]\n",
      "578 [D loss: -369.320618] [G loss: -56.489441]\n",
      "579 [D loss: -364.597076] [G loss: -52.791885]\n",
      "580 [D loss: -366.711792] [G loss: -53.462154]\n",
      "581 [D loss: -387.486328] [G loss: -40.351185]\n",
      "582 [D loss: -366.103119] [G loss: -64.256973]\n",
      "583 [D loss: -370.914062] [G loss: -63.052013]\n",
      "584 [D loss: -371.901215] [G loss: -66.921501]\n",
      "585 [D loss: -365.909393] [G loss: -69.326454]\n",
      "586 [D loss: -358.853729] [G loss: -73.763962]\n",
      "587 [D loss: -347.638916] [G loss: -74.821838]\n",
      "588 [D loss: -370.874908] [G loss: -68.144218]\n",
      "589 [D loss: -340.822693] [G loss: -76.753365]\n",
      "590 [D loss: -345.664215] [G loss: -81.187164]\n",
      "591 [D loss: -323.530304] [G loss: -91.259811]\n",
      "592 [D loss: -317.585602] [G loss: -94.851837]\n",
      "593 [D loss: -320.137146] [G loss: -95.920540]\n",
      "594 [D loss: -309.799805] [G loss: -101.975906]\n",
      "595 [D loss: -292.091217] [G loss: -91.138382]\n",
      "596 [D loss: -341.604645] [G loss: -78.886337]\n",
      "597 [D loss: -339.607330] [G loss: -80.276352]\n",
      "598 [D loss: -336.641418] [G loss: -91.125328]\n",
      "599 [D loss: -333.963562] [G loss: -90.378815]\n",
      "600 [D loss: -358.005890] [G loss: -69.704681]\n",
      "601 [D loss: -363.925476] [G loss: -63.770363]\n",
      "602 [D loss: -364.616150] [G loss: -62.911537]\n",
      "603 [D loss: -395.612549] [G loss: -50.647343]\n",
      "604 [D loss: -396.449219] [G loss: -64.161209]\n",
      "605 [D loss: -394.541687] [G loss: -64.035400]\n",
      "606 [D loss: -406.313629] [G loss: -51.786789]\n",
      "607 [D loss: -399.854523] [G loss: -55.849773]\n",
      "608 [D loss: -393.034271] [G loss: -68.883888]\n",
      "609 [D loss: -389.125977] [G loss: -54.987137]\n",
      "610 [D loss: -389.825623] [G loss: -65.715797]\n",
      "611 [D loss: -386.479675] [G loss: -71.820610]\n",
      "612 [D loss: -386.812866] [G loss: -67.119415]\n",
      "613 [D loss: -395.928650] [G loss: -66.791443]\n",
      "614 [D loss: -398.015564] [G loss: -66.970520]\n",
      "615 [D loss: -383.214813] [G loss: -65.838196]\n",
      "616 [D loss: -362.154510] [G loss: -72.590782]\n",
      "617 [D loss: -390.412933] [G loss: -66.935066]\n",
      "618 [D loss: -367.377136] [G loss: -59.513588]\n",
      "619 [D loss: -390.441101] [G loss: -65.158691]\n",
      "620 [D loss: -397.521027] [G loss: -51.715279]\n",
      "621 [D loss: -413.522705] [G loss: -63.618423]\n",
      "622 [D loss: -403.947906] [G loss: -72.357498]\n",
      "623 [D loss: -421.261536] [G loss: -57.365662]\n",
      "624 [D loss: -399.134186] [G loss: -84.140564]\n",
      "625 [D loss: -410.219421] [G loss: -65.211639]\n",
      "626 [D loss: -406.398865] [G loss: -70.419182]\n",
      "627 [D loss: -387.281586] [G loss: -73.536057]\n",
      "628 [D loss: -401.782532] [G loss: -67.117500]\n",
      "629 [D loss: -383.025391] [G loss: -67.310822]\n",
      "630 [D loss: -398.273132] [G loss: -63.267475]\n",
      "631 [D loss: -387.690247] [G loss: -78.038284]\n",
      "632 [D loss: -379.951080] [G loss: -57.038544]\n",
      "633 [D loss: -400.191803] [G loss: -67.817963]\n",
      "634 [D loss: -392.391357] [G loss: -62.201199]\n",
      "635 [D loss: -366.266846] [G loss: -81.363892]\n",
      "636 [D loss: -400.539032] [G loss: -67.770508]\n",
      "637 [D loss: -408.889404] [G loss: -66.377953]\n",
      "638 [D loss: -416.072906] [G loss: -54.214180]\n",
      "639 [D loss: -422.824127] [G loss: -51.341179]\n",
      "640 [D loss: -411.171356] [G loss: -56.090927]\n",
      "641 [D loss: -412.409729] [G loss: -46.788918]\n",
      "642 [D loss: -415.715454] [G loss: -50.596970]\n",
      "643 [D loss: -401.739349] [G loss: -49.266693]\n",
      "644 [D loss: -405.657410] [G loss: -67.559647]\n",
      "645 [D loss: -408.523499] [G loss: -66.194191]\n",
      "646 [D loss: -390.584290] [G loss: -63.586967]\n",
      "647 [D loss: -405.552460] [G loss: -49.900394]\n",
      "648 [D loss: -352.878662] [G loss: -62.974525]\n",
      "649 [D loss: -367.520752] [G loss: -58.461044]\n",
      "650 [D loss: -409.767761] [G loss: -60.278961]\n",
      "651 [D loss: -371.887817] [G loss: -54.488609]\n",
      "652 [D loss: -431.903442] [G loss: -40.797745]\n",
      "653 [D loss: -439.764282] [G loss: -31.661791]\n",
      "654 [D loss: -400.380249] [G loss: -57.466751]\n",
      "655 [D loss: -431.984894] [G loss: -60.312721]\n",
      "656 [D loss: -403.982788] [G loss: -53.330212]\n",
      "657 [D loss: -422.867493] [G loss: -44.051800]\n",
      "658 [D loss: -421.079346] [G loss: -27.484224]\n",
      "659 [D loss: -409.941589] [G loss: -41.574791]\n",
      "660 [D loss: -399.236603] [G loss: -41.859726]\n",
      "661 [D loss: -402.597839] [G loss: -42.940208]\n",
      "662 [D loss: -437.158905] [G loss: -37.466766]\n",
      "663 [D loss: -387.160431] [G loss: -50.398354]\n",
      "664 [D loss: -398.277405] [G loss: -44.447487]\n",
      "665 [D loss: -392.023071] [G loss: -55.487289]\n",
      "666 [D loss: -434.447021] [G loss: -52.047260]\n",
      "667 [D loss: -417.682373] [G loss: -59.136253]\n",
      "668 [D loss: -386.598480] [G loss: -63.577721]\n",
      "669 [D loss: -420.757446] [G loss: -49.577957]\n",
      "670 [D loss: -392.976105] [G loss: -44.704803]\n",
      "671 [D loss: -421.121277] [G loss: -51.541496]\n",
      "672 [D loss: -417.466736] [G loss: -47.877281]\n",
      "673 [D loss: -411.753967] [G loss: -43.638729]\n",
      "674 [D loss: -389.941650] [G loss: -46.705833]\n",
      "675 [D loss: -394.814331] [G loss: -36.682762]\n",
      "676 [D loss: -447.190002] [G loss: -52.653774]\n",
      "677 [D loss: -385.292572] [G loss: -34.583744]\n",
      "678 [D loss: -437.240753] [G loss: -53.302513]\n",
      "679 [D loss: -397.697845] [G loss: -40.760960]\n",
      "680 [D loss: -452.722412] [G loss: -43.816452]\n",
      "681 [D loss: -442.252502] [G loss: -35.608624]\n",
      "682 [D loss: -440.699249] [G loss: -40.472816]\n",
      "683 [D loss: -372.267212] [G loss: -46.422688]\n",
      "684 [D loss: -418.765320] [G loss: -41.126770]\n",
      "685 [D loss: -388.843140] [G loss: -46.755402]\n",
      "686 [D loss: -402.286133] [G loss: -37.339035]\n",
      "687 [D loss: -372.140320] [G loss: -34.776840]\n",
      "688 [D loss: -424.323883] [G loss: -32.870289]\n",
      "689 [D loss: -392.930878] [G loss: -50.286385]\n",
      "690 [D loss: -379.077606] [G loss: -64.424896]\n",
      "691 [D loss: -349.191650] [G loss: -60.606354]\n",
      "692 [D loss: -333.958160] [G loss: -59.553825]\n",
      "693 [D loss: -314.848206] [G loss: -42.889328]\n",
      "694 [D loss: -374.718109] [G loss: -34.051956]\n",
      "695 [D loss: -345.616821] [G loss: -47.506569]\n",
      "696 [D loss: -395.993073] [G loss: -43.869312]\n",
      "697 [D loss: -400.529358] [G loss: -43.200455]\n",
      "698 [D loss: -420.808197] [G loss: -42.318581]\n",
      "699 [D loss: -414.088531] [G loss: -41.733910]\n",
      "700 [D loss: -410.418579] [G loss: -40.529846]\n",
      "701 [D loss: -414.112366] [G loss: -31.713514]\n",
      "702 [D loss: -451.450317] [G loss: -29.576681]\n",
      "703 [D loss: -468.300537] [G loss: -30.243740]\n",
      "704 [D loss: -402.716370] [G loss: -36.129398]\n",
      "705 [D loss: -430.210754] [G loss: -32.352020]\n",
      "706 [D loss: -474.829041] [G loss: -37.768711]\n",
      "707 [D loss: -421.987152] [G loss: -44.713577]\n",
      "708 [D loss: -474.256104] [G loss: -38.763931]\n",
      "709 [D loss: -474.123779] [G loss: -41.631832]\n",
      "710 [D loss: -444.760651] [G loss: -50.407421]\n",
      "711 [D loss: -458.798401] [G loss: -56.134995]\n",
      "712 [D loss: -409.504211] [G loss: -42.806847]\n",
      "713 [D loss: -426.511322] [G loss: -25.113438]\n",
      "714 [D loss: -453.649017] [G loss: -50.955536]\n",
      "715 [D loss: -376.565552] [G loss: -33.143909]\n",
      "716 [D loss: -402.492004] [G loss: -33.236370]\n",
      "717 [D loss: -396.346680] [G loss: -50.225586]\n",
      "718 [D loss: -346.895569] [G loss: -48.518448]\n",
      "719 [D loss: -355.378174] [G loss: -41.321381]\n",
      "720 [D loss: -325.595001] [G loss: -40.286621]\n",
      "721 [D loss: -341.224670] [G loss: -34.695740]\n",
      "722 [D loss: -346.751801] [G loss: -40.813389]\n",
      "723 [D loss: -295.908844] [G loss: -39.405525]\n",
      "724 [D loss: -333.250702] [G loss: -30.886940]\n",
      "725 [D loss: -303.287567] [G loss: -33.585518]\n",
      "726 [D loss: -285.912476] [G loss: -25.402882]\n",
      "727 [D loss: -305.276093] [G loss: -36.229412]\n",
      "728 [D loss: -318.696442] [G loss: -38.261887]\n",
      "729 [D loss: -322.814758] [G loss: -49.623123]\n",
      "730 [D loss: -272.161011] [G loss: -41.343128]\n",
      "731 [D loss: -288.762085] [G loss: -53.168762]\n",
      "732 [D loss: -262.916626] [G loss: -68.180374]\n",
      "733 [D loss: -298.727844] [G loss: -65.567596]\n",
      "734 [D loss: -318.783081] [G loss: -57.486092]\n",
      "735 [D loss: -306.143707] [G loss: -51.659683]\n",
      "736 [D loss: -272.391357] [G loss: -38.751728]\n",
      "737 [D loss: -330.099762] [G loss: -47.175034]\n",
      "738 [D loss: -335.033386] [G loss: -58.743473]\n",
      "739 [D loss: -348.459015] [G loss: -70.048584]\n",
      "740 [D loss: -351.506073] [G loss: -73.131813]\n",
      "741 [D loss: -360.012085] [G loss: -79.907463]\n",
      "742 [D loss: -284.661285] [G loss: -95.013840]\n",
      "743 [D loss: -298.530273] [G loss: -95.225037]\n",
      "744 [D loss: -289.674927] [G loss: -105.380051]\n",
      "745 [D loss: -292.532104] [G loss: -96.175400]\n",
      "746 [D loss: -242.339569] [G loss: -97.712204]\n",
      "747 [D loss: -257.260651] [G loss: -97.047623]\n",
      "748 [D loss: -266.630676] [G loss: -83.541969]\n",
      "749 [D loss: -270.961182] [G loss: -90.996529]\n",
      "750 [D loss: -226.184219] [G loss: -106.229340]\n",
      "751 [D loss: -213.064270] [G loss: -80.936981]\n",
      "752 [D loss: -249.812286] [G loss: -83.683960]\n",
      "753 [D loss: -249.515228] [G loss: -81.151863]\n",
      "754 [D loss: -225.980820] [G loss: -100.188423]\n",
      "755 [D loss: -261.302002] [G loss: -90.957710]\n",
      "756 [D loss: -228.762085] [G loss: -103.947952]\n",
      "757 [D loss: -297.279449] [G loss: -98.475647]\n",
      "758 [D loss: -251.004593] [G loss: -109.647247]\n",
      "759 [D loss: -259.990875] [G loss: -109.163651]\n",
      "760 [D loss: -162.006714] [G loss: -127.133255]\n",
      "761 [D loss: -25.131195] [G loss: -128.409256]\n",
      "762 [D loss: -266.552979] [G loss: -121.897751]\n",
      "763 [D loss: -238.237000] [G loss: -114.307220]\n",
      "764 [D loss: -131.129730] [G loss: -124.772255]\n",
      "765 [D loss: -291.315002] [G loss: -112.303955]\n",
      "766 [D loss: -294.360413] [G loss: -90.062469]\n",
      "767 [D loss: -294.347778] [G loss: -79.778656]\n",
      "768 [D loss: -341.521820] [G loss: -74.201569]\n",
      "769 [D loss: -354.964355] [G loss: -78.007332]\n",
      "770 [D loss: -366.983124] [G loss: -59.418213]\n",
      "771 [D loss: -366.513062] [G loss: -49.075455]\n",
      "772 [D loss: -382.495911] [G loss: -70.576759]\n",
      "773 [D loss: -382.931946] [G loss: -72.134796]\n",
      "774 [D loss: -405.985870] [G loss: -75.700394]\n",
      "775 [D loss: -369.186401] [G loss: -87.625046]\n",
      "776 [D loss: -398.042694] [G loss: -75.126511]\n",
      "777 [D loss: -359.616272] [G loss: -73.248047]\n",
      "778 [D loss: -411.259003] [G loss: -75.777557]\n",
      "779 [D loss: -382.285065] [G loss: -55.915184]\n",
      "780 [D loss: -394.016449] [G loss: -62.976185]\n",
      "781 [D loss: -401.746613] [G loss: -44.733086]\n",
      "782 [D loss: -436.281952] [G loss: -47.181957]\n",
      "783 [D loss: -440.376038] [G loss: -49.708176]\n",
      "784 [D loss: -365.652710] [G loss: -58.368568]\n",
      "785 [D loss: -360.364868] [G loss: -68.693665]\n",
      "786 [D loss: -441.548248] [G loss: -69.263489]\n",
      "787 [D loss: -384.973938] [G loss: -93.309357]\n",
      "788 [D loss: -418.097839] [G loss: -70.968018]\n",
      "789 [D loss: -450.959167] [G loss: -70.506912]\n",
      "790 [D loss: -467.420319] [G loss: -75.474152]\n",
      "791 [D loss: -432.632996] [G loss: -77.323380]\n",
      "792 [D loss: -403.776276] [G loss: -100.661438]\n",
      "793 [D loss: -444.090881] [G loss: -90.555298]\n",
      "794 [D loss: -385.821777] [G loss: -108.221970]\n",
      "795 [D loss: -413.283600] [G loss: -112.819725]\n",
      "796 [D loss: -416.891693] [G loss: -76.639244]\n",
      "797 [D loss: -441.541656] [G loss: -75.002052]\n",
      "798 [D loss: -448.948547] [G loss: -72.238022]\n",
      "799 [D loss: -456.252014] [G loss: -69.306923]\n",
      "800 [D loss: -411.751190] [G loss: -79.043320]\n",
      "801 [D loss: -424.561157] [G loss: -75.138229]\n",
      "802 [D loss: -488.641968] [G loss: -62.058540]\n",
      "803 [D loss: -496.216553] [G loss: -83.387054]\n",
      "804 [D loss: -459.155457] [G loss: -60.630623]\n",
      "805 [D loss: -477.548584] [G loss: -55.502163]\n",
      "806 [D loss: -482.281403] [G loss: -62.059631]\n",
      "807 [D loss: -504.163025] [G loss: -55.592770]\n",
      "808 [D loss: -423.723602] [G loss: -87.064499]\n",
      "809 [D loss: -485.786682] [G loss: -75.980331]\n",
      "810 [D loss: -455.259460] [G loss: -50.219543]\n",
      "811 [D loss: -463.100922] [G loss: -49.910576]\n",
      "812 [D loss: -464.055908] [G loss: -72.861687]\n",
      "813 [D loss: -479.201630] [G loss: -64.497665]\n",
      "814 [D loss: -433.143127] [G loss: -73.138519]\n",
      "815 [D loss: -447.598663] [G loss: -63.828320]\n",
      "816 [D loss: -479.908936] [G loss: -61.509495]\n",
      "817 [D loss: -442.606110] [G loss: -56.531372]\n",
      "818 [D loss: -452.880554] [G loss: -50.457573]\n",
      "819 [D loss: -484.980469] [G loss: -82.639946]\n",
      "820 [D loss: -445.831665] [G loss: -74.413986]\n",
      "821 [D loss: -475.452179] [G loss: -60.693462]\n",
      "822 [D loss: -466.999207] [G loss: -87.372116]\n",
      "823 [D loss: -482.077393] [G loss: -66.158417]\n",
      "824 [D loss: -464.703705] [G loss: -84.006111]\n",
      "825 [D loss: -468.993225] [G loss: -74.188660]\n",
      "826 [D loss: -476.528961] [G loss: -86.601700]\n",
      "827 [D loss: -261.702423] [G loss: -107.296181]\n",
      "828 [D loss: -448.588593] [G loss: -94.205597]\n",
      "829 [D loss: -406.188660] [G loss: -100.297958]\n",
      "830 [D loss: -468.314362] [G loss: -95.451721]\n",
      "831 [D loss: -386.949249] [G loss: -106.124069]\n",
      "832 [D loss: -462.680267] [G loss: -84.712624]\n",
      "833 [D loss: -485.713409] [G loss: -74.119080]\n",
      "834 [D loss: -499.210297] [G loss: -83.769684]\n",
      "835 [D loss: -457.315552] [G loss: -117.292931]\n",
      "836 [D loss: -169.333801] [G loss: -148.909515]\n",
      "837 [D loss: -216.373077] [G loss: -165.996979]\n",
      "838 [D loss: -387.175110] [G loss: -144.643921]\n",
      "839 [D loss: -397.071167] [G loss: -132.617737]\n",
      "840 [D loss: -389.518036] [G loss: -128.940460]\n",
      "841 [D loss: -422.281738] [G loss: -131.794434]\n",
      "842 [D loss: -461.446899] [G loss: -113.507774]\n",
      "843 [D loss: -487.323608] [G loss: -106.070511]\n",
      "844 [D loss: -475.456146] [G loss: -106.789291]\n",
      "845 [D loss: -482.344177] [G loss: -102.197739]\n",
      "846 [D loss: -442.829956] [G loss: -116.487274]\n",
      "847 [D loss: -521.248596] [G loss: -111.604355]\n",
      "848 [D loss: -536.452698] [G loss: -104.536713]\n",
      "849 [D loss: -505.218903] [G loss: -122.994049]\n",
      "850 [D loss: -449.622437] [G loss: -141.277557]\n",
      "851 [D loss: -414.872009] [G loss: -160.870972]\n",
      "852 [D loss: -453.177521] [G loss: -151.309891]\n",
      "853 [D loss: -461.020203] [G loss: -159.269501]\n",
      "854 [D loss: -432.928436] [G loss: -161.186523]\n",
      "855 [D loss: -472.038208] [G loss: -157.909790]\n",
      "856 [D loss: -452.147186] [G loss: -133.411194]\n",
      "857 [D loss: -496.797882] [G loss: -118.868347]\n",
      "858 [D loss: -528.220215] [G loss: -122.283195]\n",
      "859 [D loss: -504.465820] [G loss: -137.005432]\n",
      "860 [D loss: -524.077148] [G loss: -125.405128]\n",
      "861 [D loss: -520.327209] [G loss: -118.246284]\n",
      "862 [D loss: -495.848145] [G loss: -129.573853]\n",
      "863 [D loss: -497.923218] [G loss: -161.925842]\n",
      "864 [D loss: -495.153931] [G loss: -170.765228]\n",
      "865 [D loss: -520.077637] [G loss: -151.294525]\n",
      "866 [D loss: -514.578735] [G loss: -152.110046]\n",
      "867 [D loss: -504.137512] [G loss: -147.518402]\n",
      "868 [D loss: -502.535767] [G loss: -138.199036]\n",
      "869 [D loss: -515.004089] [G loss: -144.569244]\n",
      "870 [D loss: -497.586304] [G loss: -152.368988]\n",
      "871 [D loss: -506.426636] [G loss: -158.823090]\n",
      "872 [D loss: -474.732788] [G loss: -178.691864]\n",
      "873 [D loss: -479.574524] [G loss: -165.136597]\n",
      "874 [D loss: -512.888977] [G loss: -152.689972]\n",
      "875 [D loss: -459.189453] [G loss: -128.301422]\n",
      "876 [D loss: -526.821777] [G loss: -142.864853]\n",
      "877 [D loss: -487.524536] [G loss: -162.327057]\n",
      "878 [D loss: -528.805542] [G loss: -153.195343]\n",
      "879 [D loss: -530.126343] [G loss: -165.313171]\n",
      "880 [D loss: -505.138550] [G loss: -175.926086]\n",
      "881 [D loss: -486.637421] [G loss: -181.013580]\n",
      "882 [D loss: -545.027100] [G loss: -164.593033]\n",
      "883 [D loss: -474.191833] [G loss: -187.517181]\n",
      "884 [D loss: -530.671021] [G loss: -151.760681]\n",
      "885 [D loss: -413.346252] [G loss: -165.050735]\n",
      "886 [D loss: -557.382080] [G loss: -162.000473]\n",
      "887 [D loss: -556.030090] [G loss: -161.360535]\n",
      "888 [D loss: -559.544189] [G loss: -151.447601]\n",
      "889 [D loss: -516.241638] [G loss: -188.153061]\n",
      "890 [D loss: -540.434937] [G loss: -167.667130]\n",
      "891 [D loss: -558.212585] [G loss: -180.629974]\n",
      "892 [D loss: -521.045898] [G loss: -175.637314]\n",
      "893 [D loss: -519.708984] [G loss: -199.598358]\n",
      "894 [D loss: -442.044861] [G loss: -192.780792]\n",
      "895 [D loss: -515.846558] [G loss: -229.010468]\n",
      "896 [D loss: -456.678589] [G loss: -210.140823]\n",
      "897 [D loss: -554.654175] [G loss: -202.367096]\n",
      "898 [D loss: -540.421631] [G loss: -187.131653]\n",
      "899 [D loss: -555.660950] [G loss: -186.605392]\n",
      "900 [D loss: -575.674072] [G loss: -172.273712]\n",
      "901 [D loss: -590.774780] [G loss: -181.946289]\n",
      "902 [D loss: -601.473267] [G loss: -176.865906]\n",
      "903 [D loss: -537.259644] [G loss: -198.708038]\n",
      "904 [D loss: -528.008423] [G loss: -213.624512]\n",
      "905 [D loss: -570.558533] [G loss: -196.070618]\n",
      "906 [D loss: -581.088562] [G loss: -205.706451]\n",
      "907 [D loss: -541.301819] [G loss: -209.004608]\n",
      "908 [D loss: -554.757935] [G loss: -202.028214]\n",
      "909 [D loss: -580.904175] [G loss: -190.849564]\n",
      "910 [D loss: -552.700439] [G loss: -189.346649]\n",
      "911 [D loss: -588.507263] [G loss: -175.515732]\n",
      "912 [D loss: -581.917786] [G loss: -168.413940]\n",
      "913 [D loss: -584.872925] [G loss: -160.764526]\n",
      "914 [D loss: -620.051208] [G loss: -151.580109]\n",
      "915 [D loss: -606.250732] [G loss: -175.130035]\n",
      "916 [D loss: -603.571777] [G loss: -169.510254]\n",
      "917 [D loss: -641.459717] [G loss: -168.879425]\n",
      "918 [D loss: -581.627869] [G loss: -176.666550]\n",
      "919 [D loss: -564.021301] [G loss: -183.795654]\n",
      "920 [D loss: -615.639221] [G loss: -177.843018]\n",
      "921 [D loss: -644.357544] [G loss: -184.169556]\n",
      "922 [D loss: -667.535767] [G loss: -168.439301]\n",
      "923 [D loss: -684.077637] [G loss: -167.781342]\n",
      "924 [D loss: -628.453674] [G loss: -173.845703]\n",
      "925 [D loss: -658.922058] [G loss: -159.268494]\n",
      "926 [D loss: -648.985718] [G loss: -169.228271]\n",
      "927 [D loss: -692.393433] [G loss: -151.191956]\n",
      "928 [D loss: -659.807007] [G loss: -157.993805]\n",
      "929 [D loss: -719.114685] [G loss: -163.384064]\n",
      "930 [D loss: -671.627625] [G loss: -163.772949]\n",
      "931 [D loss: -643.531860] [G loss: -168.632889]\n",
      "932 [D loss: -681.708679] [G loss: -165.749023]\n",
      "933 [D loss: -693.797607] [G loss: -159.911179]\n",
      "934 [D loss: -668.397827] [G loss: -146.622101]\n",
      "935 [D loss: -704.580078] [G loss: -122.453415]\n",
      "936 [D loss: -650.383423] [G loss: -140.550812]\n",
      "937 [D loss: -667.460815] [G loss: -151.095795]\n",
      "938 [D loss: -714.523071] [G loss: -137.584579]\n",
      "939 [D loss: -701.947754] [G loss: -149.502075]\n",
      "940 [D loss: -684.340332] [G loss: -135.543518]\n",
      "941 [D loss: -718.010986] [G loss: -128.181900]\n",
      "942 [D loss: -683.869629] [G loss: -137.275116]\n",
      "943 [D loss: -635.751465] [G loss: -153.812683]\n",
      "944 [D loss: -665.888672] [G loss: -145.558624]\n",
      "945 [D loss: -695.207764] [G loss: -127.370110]\n",
      "946 [D loss: -645.394409] [G loss: -155.715881]\n",
      "947 [D loss: -677.192627] [G loss: -146.502625]\n",
      "948 [D loss: -664.575317] [G loss: -157.215866]\n",
      "949 [D loss: -667.388306] [G loss: -153.031158]\n",
      "950 [D loss: -615.827209] [G loss: -147.035385]\n",
      "951 [D loss: -606.270325] [G loss: -144.632141]\n",
      "952 [D loss: -730.254211] [G loss: -135.152466]\n",
      "953 [D loss: -711.189209] [G loss: -141.742447]\n",
      "954 [D loss: -709.687317] [G loss: -131.643768]\n",
      "955 [D loss: -723.827637] [G loss: -140.096420]\n",
      "956 [D loss: -711.536804] [G loss: -131.553940]\n",
      "957 [D loss: -723.566956] [G loss: -135.153656]\n",
      "958 [D loss: -718.941162] [G loss: -140.805527]\n",
      "959 [D loss: -723.770508] [G loss: -134.691406]\n",
      "960 [D loss: -657.369873] [G loss: -144.460968]\n",
      "961 [D loss: -736.322632] [G loss: -121.721519]\n",
      "962 [D loss: -661.874634] [G loss: -129.090347]\n",
      "963 [D loss: -739.433411] [G loss: -132.704285]\n",
      "964 [D loss: -714.397278] [G loss: -127.541985]\n",
      "965 [D loss: -774.570312] [G loss: -142.869461]\n",
      "966 [D loss: -732.231201] [G loss: -157.371155]\n",
      "967 [D loss: -753.482300] [G loss: -142.695450]\n",
      "968 [D loss: -765.571350] [G loss: -147.697418]\n",
      "969 [D loss: -756.022217] [G loss: -156.749557]\n",
      "970 [D loss: -754.794434] [G loss: -152.579239]\n",
      "971 [D loss: -782.952698] [G loss: -144.804001]\n",
      "972 [D loss: -779.820190] [G loss: -140.817200]\n",
      "973 [D loss: -765.663574] [G loss: -159.016739]\n",
      "974 [D loss: -783.998291] [G loss: -126.881729]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGANGP()\n",
    "wgan.train(epochs=3000, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1696fff12340fbeacd8891884860ba5d4999e3a236c837e4d2afed27776e33eb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
